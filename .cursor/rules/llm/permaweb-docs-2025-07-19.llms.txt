# Permaweb Documentation Collection

Generated on: 2025-07-19T04:38:42.721Z
Total documents: 324
Total words: 161907

## Table of Contents

### Included Documents

1. [Glossary](https://glossary.arweave.net/glossary.txt)
2. [Arns viewer - Guides](https://docs.ar.io/build/guides/arns-viewer)
3. [Arns viewer - Guides](https://docs.ar.io/guides/arns-viewer)
4. [Register an IP Asset on Arweave](https://docs.ar.io/guides/story)
5. [Minimal Svelte Starter Kit](https://cookbook.arweave.net/kits/svelte/minimal.html)
6. [wasm6410](https://hyperbeam.arweave.net/build/devices/wasm64-at-1-0.html)
7. [useWayfinder](https://docs.ar.io/wayfinder/react/use-wayfinder)
8. [Create React App Starter Kit](https://cookbook.arweave.net/kits/react/create-react-app.html)
9. [TEE Nodes](https://hyperbeam.arweave.net/run/tee-nodes.html)
10. [React Starter Kit wvite ArDrive](https://cookbook.arweave.net/kits/react/turbo.html)
11. [Getting started - Wayfinder](https://docs.ar.io/wayfinder/getting-started)
12. [Fetching Transaction Data](https://cookbook.arweave.net/guides/http-api.html)
13. [Create Vue Starter Kit](https://cookbook.arweave.net/kits/vue/create-vue.html)
14. [Core - Wayfinder](https://docs.ar.io/wayfinder/core)
15. [Overview](https://hyperbeam.arweave.net/build/devices/hyperbeam-devices.html)
16. [Module dev_routererl](https://hyperbeam.arweave.net/build/devices/source-code/dev_router.html)
17. [Module dev_p4erl](https://hyperbeam.arweave.net/build/devices/source-code/dev_p4.html)
18. [JoiningRunning a Router](https://hyperbeam.arweave.net/run/joining-running-a-router.html)
19. [Running a HyperBEAM Node](https://hyperbeam.arweave.net/run/running-a-hyperbeam-node.html)
20. [joinNetwork](https://docs.ar.io/ar-io-sdk/ario/gateways/join-network)
21. [ARIO Network Testnet](https://docs.ar.io/guides/testnet)
22. [Querying Transactions](https://cookbook.arweave.net/concepts/queryTransactions.html)
23. [Hello World (CLI)](https://cookbook.arweave.net/getting-started/quick-starts/hw-cli.html)
24. [Atomic Tokens](https://cookbook.arweave.net/guides/atomic-tokens/intro.html)
25. [Github Action](https://cookbook.arweave.net/guides/deployment/github-action.html)
26. [Posting Transactions using arweave-js](https://cookbook.arweave.net/guides/posting-transactions/arweave-js.html)
27. [Querying Arweave with GraphQL](https://cookbook.arweave.net/guides/querying-arweave/queryingArweave.html)
28. [ar-gql](https://cookbook.arweave.net/guides/querying-arweave/ar-gql.html)
29. [Hyperbeam.arweave.net - Hyperbeam](https://hyperbeam.arweave.net/)
30. [meta10](https://hyperbeam.arweave.net/build/devices/meta-at-1-0.html)
31. [relay10](https://hyperbeam.arweave.net/build/devices/relay-at-1-0.html)
32. [scheduler10](https://hyperbeam.arweave.net/build/devices/scheduler-at-1-0.html)
33. [Core Capabilities](https://hyperbeam.arweave.net/build/hyperbeam-capabilities.html)
34. [Intro to AO-Core](https://hyperbeam.arweave.net/build/introduction/what-is-ao-core.html)
35. [Pathing in HyperBEAM](https://hyperbeam.arweave.net/build/pathing-in-hyperbeam.html)
36. [Troubleshooting](https://hyperbeam.arweave.net/run/reference/troubleshooting.html)
37. [Configuring Your Machine](https://hyperbeam.arweave.net/run/configuring-your-machine.html)
38. [FAQ](https://hyperbeam.arweave.net/run/reference/faq.html)
39. [ao Specs](https://cookbook_ao.arweave.net/concepts/specs.html)
40. [Messages](https://cookbook_ao.arweave.net/concepts/messages.html)
41. [A whistle stop tour of Lua](https://cookbook_ao.arweave.net/concepts/lua.html)
42. [aos Brief Tour](https://cookbook_ao.arweave.net/concepts/tour.html)
43. [Sending a Message to a Process](https://cookbook_ao.arweave.net/guides/aoconnect/sending-messages.html)
44. [DataItem Signers](https://cookbook_ao.arweave.net/guides/aoconnect/signers.html)
45. [Staking Blueprint](https://cookbook_ao.arweave.net/guides/aos/blueprints/staking.html)
46. [CRED Utils Blueprint](https://cookbook_ao.arweave.net/guides/aos/blueprints/cred-utils.html)
47. [Spawning a Process](https://cookbook_ao.arweave.net/guides/aoconnect/spawning-processes.html)
48. [Voting Blueprint](https://cookbook_ao.arweave.net/guides/aos/blueprints/voting.html)
49. [Editor setup](https://cookbook_ao.arweave.net/guides/aos/editor.html)
50. [FAQ](https://cookbook_ao.arweave.net/guides/aos/faq.html)
51. [CLI](https://cookbook_ao.arweave.net/guides/aos/cli.html)
52. [Token Blueprint](https://cookbook_ao.arweave.net/guides/aos/blueprints/token.html)
53. [aos AO Operating System](https://cookbook_ao.arweave.net/guides/aos/index.html)
54. [Building a Token in ao](https://cookbook_ao.arweave.net/guides/aos/token.html)
55. [AO Dev-Cli 01](https://cookbook_ao.arweave.net/guides/dev-cli/index.html)
56. [Guides](https://cookbook_ao.arweave.net/guides/index.html)
57. [Troubleshooting using aolink](https://cookbook_ao.arweave.net/guides/aos/troubleshooting.html)
58. [HyperBEAM from AO Connect](https://cookbook_ao.arweave.net/guides/migrating-to-hyperbeam/ao-connect.html)
59. [Reading Dynamic State](https://cookbook_ao.arweave.net/guides/migrating-to-hyperbeam/reading-dynamic-state.html)
60. [Getting started with SQLite](https://cookbook_ao.arweave.net/guides/snacks/sqlite.html)
61. [Using WeaveDrive](https://cookbook_ao.arweave.net/guides/snacks/weavedrive.html)
62. [Community Resources](https://cookbook_ao.arweave.net/references/community.html)
63. [Editor setup](https://cookbook_ao.arweave.net/references/editor-setup.html)
64. [Meet Lua](https://cookbook_ao.arweave.net/references/lua.html)
65. [Accessing Data from Arweave with ao](https://cookbook_ao.arweave.net/references/data.html)
66. [Messaging Patterns in ao](https://cookbook_ao.arweave.net/references/messaging.html)
67. [ao Token and Subledger Specification](https://cookbook_ao.arweave.net/references/token.html)
68. [Meet Web Assembly](https://cookbook_ao.arweave.net/references/wasm.html)
69. [Crafting a Token](https://cookbook_ao.arweave.net/tutorials/begin/token.html)
70. [Preparations](https://cookbook_ao.arweave.net/tutorials/begin/preparations.html)
71. [Automated Responses](https://cookbook_ao.arweave.net/tutorials/bots-and-games/attacking.html)
72. [Lets Play A Game](https://cookbook_ao.arweave.net/tutorials/bots-and-games/ao-effect.html)
73. [Tokengating the Chatroom](https://cookbook_ao.arweave.net/tutorials/begin/tokengating.html)
74. [Mechanics of the Arena](https://cookbook_ao.arweave.net/tutorials/bots-and-games/arena-mechanics.html)
75. [Strategic Decisions](https://cookbook_ao.arweave.net/tutorials/bots-and-games/decisions.html)
76. [Bots and Games](https://cookbook_ao.arweave.net/tutorials/bots-and-games/index.html)
77. [Bringing it Together](https://cookbook_ao.arweave.net/tutorials/bots-and-games/bringing-together.html)
78. [Get started in 5 minutes](https://cookbook_ao.arweave.net/welcome/getting-started.html)
79. [Introduction to AO-Core](https://cookbook_ao.arweave.net/welcome/ao-core-introduction.html)
80. [Ar io sdk - Docs](https://docs.ar.io/ar-io-sdk)
81. [setBaseNameRecord](https://docs.ar.io/ar-io-sdk/ants/set-base-name-record)
82. [setRecord](https://docs.ar.io/ar-io-sdk/ants/set-record)
83. [Extend lease - Arns](https://docs.ar.io/ar-io-sdk/ario/arns/extend-lease)
84. [buyRecord](https://docs.ar.io/ar-io-sdk/ario/arns/buy-record)
85. [setUndernameRecord](https://docs.ar.io/ar-io-sdk/ants/set-undername-record)
86. [getCostDetails](https://docs.ar.io/ar-io-sdk/ario/arns/get-cost-details)
87. [Increase undername limit - Arns](https://docs.ar.io/ar-io-sdk/ario/arns/increase-undername-limit)
88. [Update gateway settings - Gateways](https://docs.ar.io/ar-io-sdk/ario/gateways/update-gateway-settings)
89. [Redelegate stake - Gateways](https://docs.ar.io/ar-io-sdk/ario/gateways/redelegate-stake)
90. [Deploy a dApp with ArDrive web](https://docs.ar.io/build/guides/ardrive-web)
91. [ARIO Smart Contract](https://docs.ar.io/ario-contract)
92. [Arweave Name System (ArNS)](https://docs.ar.io/arns)
93. [ARIO SDK Release Notes](https://docs.ar.io/ar-io-sdk/release-notes)
94. [Arlink Deploy](https://docs.ar.io/build/guides/arlink)
95. [Gql - Guides](https://docs.ar.io/build/guides/gql)
96. [Managing Undernames](https://docs.ar.io/build/guides/managing-undernames)
97. [Permaweb deploy - Guides](https://docs.ar.io/build/guides/permaweb-deploy)
98. [Gateway Architecture](https://docs.ar.io/gateways)
99. [Advanced - Gateways](https://docs.ar.io/gateways/advanced)
100. [Normalized Addresses](https://docs.ar.io/concepts/normalized-addresses)
101. [AO Compute Unit (CU)](https://docs.ar.io/gateways/cu)
102. [ARIO Gateway Environment Variables](https://docs.ar.io/gateways/env)
103. [Advanced config - Ar io node](https://docs.ar.io/gateways/ar-io-node/advanced-config.html)
104. [Bundler - Gateways](https://docs.ar.io/gateways/bundler)
105. [Gateway Network](https://docs.ar.io/gateways/gateway-network)
106. [ARIO Gateway Grafana](https://docs.ar.io/gateways/grafana)
107. [Join the Gateway Network](https://docs.ar.io/gateways/join-network)
108. [ARIO Node Filtering System](https://docs.ar.io/gateways/filters)
109. [Linux setup - Gateways](https://docs.ar.io/gateways/linux-setup)
110. [Observation and Incentives](https://docs.ar.io/gateways/observer)
111. [Content Moderation](https://docs.ar.io/gateways/moderation)
112. [Parquet and ClickHouse Usage Guide](https://docs.ar.io/gateways/parquet)
113. [Optimizing Data Handling in ARIO Gateway](https://docs.ar.io/gateways/optimize-data)
114. [ARIO Node Release Notes](https://docs.ar.io/gateways/release-notes)
115. [ARIO Node Release Notes](https://docs.ar.io/gateways/release-notes#)
116. [Upgrading - Gateways](https://docs.ar.io/gateways/upgrading)
117. [Gateway Troubleshooting FAQ](https://docs.ar.io/gateways/troubleshooting)
118. [Windows setup - Gateways](https://docs.ar.io/gateways/windows-setup)
119. [Importing SQLite Database Snapshots](https://docs.ar.io/gateways/snapshots)
120. [Deploy a dApp with ArDrive web](https://docs.ar.io/guides/ardrive-web)
121. [ANTs on Bazar](https://docs.ar.io/guides/ants-on-bazar)
122. [Arlink Deploy](https://docs.ar.io/guides/arlink)
123. [Glossary](https://docs.ar.io/glossary)
124. [Gql - Guides](https://docs.ar.io/guides/gql)
125. [Permaweb deploy - Guides](https://docs.ar.io/guides/permaweb-deploy)
126. [Managing Undernames](https://docs.ar.io/guides/managing-undernames)
127. [Managing Primary Names](https://docs.ar.io/guides/primary-names)
128. [ANTs on Bazar](https://docs.ar.io/learn/guides/ants-on-bazar)
129. [Uploading to Arweave](https://docs.ar.io/guides/uploading-to-arweave)
130. [The ARIO Token](https://docs.ar.io/token)
131. [Wayfinder - Docs](https://docs.ar.io/wayfinder)
132. [Local storage - Gateway providers](https://docs.ar.io/wayfinder/core/gateway-providers/local-storage)
133. [Telemetry - Core](https://docs.ar.io/wayfinder/core/telemetry)
134. [Signature Verification Strategy](https://docs.ar.io/wayfinder/core/verification-strategies/signature-verification)
135. [Privacy](https://cookbook.arweave.net/concepts/arfs/privacy.html)
136. [ArFS Protocol A Decentralized File System on Arweave](https://cookbook.arweave.net/concepts/arfs/arfs.html)
137. [Entity Types](https://cookbook.arweave.net/concepts/arfs/entity-types.html)
138. [Data Model](https://cookbook.arweave.net/concepts/arfs/data-model.html)
139. [Content Types](https://cookbook.arweave.net/concepts/arfs/content-types.html)
140. [ArNS - Arweave Name System](https://cookbook.arweave.net/concepts/arns.html)
141. [Wallets and Keys](https://cookbook.arweave.net/concepts/keyfiles-and-wallets.html)
142. [Permaweb Applications](https://cookbook.arweave.net/concepts/permawebApplications.html)
143. [Path Manifests](https://cookbook.arweave.net/concepts/manifests.html)
144. [Welcome to the Permaweb](https://cookbook.arweave.net/concepts/permaweb.html)
145. [Posting Transactions](https://cookbook.arweave.net/concepts/post-transactions.html)
146. [SmartWeave](https://cookbook.arweave.net/concepts/smartweave.html)
147. [Transaction Metadata (Tags)](https://cookbook.arweave.net/concepts/tags.html)
148. [Arseeding js - Deploying manifests](https://cookbook.arweave.net/guides/deploying-manifests/arseeding-js.html)
149. [Developing on the Permaweb](https://cookbook.arweave.net/getting-started/welcome.html)
150. [arkb](https://cookbook.arweave.net/guides/deployment/arkb.html)
151. [ServerSide DNS Integration](https://cookbook.arweave.net/guides/dns-integration/server-side.html)
152. [Posting a Transaction using Dispatch](https://cookbook.arweave.net/guides/posting-transactions/dispatch.html)
153. [Posting Transactions using arseedingjs](https://cookbook.arweave.net/guides/posting-transactions/arseeding-js.html)
154. [Posting Transactions using Ardrive Turbo](https://cookbook.arweave.net/guides/posting-transactions/turbo.html)
155. [Warp (SmartWeave) SDK - Deploying Contracts](https://cookbook.arweave.net/guides/smartweave/warp/deploying-contracts.html)
156. [Warp (SmartWeave) SDK - Evolve](https://cookbook.arweave.net/guides/smartweave/warp/evolve.html)
157. [Warp (SmartWeave) SDK - ReadState](https://cookbook.arweave.net/guides/smartweave/warp/readstate.html)
158. [Warp WriteInteractions](https://cookbook.arweave.net/guides/smartweave/warp/write-interactions.html)
159. [Vouch](https://cookbook.arweave.net/guides/vouch.html)
160. [Warp (SmartWeave) SDK Intro](https://cookbook.arweave.net/guides/smartweave/warp/intro.html)
161. [React Starter Kits](https://cookbook.arweave.net/kits/react/index.html)
162. [SvelteVite Starter Kit](https://cookbook.arweave.net/kits/svelte/vite.html)
163. [Vue Starter Kits](https://cookbook.arweave.net/kits/vue/index.html)
164. [Svelte Starter Kits](https://cookbook.arweave.net/kits/svelte/index.html)
165. [ANT Configuration](https://docs.ar.io/ar-io-sdk/ants/configuration)
166. [ARIO Configuration](https://docs.ar.io/ar-io-sdk/ario/configuration)
167. [getArNSReservedNames](https://docs.ar.io/ar-io-sdk/ario/arns/get-arns-reserved-names)
168. [getArNSRecords](https://docs.ar.io/ar-io-sdk/ario/arns/get-arns-records)
169. [Hello World (No Code)](https://cookbook.arweave.net/getting-started/quick-starts/hw-no-code.html)
170. [getGateways](https://docs.ar.io/ar-io-sdk/ario/gateways/get-gateways)
171. [process10](https://hyperbeam.arweave.net/build/devices/process-at-1-0.html)
172. [approvePrimaryNameRequest](https://docs.ar.io/ar-io-sdk/ants/approve-primary-name-request)
173. [BetterIDEa](https://cookbook_ao.arweave.net/references/betteridea/index.html)
174. [addController](https://docs.ar.io/ar-io-sdk/ants/add-controller)
175. [cancelWithdrawal](https://docs.ar.io/ar-io-sdk/ario/gateways/cancel-withdrawal)
176. [upgradeRecord](https://docs.ar.io/ar-io-sdk/ario/arns/upgrade-record)
177. [Release name - Ants](https://docs.ar.io/ar-io-sdk/ants/release-name)
178. [getTokenCost](https://docs.ar.io/ar-io-sdk/ario/arns/get-token-cost)
179. [ariowayfinder-react](https://docs.ar.io/wayfinder/react)
180. [setKeywords](https://docs.ar.io/ar-io-sdk/ants/set-keywords)
181. [transfer](https://docs.ar.io/ar-io-sdk/ants/transfer)
182. [Network - Gateway providers](https://docs.ar.io/wayfinder/core/gateway-providers/network)
183. [Module hb_singletonerl](https://hyperbeam.arweave.net/build/devices/source-code/hb_singleton.html)
184. [Permaweb Cookbook - Guides](https://cookbook.arweave.net/guides/index.html)
185. [Get delegations - Gateways](https://docs.ar.io/ar-io-sdk/ario/gateways/get-delegations)
186. [Get demand factor - Arns](https://docs.ar.io/ar-io-sdk/ario/arns/get-demand-factor)
187. [Leave network - Gateways](https://docs.ar.io/ar-io-sdk/ario/gateways/leave-network)
188. [Get arns returned names - Arns](https://docs.ar.io/ar-io-sdk/ario/arns/get-arns-returned-names)
189. [getBalance](https://docs.ar.io/ar-io-sdk/ants/get-balance)
190. [Get redelegation fee - Gateways](https://docs.ar.io/ar-io-sdk/ario/gateways/get-redelegation-fee)
191. [getArNSReservedName](https://docs.ar.io/ar-io-sdk/ario/arns/get-arns-reserved-name)
192. [Get gateway delegates - Gateways](https://docs.ar.io/ar-io-sdk/ario/gateways/get-gateway-delegates)
193. [getDemandFactorSettings](https://docs.ar.io/ar-io-sdk/ario/arns/get-demand-factor-settings)
194. [Simple cache - Gateway providers](https://docs.ar.io/wayfinder/core/gateway-providers/simple-cache)
195. [Get allowed delegates - Gateways](https://docs.ar.io/ar-io-sdk/ario/gateways/get-allowed-delegates)
196. [getRecords](https://docs.ar.io/ar-io-sdk/ants/get-records)
197. [getRegistrationFees](https://docs.ar.io/ar-io-sdk/ario/arns/get-registration-fees)
198. [getState](https://docs.ar.io/ar-io-sdk/ants/get-state)
199. [getInfo](https://docs.ar.io/ar-io-sdk/ants/get-info)
200. [getArNSRecord](https://docs.ar.io/ar-io-sdk/ario/arns/get-arns-record)
201. [Get arns returned name - Arns](https://docs.ar.io/ar-io-sdk/ario/arns/get-arns-returned-name)
202. [Module dev_metaerl](https://hyperbeam.arweave.net/build/devices/source-code/dev_meta.html)
203. [Module dev_snperl](https://hyperbeam.arweave.net/build/devices/source-code/dev_snp.html)
204. [Exposing Process State to HyperBEAM](https://cookbook_ao.arweave.net/guides/migrating-to-hyperbeam/exposing-process-state.html)
205. [Installing aos](https://cookbook_ao.arweave.net/guides/aos/installing.html)
206. [Reassign name - Ants](https://docs.ar.io/ar-io-sdk/ants/reassign-name)
207. [Remove primary names - Ants](https://docs.ar.io/ar-io-sdk/ants/remove-primary-names)
208. [removeRecord](https://docs.ar.io/ar-io-sdk/ants/remove-record)
209. [Set logo - Ants](https://docs.ar.io/ar-io-sdk/ants/set-logo)
210. [Instant withdrawal - Gateways](https://docs.ar.io/ar-io-sdk/ario/gateways/instant-withdrawal)
211. [Decrease operator stake - Gateways](https://docs.ar.io/ar-io-sdk/ario/gateways/decrease-operator-stake)
212. [Increase operator stake - Gateways](https://docs.ar.io/ar-io-sdk/ario/gateways/increase-operator-stake)
213. [getBalances](https://docs.ar.io/ar-io-sdk/ants/get-balances)
214. [getHandlers](https://docs.ar.io/ar-io-sdk/ants/get-handlers)
215. [removeUndernameRecord](https://docs.ar.io/ar-io-sdk/ants/remove-undername-record)
216. [getGatewayVaults](https://docs.ar.io/ar-io-sdk/ario/gateways/get-gateway-vaults)
217. [Remove controller - Ants](https://docs.ar.io/ar-io-sdk/ants/remove-controller)
218. [getGateway](https://docs.ar.io/ar-io-sdk/ario/gateways/get-gateway)
219. [Decrease delegate stake - Gateways](https://docs.ar.io/ar-io-sdk/ario/gateways/decrease-delegate-stake)
220. [Increase delegate stake - Gateways](https://docs.ar.io/ar-io-sdk/ario/gateways/increase-delegate-stake)
221. [json10](https://hyperbeam.arweave.net/build/devices/json-at-1-0.html)
222. [Building Devices](https://hyperbeam.arweave.net/build/devices/building-devices.html)
223. [lua53a](https://hyperbeam.arweave.net/build/devices/lua-at-5-3a.html)
224. [message10](https://hyperbeam.arweave.net/build/devices/message-at-1-0.html)
225. [Module dev_fafferl](https://hyperbeam.arweave.net/build/devices/source-code/dev_faff.html)
226. [Module dev_codec_structurederl](https://hyperbeam.arweave.net/build/devices/source-code/dev_codec_structured.html)
227. [Module dev_cronerl](https://hyperbeam.arweave.net/build/devices/source-code/dev_cron.html)
228. [Module dev_patcherl](https://hyperbeam.arweave.net/build/devices/source-code/dev_patch.html)
229. [Module dev_stackerl](https://hyperbeam.arweave.net/build/devices/source-code/dev_stack.html)
230. [Intro to HyperBEAM](https://hyperbeam.arweave.net/build/introduction/what-is-hyperbeam.html)
231. [FAQ](https://hyperbeam.arweave.net/build/reference/faq.html)
232. [Troubleshooting](https://hyperbeam.arweave.net/build/reference/troubleshooting.html)
233. [Fuel Your LLM](https://hyperbeam.arweave.net/llms.html)
234. [Glossary](https://hyperbeam.arweave.net/build/reference/glossary.html)
235. [Glossary](https://hyperbeam.arweave.net/run/reference/glossary.html)
236. [How ao messaging works](https://cookbook_ao.arweave.net/concepts/how-it-works.html)
237. [Concepts](https://cookbook_ao.arweave.net/concepts/index.html)
238. [Processes](https://cookbook_ao.arweave.net/concepts/processes.html)
239. [Sending an Assignment to a Process](https://cookbook_ao.arweave.net/guides/aoconnect/assign-data.html)
240. [Connecting to specific ao nodes](https://cookbook_ao.arweave.net/guides/aoconnect/connecting.html)
241. [Units](https://cookbook_ao.arweave.net/concepts/units.html)
242. [Calling DryRun](https://cookbook_ao.arweave.net/guides/aoconnect/calling-dryrun.html)
243. [Reading results from an ao Process](https://cookbook_ao.arweave.net/guides/aoconnect/reading-results.html)
244. [Chatroom Blueprint](https://cookbook_ao.arweave.net/guides/aos/blueprints/chatroom.html)
245. [Understanding the Inbox](https://cookbook_ao.arweave.net/guides/aos/inbox-and-handlers.html)
246. [Introduction](https://cookbook_ao.arweave.net/guides/aos/intro.html)
247. [Load Lua Files with load filename](https://cookbook_ao.arweave.net/guides/aos/load.html)
248. [Base64](https://cookbook_ao.arweave.net/guides/aos/modules/base64.html)
249. [crypto](https://cookbook_ao.arweave.net/guides/aos/modules/crypto.html)
250. [Creating a Pingpong Process in aos](https://cookbook_ao.arweave.net/guides/aos/pingpong.html)
251. [Utils](https://cookbook_ao.arweave.net/guides/aos/modules/utils.html)
252. [Customizing the Prompt in aos](https://cookbook_ao.arweave.net/guides/aos/prompt.html)
253. [Connecting to HyperBEAM with aos](https://cookbook_ao.arweave.net/guides/migrating-to-hyperbeam/aos-with-hyperbeam.html)
254. [Why Migrate to HyperBEAM](https://cookbook_ao.arweave.net/guides/migrating-to-hyperbeam/why-migrate.html)
255. [ao Module](https://cookbook_ao.arweave.net/references/ao.html)
256. [Cron Messages](https://cookbook_ao.arweave.net/references/cron.html)
257. [Handlers (Version 005)](https://cookbook_ao.arweave.net/references/handlers.html)
258. [References](https://cookbook_ao.arweave.net/references/index.html)
259. [Release Notes](https://cookbook_ao.arweave.net/releasenotes/index.html)
260. [Lua Optimization Guide for AO Platform](https://cookbook_ao.arweave.net/references/lua-optimization.html)
261. [Messaging in ao](https://cookbook_ao.arweave.net/tutorials/begin/messaging.html)
262. [Building a Chatroom in aos](https://cookbook_ao.arweave.net/tutorials/begin/chatroom.html)
263. [Begin An Interactive Tutorial](https://cookbook_ao.arweave.net/tutorials/begin/index.html)
264. [Interpreting Announcements](https://cookbook_ao.arweave.net/tutorials/bots-and-games/announcements.html)
265. [Fetching Game State](https://cookbook_ao.arweave.net/tutorials/bots-and-games/game-state.html)
266. [Expanding the Arena](https://cookbook_ao.arweave.net/tutorials/bots-and-games/build-game.html)
267. [Welcome to ao](https://cookbook_ao.arweave.net/welcome/index.html)
268. [AO Processes](https://cookbook_ao.arweave.net/welcome/ao-processes.html)
269. [Legacynet HyperBEAM](https://cookbook_ao.arweave.net/welcome/legacynet-info/index.html)
270. [ARIO Documentation](https://docs.ar.io/)
271. [Set description - Ants](https://docs.ar.io/ar-io-sdk/ants/set-description)
272. [Set name - Ants](https://docs.ar.io/ar-io-sdk/ants/set-name)
273. [Set ticker - Ants](https://docs.ar.io/ar-io-sdk/ants/set-ticker)
274. [Getting started - Ar io sdk](https://docs.ar.io/ar-io-sdk/getting-started)
275. [Gateway Apex Domain Content Resolution](https://docs.ar.io/gateways/apex)
276. [Admin - Gateways](https://docs.ar.io/gateways/admin)
277. [Gateway ArNS Resolution](https://docs.ar.io/gateways/arns-resolution)
278. [Quick Start Guides](https://docs.ar.io/guides)
279. [Introduction](https://docs.ar.io/introduction)
280. [Staking](https://docs.ar.io/staking)
281. [ARIO Network Composition](https://docs.ar.io/network-composition)
282. [Round robin - Routing strategies](https://docs.ar.io/wayfinder/core/routing-strategies/round-robin)
283. [Preferred with fallback - Routing strategies](https://docs.ar.io/wayfinder/core/routing-strategies/preferred-with-fallback)
284. [Random - Routing strategies](https://docs.ar.io/wayfinder/core/routing-strategies/random)
285. [Fastest ping - Routing strategies](https://docs.ar.io/wayfinder/core/routing-strategies/fastest-ping)
286. [Data Root Verification Strategy](https://docs.ar.io/wayfinder/core/verification-strategies/data-root-verification)
287. [StaticRoutingStrategy](https://docs.ar.io/wayfinder/core/routing-strategies/static)
288. [Verification Strategies](https://docs.ar.io/wayfinder/core/verification-strategies)
289. [Bundling Services](https://cookbook.arweave.net/concepts/bundlers.html)
290. [Transaction Bundles](https://cookbook.arweave.net/concepts/bundles.html)
291. [Gateways](https://cookbook.arweave.net/concepts/gateways.html)
292. [Overview](https://cookbook.arweave.net/concepts/psts.html)
293. [Transaction Types](https://cookbook.arweave.net/concepts/transaction-types.html)
294. [Cooking with the Permaweb](https://cookbook.arweave.net/getting-started/index.html)
295. [Vouch](https://cookbook.arweave.net/concepts/vouch.html)
296. [Contributing Workflow](https://cookbook.arweave.net/getting-started/contributing.html)
297. [Search Indexing Service](https://cookbook.arweave.net/guides/querying-arweave/search-indexing-service.html)
298. [Cooking with the Permaweb](https://cookbook.arweave.net/index.html)
299. [Hash Verification Strategy](https://docs.ar.io/wayfinder/core/verification-strategies/hash-verification)
300. [ao](https://cookbook_ao.arweave.net/guides/aos/modules/ao.html)
301. [Building ao Processes](https://hyperbeam.arweave.net/build/building-on-ao.html)
302. [JSON](https://cookbook_ao.arweave.net/guides/aos/modules/json.html)
303. [LLMs Documentation](https://cookbook_ao.arweave.net/llms-explanation.html)
304. [Eval](https://cookbook_ao.arweave.net/concepts/eval.html)
305. [Creating and Deploying Manifests](https://cookbook.arweave.net/guides/deploying-manifests/deployingManifests.html)
306. [Monitoring Cron](https://cookbook_ao.arweave.net/guides/aoconnect/monitoring-cron.html)
307. [Gateway providers - Core](https://docs.ar.io/wayfinder/core/gateway-providers)
308. [Ardrive - Deploying manifests](https://cookbook.arweave.net/guides/deploying-manifests/ardrive.html)
309. [Static - Gateway providers](https://docs.ar.io/wayfinder/core/gateway-providers/static)
310. [Schema diagrams - Arfs](https://cookbook.arweave.net/concepts/arfs/schema-diagrams.html)
311. [Permaweb Cookbook - Core Concepts](https://cookbook.arweave.net/concepts/index.html)
312. [Permaweb Cookbook - References](https://cookbook.arweave.net/references/index.html)
313. [Tutorials](https://cookbook_ao.arweave.net/tutorials/index.html)
314. [Arweave app - Deploying manifests](https://cookbook.arweave.net/guides/deploying-manifests/arweave-app.html)
315. [Pretty](https://cookbook_ao.arweave.net/guides/aos/modules/pretty.html)
316. [Installing ao connect](https://cookbook_ao.arweave.net/guides/aoconnect/installing-connect.html)
317. [aoconnect](https://cookbook_ao.arweave.net/guides/aoconnect/aoconnect.html)
318. [Blueprints](https://cookbook_ao.arweave.net/guides/aos/blueprints/index.html)
319. [LLMstxt](https://cookbook.arweave.net/references/llms.html)
320. [Glossary](https://cookbook.arweave.net/references/glossary.html)
321. [Permaweb Cookbook - Legacy](https://cookbook.arweave.net/legacy/index.html)
322. [Permaweb Cookbook - Community](https://cookbook.arweave.net/community/index.html)
323. [Starter Kits](https://cookbook.arweave.net/kits/index.html)
324. [Routing Strategies](https://docs.ar.io/wayfinder/core/routing-strategies)

### Excluded Documents (Quality Filtered)

1. https://cookbook_ao.arweave.net/guides/aos/modules/index.html

---

# 1. Glossary

Document Number: 1
Source: https://glossary.arweave.net/glossary.txt
Words: 9868
Extraction Method: plain-text

ANS-104: A standard for bundled data on Arweave, allowing for efficient storage of multiple pieces of data in a single transaction. It forms the basis for scalable data storage and retrieval on the network.
Messaging Patterns: Communication frameworks in AO that define how processes exchange information through message passing. These patterns include asynchronous fire-and-forget messaging (ao.send), request-response cycles (ao.send().receive()), message forwarding chains (msg.forward), and various blocking and non-blocking communication models. Messaging patterns provide structured approaches for inter-process communication, enabling complex distributed systems to coordinate effectively.
ao.send: A non-blocking function in AO for asynchronous message sending that enables fire-and-forget communication between processes. It returns immediately after sending, allowing the sending process to continue execution without waiting for a response. The function returns a promise-like object that can be chained with .receive() for request-response patterns when needed.
ao.send().receive(): A blocking message pattern in AO that combines sending a message and waiting for a specific reply, enabling request-response cycles between processes. It only matches messages linked by X-Reference and can specify a target process ID to indicate which process will reply. This pattern is essential for synchronous interactions in the otherwise asynchronous AO environment.
msg.reply: A non-blocking function in AO handlers used to respond to incoming messages with automatic reference tracking. It enables asynchronous request-response patterns by automatically linking the response to the original message via X-Reference and setting the Target to the original sender or Reply-To address if specified.
msg.forward: A non-blocking function in AO for message routing across multiple processes that creates a sanitized copy of the original message. It preserves Reply-To and X-Reference properties for complete message tracking and sets X-Origin to the original sender, enabling final services to reply directly to the originator. This function is key for creating multi-step processing pipelines (A → B → C → A patterns).
Receive: A blocking function in AO (with capital R) that s execution until any message matching a specified pattern arrives from any sender. Unlike the lowercase .receive() method, it's not bound to a specific conversation and can match messages from any process, making it useful for synchronous message processing flows or event listening.
Handlers.utils.reply: A utility function in AO that creates a handler function that automatically replies with a fixed response. It serves as a wrapper around msg.reply for common use cases, enabling concise creation of simple response handlers without writing full handler functions.
Reference: A unique identifier automatically assigned to each message in AO that enables tracking message history and relationships. References form the foundation of AO's message correlation system, allowing processes to maintain conversation context across asynchronous communications.
Reply-To: A message property in AO that explicitly specifies the destination for responses, overriding the default behavior of replying to the original sender. This enables more flexible routing patterns where responses can be directed to different processes than the message originator.
X-Reference: A special message property in AO that maintains conversation chains across replies and forwards. It links related messages together in a conversation, enabling processes to track which response corresponds to which request, even in complex multi-step processing pipelines.
X-Origin: A message property in AO that tracks the original sender of a message through forwarding chains. It enables final services in a processing pipeline to reply directly to the conversation originator without knowing its specific identity in advance, facilitating multi-step processing workflows.
Blocking Communication: A messaging approach in AO where the sending process s execution until a response is received. This pattern is implemented through functions like Receive and ao.send().receive(), creating synchronous interaction points in an otherwise asynchronous system. Blocking is useful for scenarios requiring guaranteed order of operations or direct response handling.
Non-blocking Communication: A messaging approach in AO where the sending process continues execution immediately after sending a message, without waiting for responses. Implemented through functions like ao.send, msg.reply, and msg.forward, this pattern enables parallel processing and higher throughput in distributed systems by avoiding execution s.
Process: A computational unit in AO that can receive and send messages. Processes are represented by a log of interacting messages stored on Arweave, as well as an initialization data item.
Message: The fundamental unit of communication in AO and the AO Core Protocol. In AO, messages are ANS-104 compliant data items that can be sent between processes or from users to processes. In the AO Core Protocol, every item on the permaweb is described as a Message, interpretable as either a map of named functions or as a concrete binary term. Messages can be called through the creation of another message, providing a map of arguments to the execution. Both systems use messages as the primary mechanism for communication and state management.
Handler: A function in an AO process that responds to specific message patterns. Handlers are registered to match particular message patterns and execute code when matching messages are received.
Resolvers: Special tables in AO handlers where each key is a pattern matching table and its corresponding value is a function that executes when that pattern matches. Resolvers enable conditional execution based on additional pattern matching, creating switch/case-like structures where different functions are triggered based on which pattern matches the incoming message.
Pattern Matching Tables: Declarative structures in AO that define how to match incoming messages based on their attributes. These tables support simple tag matching, wildcard matching with underscore ('_'), Lua string pattern matching (similar to regex), and function-based validation. They provide a flexible way to filter messages for processing by specific handlers.
Handlers.utils.hasMatchingData: A utility function in AO that returns a pattern matching function for checking if a message's Data field contains a specified string. This helper simplifies creating handlers that respond to specific message content rather than just tags, enabling content-based message routing and processing.
Handlers.utils.hasMatchingTag: A utility function in AO that returns a pattern matching function for checking if a message has a tag with a specified name and value. This helper enables precise matching of messages based on their tag attributes, facilitating targeted message handling and routing in AO processes.
Handlers.add: A core function in AO for registering message handlers that takes a name, pattern, and handler function as arguments. It adds or updates a handler in the process's handler list, enabling processes to respond to specific message patterns. The pattern can be a table, function, or string (in AOS 2.0+), and the handler can be a function or resolver table for conditional execution.
Handlers.once: A specialized handler registration function in AO that creates a handler which runs only once when its pattern is matched, then is automatically removed. This is equivalent to setting maxRuns = 1 and is useful for one-time initialization or handling unique events that should trigger only a single response.
Handler Execution Flow: The process by which AO determines which handlers to run when a message arrives. Handlers are executed in sequential order as they appear in the Handlers.list, with pattern functions determining whether to: skip the handler (false), process the message and continue checking subsequent handlers ("continue"), or process the message and checking further handlers ("break"). This flow control enables sophisticated message processing pipelines within AO processes.
ArFS: A data modeling, storage, and retrieval protocol designed to emulate common file system operations on Arweave's permanent storage. ArFS solves key challenges by implementing a hierarchical file structure, metadata management, file permissions (public and private with encryption), file versioning, data deduplication, search capabilities, and interoperability with other decentralized applications. It works around Arweave's immutable nature by using an append-only transaction data model with tags in Arweave Transaction headers, enabling features like file renaming and organization despite the underlying permanent storage.
Spawn: The action of creating a new process in AO. Processes can spawn other processes, creating a hierarchical relationship.
Actor Model: The computational paradigm that AO is based on, where each process (actor) is an independent unit with its own state that communicates exclusively through message passing.
HyperBEAM: The distributed execution engine that powers AO, built on Erlang/OTP. It provides distributed computation, deterministic execution, message scheduling, and WASM runtime capabilities.
Arweave Wallet Kit: A modular toolkit that simplifies interactions between Arweave wallets and dApps by providing a unified API that supports any Arweave wallet. It's built around a core package as the foundation, with additional packages for React hooks, components, and styles. The kit uses 'strategies' as modular implementations for different wallet providers, currently supporting Wander.app, Arweave.app, Othent, and general browser wallets. This architecture enables users to interact with apps using their preferred wallet while giving developers a consistent interface across all wallet types.
Compute Unit: A node in the AO network responsible for executing process code and managing state. CUs offer the service of resolving process state in competition with one another.
Scheduler Unit: A node in the AO network responsible for the single assignment of atomically incrementing slot numberings to messages sent to a process, ensuring deterministic ordering.
Messenger Unit: A node in the AO network that relays messages around the system, serving as the entry point for users to interact with processes within AO.
Cron: A time-based job scheduler in AO that enables processes to execute functions on a schedule, allowing for autonomous contract execution.
Holographic State: A state management approach in AO where each process maintains its state independently of other processes, represented by a log of messages stored on Arweave. This independence allows processes to operate and be evaluated separately, facilitating faster interactions and improved scalability. The state is considered 'holographic' because it's implied by the associated message log rather than being explicitly stored, enabling true parallel processing while maintaining verifiability.
AOS: The AO operating system, which serves as a reference model for software creation on AO. It allows developers to initiate processes, install packages, and edit code within a live environment.
Arweave: A decentralized storage network that allows users to store data permanently and sustainably. It serves as the foundation layer for the permaweb ecosystem and has its own native cryptocurrency called AR Token that powers the network's economic model.
Blockweave: Arweave's innovative data structure that improves upon traditional blockchain technology. Unlike sequential blockchain verification, it requires only random data verification for new additions, creating a 3D spider web-like structure. This approach dramatically reduces energy consumption while maintaining data immutability. The blockweave supports petabyte-scale storage and features continuous data validation (over 5,670 validations daily). It includes advanced features like bundled transactions (up to 1000 per block) and incentivized data replication, enabling efficient storage and verification of large datasets.
Permaweb: A permanent, decentralized web built on top of the Arweave network, allowing for the creation of websites and applications that are accessible forever. It represents the user-facing layer of the Arweave ecosystem.
AR.IO: The first permanent cloud network. No 404s, no lost dependencies, no subscriptions - just reliable access to applications and data for site and app hosting.
AR.IO Gateway: A server that provides access to data stored on the Arweave network. Part of the AR.IO network's decentralized infrastructure for serving permanent data and managing network traffic.
Bundling: A feature in Arweave that groups multiple data transactions together, processing them as Layer-1 transactions to improve efficiency and reduce costs. This is implemented through the ANS-104 standard.
AO Token: The native token of the AO ecosystem, used for resource allocation, governance, and payments within the network. One AO is equal to one trillion (1,000,000,000,000) Armstrongs, which is the smallest unit of the token. When specifying token quantities in transactions, the amount is typically expressed in Armstrongs rather than AO (e.g., 1,000,000 Armstrongs = 0.0000001 AO).
Pi Token (π): A token in the AO ecosystem representing ownership of key permaweb assets, including AO, AR, and fair-launch projects, serving as a default means of exchange.
AO-SQLite: A module for AO that combines the AO operating system with SQLite to create a lightweight but powerful indexer for the AOS experience.
ArFleet: A framework in AO designed to facilitate the purchase of time-limited data storage from permissionless peers without requiring third-party enforcement.
Trusted Execution Environment: A secure area of a processor in AO that guarantees code and data loaded inside is protected with respect to confidentiality and integrity, similar to a CPU with the security of a hardware wallet.
ArNS: A decentralized domain name system for the permaweb.
Assignments: A mechanism in AO that enables processes to access and share data from Arweave. Assignments allow processes to request specific data items by their transaction IDs and distribute messages to multiple processes. They are implemented through the ao module's assign and send functions, supporting features like data loading and cross-process message sharing.
ao Module: A core library in AO that provides essential functionality for process communication and management. It includes functions for sending messages (ao.send), spawning processes (ao.spawn), managing assignments, and handling process state. The module maintains process environment information and manages message routing through an outbox system.
AO Computer: The AO Computer is an actor-oriented machine on the Arweave network, creating a unified computing environment across diverse nodes. It supports many parallel processes through an open message-passing layer, linking independent processes into a cohesive system, similar to how websites are interconnected via hyperlinks.
Arweave Name Token: A token based on the AO Computer that is connected to each registered ArNS Name. Each ANT gives the owner the ability to update the subdomains and Arweave Transaction IDs used by the registered name as well as transfer ownership and other functions.
Arweave Network Standards: Drafts and finalized standards for data formats, tag formats, data protocols, custom gateway features and anything that is built on top the Arweave Network. Specific standards are denoted by an associated number, e.g., ANS-###.
Base Layer Transaction: One of up to 1,000 transactions that make up a single Arweave block. These transactions form the foundation of the Arweave network's data structure and can contain bundled data items.
Bundled Data Item: A data item or transaction nested within an ANS-104 bundled transaction. These items enable efficient storage and retrieval of multiple data pieces within the Arweave network.
Bundler: A third-party service and gateway feature that bundles data files on a user's behalf.
Chunk: A unit of data that is stored on the Arweave network. It represents a piece of a larger file that has been split into smaller, manageable segments for efficient storage and retrieval.
Epoch: A specific duration (e.g., one day) during which network activities and evaluations are conducted. It serves as a key time frame for processes such as observation duties, performance assessments, and reward distributions within the network's protocols.
Eval: An onboard handler in AO processes that evaluates new code and verifies message origin. It processes incoming code, determines appropriate actions, and can be manually triggered to evaluate the Data field from messages. The Eval handler is also used internally by functions like .load to evaluate file contents.
Gateway Address Registry: A decentralized directory maintained in the AR.IO smart contract. It serves as the authoritative list of all registered gateways on the AR.IO Network, facilitating discovery, health monitoring, and data sharing.
Indexing: The act of organizing transaction data tags into queryable databases.
Layer 2 Infrastructure: Technology / infrastructure stack built 'above' a base layer. In this use, the AR.IO Network would be considered Layer 2 infrastructure to the base Arweave protocol.
Manifest: Special 'aggregate' files uploaded to Arweave that map user-definable sub-paths with other Arweave transaction IDs. This allows users to create logical groups of content, for example a directory of related files, or the files and assets that make up a web page or application.
Mempool: Short for 'memory pool,' is a component of Arweave mining nodes that temporarily stores valid transactions that have been broadcasted to the network but have not yet been added to a block.
Native Address: The way public addresses are commonly (or by spec) represented in their native blockchain. Arweave keys are 43 character base64url representations of the public key, while Ethereum keys use a different hashing algorithm and start with 0x etc.
Normalized Address: 43 character base64url representation of the sha256 hash of a public key. Public keys for other chains can be normalized by this representation.
Observer: A gateway selected to evaluate the performance of peer gateways in resolving ArNS names. Observers assess and report on the operational efficacy of other gateways.
Optimistic Indexing: Indexing transaction or data item headers before the associated L1 transaction has been accepted and confirmed in a chain block.
Period: A predefined time span (e.g., a day) that serves as a cycle for network activities such as dynamic pricing. It is a fundamental unit of time for operational and protocol processes within the network.
Permanent Cloud Network: A decentralized network that securely stores, distributes, and serves data and applications in a timeless, tamper-proof, and universally accessible way. Unlike traditional clouds, it ensures data permanence and user sovereignty by eliminating reliance on centralized providers and creating a resilient, censorship-resistant infrastructure.
Process ID: Every process in AO is assigned a unique immutable identifier code.
Protocol Balance: The primary sink and source of ARIO tokens circulating through the AR.IO Network. This balance functions as a programmatically encoded vault in the network's smart contract for managing ArNS revenue and incentive rewards distribution.
Protocol Rewards: ARIO Token incentive rewards distributed by the AR.IO protocol to eligible users and gateway operators, forming part of the network's economic incentive structure.
Public Key: The publicly known keys for a signer (wallet). Public keys are different byte lengths depending on the signer type (e.g. Arweave vs. Ethereum (ECDSA), vs Solana, etc.)
Seeding: The act of propagating new data throughout the network. Miner nodes seed Arweave base layer transaction data to other miners, while gateways ensure that the transactions they receive reach the Arweave nodes. Both gateways and Arweave nodes seed base layer transactions and data chunks.
Staking: The process of locking ARIO tokens into a protocol-facilitated vault, temporarily removing them from circulation until unlocked. This action represents an opportunity cost for the gateway operator and serves as a motivator to prioritize the network's collective interests.
Stake Redelegation: The process by which stakers move their delegated tokens from one gateway to another.
Stake Redemption: A feature allowing stakers to use their staked tokens for ArNS-related activities, such as purchasing names, extending leases, or increasing undername capacity.
Transaction ID: Every transaction and data file uploaded to Arweave is assigned a unique identifier code known as the Transaction ID.
Trust-minimization: Relates to enacting network security by minimizing the number of entities and the degree to which they must be trusted to achieve reliable network interactions. A network with trust-minimizing mechanisms means that it has reduced exposure to undesirable third-party actions and built-in incentives to reward good behavior while punishing bad behavior.
Vault: Token vaults are protocol level mechanisms used to contain staked tokens over time. Each vault contains a starting timestamp, ending timestamp (if applicable), along with a balance of tokens.
Wayfinder Protocol: The Wayfinder protocol provides applications with a pattern for dynamically switching / routing between network gateways. It also allows for abstraction of top level domain names from Arweave data and verifies the responses from AR.IO Gateways. It forms the basis of the ar:// schema, so users can seamlessly access ArNS names, Arweave base layer transactions, and bundled data items without the user providing a top-level domain.
Bootloader: A feature in AO that enables users to include a script to evaluate when spawning a process. The script can be included either with the Data property or with a txId specified on the On-Boot Tag.
Coroutine: A programming feature in AO similar to async/await or generators, allowing processes to execution (yield) and resume later. Used for handling asynchronous operations and message passing.
Handler Pattern: A mechanism in AO for matching and processing incoming messages based on their attributes. Can be defined using strings, tables, or functions to match message properties like Action tags.
Resolver: A special table in AO handlers where each key is a pattern matching table and its corresponding value is a function that executes when that pattern matches, enabling conditional execution based on message attributes.
WeaveDrive: A protocol (AOP-5) that provides virtual file system support for AO processes to efficiently read data directly from Arweave. It enables processes to access and manipulate data stored on Arweave through a standardized file system interface, with support for lazy loading, caching, and deterministic data access patterns. The protocol includes mechanisms for data attestation, availability verification, and process boot loading.
Attestor: An Arweave wallet address authorized to create attestation data items that grant access to specific Arweave data items using WeaveDrive.
Token Subledger: A process type in AO that implements the full messaging protocol of token contracts and enables splitting tokens from a parent process into a child process while maintaining fungibility.
Message Reference: A unique identifier assigned to messages in AO that enables tracking message chains and responses across processes, facilitating request-response patterns and message forwarding.
Message Forward: A mechanism in AO that enables passing messages between multiple processes while maintaining reference chains and origin information, useful for creating multi-step processing pipelines.
Assignment Check: A security feature in AO that provides whitelisted assignment protection for processes by verifying the trustworthiness of message assignments.
AO Environment: The core runtime environment in AO that manages process state, message handling, and execution context. It includes process identification, module references, authorities, and outbox management for coordinating process communication.
Process Authority: A security mechanism in AO that defines which entities (identified by their addresses) are trusted to interact with a process. Authorities can send messages that will be accepted and processed by the process.
Process Outbox: A component of an AO process that temporarily stores outgoing messages, spawns, and assignments before they are processed by the network. The outbox manages the process's communication with other processes.
Message Tags: Metadata attached to AO messages that provide context and instructions for message processing. Tags can be extractable or non-extractable, and forwardable or non-forwardable, affecting how messages are processed and routed.
Message Normalization: The process of standardizing message format in AO by extracting tags into a consistent structure and ensuring proper handling of message attributes. This enables reliable message processing across the network.
Message Sanitization: A security feature in AO that removes non-forwardable tags from messages before forwarding, ensuring that sensitive or system-specific information is not inappropriately propagated through the network.
Process Inbox: A component of an AO process that stores incoming messages for processing. The inbox has a maximum capacity and implements overflow protection to prevent memory issues while maintaining message order.
Process Module: A reusable component in AO that defines the behavior and capabilities of a process. Modules can be referenced and reused across different processes, providing standardized functionality and interfaces.
Process Result: The output of an AO process execution, including any generated messages, spawned processes, assignments, and error information. Results are used to coordinate process communication and manage state changes.
Single System Image: A unified computing environment in AO that creates a cohesive system across diverse nodes in a distributed network. This allows processes to operate independently while maintaining a unified user experience, similar to how websites on different servers are connected via hyperlinks.
Stake-Exclusivity Period: A time window in AO during which staked collateral is exclusively reserved for a specific message transmission, preventing it from being reused. This mechanism ensures stake availability for potential slashing if discrepancies are found in the message.
Time Value of Stake: The economic cost in AO associated with locking stake capital for a specific duration to secure message transmission. This value is calculated based on the opportunity cost of the locked capital and expected annual return rates.
Permissionless Ecosystem Funding: A token issuance mechanism in AO that allows developers to fund their applications by receiving AO tokens from users who bridge assets into their protocols. The funding rate automatically adjusts with the token emission curve.
Permaweb Ecosystem Development Guild: An alliance of organizations and builders in AO that develop and maintain core infrastructure, funded through native yield generated by bridged assets. The funding naturally decreases with token emission rate.
Pushing: The process in AO where Messenger Units relay messages through the network, coordinating with Scheduler Units for message ordering and Compute Units for state calculation. This continues recursively until there are no more messages to process.
State Attestation: A cryptographic assurance mechanism in AO that verifies the correctness of computations and states. In AO's state management, attestations are cryptographically signed statements by Compute Units verifying process state computation results. In the AO Core Protocol, attestations provide cryptographic assurance that verifies the correctness of an output, representing proof that a specific computation produced the expected result when combined with the message and hashpath. Both forms enable trustless verification without requiring full recomputation.
Bridge: A system that enables assets from other networks to be represented and used within AO. Bridges consist of smart contracts on both the native chain and AO network, generating derivative tokens that represent the bridged assets. They facilitate cross-chain interoperability and asset transfer between different blockchain networks.
AOX: A general-purpose bridge that facilitates the transfer of assets from traditional exchanges and Ethereum to the AO network. It enables users to bring their assets from external networks into the AO ecosystem, expanding the range of assets available for use within AO.
Quantum: A bridge developed by Astro Protocol that enables the transfer of AR tokens from the Arweave network to the AO network. It facilitates seamless movement of Arweave's native token into the AO ecosystem, enabling users to leverage their AR holdings within AO-based applications and protocols.
Cast Message: A type of message in AO that is sent to a Scheduler Unit without waiting for a response, enabling one-way communication patterns. This allows for more efficient message passing when immediate responses aren't needed.
aoconnect: A JavaScript/TypeScript library that enables interaction with the AO system from Node.js or browser environments. It provides a comprehensive set of tools and utilities for developers to interact with AO processes, send messages, and manage AO resources programmatically.
AO Dev-Cli: A command-line tool for building AO WebAssembly modules, supporting both Lua and C/C++ development. It provides a streamlined workflow for initializing projects, building modules, and deploying them to the AO network. The tool features configurable build settings, external library support, and predefined memory presets for different use cases.
aos console: A command-line interface tool for interacting with AO networks. It allows developers to deploy processes, send messages, and manage AO resources through terminal commands.
DataItemSigner: A function used to cryptographically sign data items before they're submitted to the AO network, ensuring authenticity and ownership.
Relay Operator: An entity that facilitates message transmission in the AO network, often requiring payment in the form of tokens to relay messages.
Tag System: The structured protocol for message routing in AO using key-value pairs. Common patterns include Action-Target-Quantity for token transfers and other operations.
Balances Implementation: The pattern for implementing token balances within AO processes, typically using a Lua table with wallet addresses as keys.
Blueprints: Templates in AOS that streamline the development of distributed applications by providing a framework for creating consistent and efficient processes across the AO network.
AR Token: The native cryptocurrency of the Arweave network used for transaction fees, data storage payments, and mining rewards. AR tokens are required to store data on the network, incentivize miners to maintain the blockweave, and participate in network governance. The token has a fixed maximum supply and its value is tied to the network's storage capacity. One AR is equal to one trillion (1,000,000,000,000) Winstons, which is the smallest unit of the token.
Atomic Assets: A unique data type on Arweave, linking data, metadata, and a smart contract under one identifier.
Arweave Gateway: Nodes that provide HTTP access to data stored on the Arweave network.
Proof of Access: Arweave's consensus mechanism that combines proof of work with proof of storage.
SmartWeave: A smart contract platform for Arweave that uses lazy-evaluation.
Succinct Proof of Random Access: Arweave's current mining mechanism that combines proof of work with proof of storage.
Universal Data License: Arweave's on-chain content licensing system.
Wallet: A software application that allows users to store, send, and receive AR tokens, as well as interact with Arweave applications.
Warp: A smart contract execution environment for Arweave.
Weave: The collective data stored on the Arweave network.
Wildfire: Arweave's self-sustaining network participation reward system.
AO Package Manager: A package manager for AO, enabling developers to manage and distribute AO modules and dependencies.
AO Ventures: A venture capital firm investing in projects within the AO ecosystem.
Arlink: Arlink is a one click deployment platform built on Arweave, enabling developers to permanently host front-end applications such as React, Next.js, or static websites with a single click. It functions like Vercel but leverages the Arweave Permaweb for censorship-resistant, immutable, and durable hosting.
Autonomous Finance: A decentralized finance initiative utilizing AO technologies for autonomous financial operations.
Bazar: A fully decentralized atomic asset exchange built on the permaweb, leveraging the Universal Content Marketplace (UCM) protocol and AO. It enables content creators to trade digital assets with real-world rights through a trustless orderbook process. As the first user interface operating on the UCM protocol, Bazar facilitates the exchange of various digital content types, from images and music to applications, while ensuring secure and efficient trading through AO processes.
BetterIDEa: A native web-based IDE for AO development, featuring Lua language server integration and AO Package Manager support.
Botega: A decentralized finance platform built on AO that facilitates asset exchanges through multiple models including AMMs and order books. Its unique architecture treats each liquidity pool as an independent process with localized state, enabling high-throughput transactions, autonomous execution of advanced orders, and AI integration. Botega is fully decentralized with its frontend on Arweave and execution logic in AO processes.
Community Labs: A collaborative group dedicated to advancing the permaweb ecosystem.
LiquidOps: An over collateralised lending and borrowing protocol built on Arweave and AO.
Dexi: An autonomous application that aggregates real-time financial data from events within the AO network, including asset prices, token swaps, and liquidity fluctuations. The Dexi Terminal provides a web interface similar to TradingView for monitoring assets and trading tokens. AO applications can subscribe to Dexi's data agents to receive verifiable real-time data, eliminating the need for external oracles.
Forward Research: A research organization exploring and expanding the capabilities of the AO network.
Longview Labs: An organization focused on developing decentralized technologies and applications.
Odysee: A decentralized video platform utilizing blockchain for content distribution. Currently being migrated to Arweave/AO via ArFleet.
Permaswap: A decentralized exchange built on the Arweave network, supporting transactions within the permaweb ecosystem.
Redstone: A decentralized data analytics platform built on Arweave.
Trackgood: A decentralized supply chain management system using Arweave for data integrity.
Wander: A non-custodial Arweave and AO native wallet with extensive features. Wander is available as a browser extension, mobile application, and embedded smart account.
Beacon: The most secure wallet for AO and AR networks, offering features such as two-factor authentication (2FA), multi-signature capabilities, passkeys, recovery options, and more to ensure robust security.
GraphQL for Arweave: A query language used to interact with Arweave data in a flexible and efficient manner, allowing developers to specify exactly what data they need.
Multi-sig: A multi-signature wallet or smart contract that requires multiple signatures or approvals from different parties before executing a transaction or action. This enhances security and control by distributing decision-making authority.
Permaweb Journal: An on-chain publication and knowledge hub focused on the Arweave and AO ecosystem. It serves as a central resource for news, insights, and developments within the permaweb community.
Lua: A lightweight, embeddable scripting language used in AO for process development. Lua is utilized within the AO environment for writing process code and interacting with the AO system.
Under_names: ANT owners and controllers can configure multiple subdomains for their registered ArNS name known as "under_names" or more easily written "undernames". These undernames are assigned individually at the time of registration or can be added on to any registered name at any time. Under_names use an underscore "_" in place of a more typically used dot "." to separate the subdomain from the main ArNS domain.
Permaweb Glossary: A comprehensive reference guide for the Arweave and AO ecosystem, providing detailed explanations of key terms, concepts, and technologies. It serves as an educational resource for developers, users, and community members to better understand the permaweb ecosystem. A key feature is its portability - the glossary can be embedded in other websites and applications via iframes, making it easily accessible across the permaweb.
Protocol.Land: Decentralized, source controlled, code collaboration where you own your code.
Stablecoin: A type of cryptocurrency designed to maintain a stable value relative to a specific asset or basket of assets, typically a fiat currency like the US dollar. Stablecoins provide price stability while leveraging the benefits of blockchain technology, making them useful for trading, payments, and as a store of value in the crypto ecosystem.
USDA: Astro USD, or USDA, is an AO-native stablecoin used to transact, facilitate, and interact with various other DeFi protocols in the ecosystem, while acting as a store of value, a medium of exchange, and a unit of account. USDA is a stablecoin backed by assets within the Arweave and AO ecosystems. It leverages Arweave's decentralized storage and AO's smart contract-like processes for secure and efficient value exchange, balancing transparency and decentralization.
Astro Protocol: A blockchain-based framework that enables the minting of stablecoins and synthetic assets, backed by on-chain collateral stored and managed by processes developed within the AO compute environment. Astro introduces dynamic liquidation processes and stability modules to enhance system resilience and user trust.
Data Item: A standardized data format defined in the ANS-104 bundle specification, used as the preferred storage format for Arweave bundles. Data items are signed pieces of data that can be bundled together for efficient storage on Arweave. In AO, messages are implemented as data items with specific protocol tags, making them the foundation of process communication and state management.
AO Link: A fully decentralized transaction explorer for the AO network (accessible at aolink.ar.io or ao.link) that provides real-time visualization and interaction with on-chain messages and computations. It features message exploration, graphical visualization of message relationships, manual message interaction capabilities, and real-time streaming of network activity. The platform enables users to view token balances, process inboxes, and inspect process code, offering comprehensive insight into the AO network's operations.
AO Teal: A typed version of Lua for AO development, similar to TypeScript for JavaScript, that adds static type checking and annotations to enhance code safety and reliability. It includes features for secure coding practices and integrates with a biginteger implementation for precise handling of large numbers in financial applications. Teal enables developers to write more robust and maintainable code for AO processes.
AO Form: A declarative infrastructure management tool for the AO ecosystem, similar to Terraform, that enables developers to deploy and upgrade multiprocess systems through a single configuration file. It provides state management for deployments, efficient component upgrades, and streamlined infrastructure maintenance, making it essential for managing complex AO process networks and optimizing development workflows.
Basejump: A social gaming platform built on AO that features the Action substrate. It provides a no-code environment for creating, discovering, and monetizing interoperable games and assets. Basejump is consumer-focused with an avatar-centric interface, featuring a PVP battle game for hyperobjects that runs in Discord and on the web.
AO Core Protocol: The foundational execution protocol of AO that enables decentralized computing and communication between nodes. AO Core provides a framework into which any number of different computational models, encapsulated as primitive devices, can be attached. It serves as the base layer for the AO ecosystem, offering universal primitives for decentralized computations including hashpaths for state-space referencing, unified data structures for program states, attestation mechanisms, and a meta-VM for executing various computational models. HyperBEAM is a modular system that implements this protocol, while AO builds an actor-based environment on top of it. The protocol is designed to be computer-native to internet technologies, particularly compatible with HTTP protocols. Every item on the permaweb is described as a Message, which can be interpreted as either a map of named functions or as a concrete binary term. Messages can be called through the creation of another message, providing a map of arguments to the execution. The protocol does not enforce any forms of consensus or particular virtual machine, instead focusing on offering the simplest possible representation of data and computation.
Asynchronous Message Passing: A communication paradigm where senders don't wait for receivers to be ready, allowing for non-blocking operations and better scalability. This enables efficient process communication in distributed systems.
Checkpoint: A saved state of a process that can be used to resume execution from a known point, used for persistence and recovery. This enables process state preservation and restoration capabilities.
Decentralized Execution: The ability to run processes across a distributed network without centralized control or coordination. This enables truly distributed computing capabilities in the HyperBEAM ecosystem.
Erlang: The programming language used to implement the HyperBEAM core, known for its robustness and support for building distributed, fault-tolerant applications. It provides the foundation for HyperBEAM's reliable message passing and process management.
~json-iface@1.0: A device in HyperBEAM that offers a translation layer between the JSON-encoded message format used by legacy versions and HyperBEAM's native HTTP message format.
HTTP Message Signature: A standardized method (RFC9421) for creating, encoding, and verifying digital signatures within HTTP messages, crucial for the security model of HyperBEAM and the AO Core protocol. In HyperBEAM, HTTP Signed Messages are used to authenticate node configuration changes, verify message integrity, and establish trust between nodes. The hb_message module in HyperBEAM provides functionality to convert between different message formats, including HTTP Signed Messages, enabling secure communication across the decentralized network. This signature mechanism is fundamental to the attestation system that allows nodes to cryptographically prove the correctness of computations without requiring full recomputation.
~p4@1.0: A device that runs as a pre-processor and post-processor in HyperBEAM, enabling a framework for node operators to sell usage of their machine's hardware to execute AO Core devices.
Sam Williams: The founder of Arweave and the creator of the AO Computer concept. He pioneered the development of the permaweb and decentralized storage technologies, leading the creation of both the Arweave network and the AO ecosystem. His vision has been instrumental in advancing decentralized computing and permanent data storage solutions.
Metalinks: A decentralized link hub platform that enables users to create and manage their web presence without intermediaries. Metalinks provides a web3-native solution where users can own their content and control their digital presence, embodying the principles of decentralization and user sovereignty. The platform allows users to create their own decentralized link hub, offering pure web3 freedom without traditional gatekeepers or centralized control.
Fair Launch: A cryptocurrency distribution method that ensures equal access and sustainable funding for projects without traditional gatekeepers or early investor advantages. In the AO ecosystem, fair launches enable projects to receive funding through the Permaweb Index, where users can allocate their AO yield to support development. This model promotes transparency and decentralization by avoiding pre-allocation, pre-mining, or insider advantages, while ensuring builders can secure sustainable funding. Projects launched this way receive tokens based on community allocation of yield, creating a more equitable system where success is tied directly to community support and utility rather than speculative investment.
Converge Protocol: The codename for AO Core Protocol. See AO Core Protocol for the complete definition.
Device: A component in the AO Core Protocol that specifies how a message should be interpreted. Each message may optionally state a Device which should be used by Converge-compatible systems to interpret its contents. If no Device is explicitly stated, it must be inferred as 'Message'. Every Device must implement functions with the names 'ID' and 'Keys'.
Hashpath: A cryptographic memoization of the tree of executions that were the source of a given piece of data in the AO Core Protocol. It is derived by cryptographically mixing two prior commitments, resulting in a short (32 bytes with SHA2-256) but verifiable reference to the entire computation tree.
Stack Device: A special device in HyperBEAM that combines a series of devices on a message into a single 'stack' of executable transformations. When added as the highest Device tag on a message, it scans the remainder of the message's tags looking for and loading other messages it finds, passing through each element of the stack in turn during execution.
Attestation: A cryptographic assurance mechanism in AO that verifies the correctness of computations and states. In AO's state management, attestations are cryptographically signed statements by Compute Units verifying process state computation results. In the AO Core Protocol, attestations provide cryptographic assurance that verifies the correctness of an output, representing proof that a specific computation produced the expected result when combined with the message and hashpath. Both forms enable trustless verification without requiring full recomputation.
Path: A way to reference messages in the AO Core Protocol that starts from a given message and applies a series of additional messages on top of it. Each resulting message must have an ID resolvable via its device, enabling additional paths to be described atop the intermediate message. Paths are designed to be compatible with HTTP protocols.
Permissionlessness: A property of computation systems where no actor in the ecosystem may be denied the ability to use the network by any other actor or group. This is a fundamental property of decentralized computation machines, though it can only be offered in degrees rather than absolutely.
Trustlessness: A property of computation systems where users can participate in the network without needing to trust other parties are not acting maliciously. Like permissionlessness, this is a fundamental property of decentralized computation machines that can only be offered in degrees rather than absolutely.
~meta@1.0: A configuration device in HyperBEAM that provides an interface for specifying a node's hardware, supported devices, metering, and payments information. It allows external clients to find and validate node configurations in the network, serving as the primary configuration mechanism for HyperBEAM nodes.
~relay@1.0: A device in HyperBEAM used to relay messages between nodes and the wider HTTP network. It provides an interface for sending and receiving messages using various execution strategies, facilitating communication across the network.
~wasm64@1.0: A device in HyperBEAM that executes WebAssembly code using the Web Assembly Micro-Runtime (WAMR). It enables the execution of WASM modules from any other device and supports devices written in languages like Rust, C, and C++.
~compute-lite@1.0: A lightweight device in HyperBEAM that wraps a local WASM executor, used for executing legacy AO processes. It enables compatibility with older AO network processes within the HyperBEAM environment.
~snp@1.0: A device in HyperBEAM used to generate and validate proofs that a node is executing inside a Trusted Execution Environment (TEE). It enables trust-minimized attestations of AO Core executions using ephemeral key pairs that exist only within the TEE.
Trustless Execution: A property of computation systems where users can participate in the network without needing to trust other parties are not acting maliciously. Like permissionlessness, this is a fundamental property of decentralized computation machines that can only be offered in degrees rather than absolutely.
~simple-pay@1.0: A simple, flexible pricing device in HyperBEAM that can be used with p4@1.0 to offer flat-fees for the execution of AO Core messages. It provides a straightforward mechanism for monetizing node services.
~faff@1.0: A simple pricing and ledger device for p4@1.0 in HyperBEAM that allows nodes to offer access to their services only to a specific set of users. It's useful for personal use or servicing specific applications.
scheduler@1.0: A device in HyperBEAM used to assign a linear hashpath to an execution, ensuring deterministic ordering that all users can access. It enables the creation of executions that mirror traditional smart contracting networks when used with other AO Core devices.
stack@1.0: A device in HyperBEAM used to execute an ordered set of devices over the same inputs. It allows users to create complex combinations of other devices and apply them as a single unit with a single hashpath.
~process@1.0: A device in HyperBEAM that enables users to create persistent, shared executions accessible by multiple users. It allows customization of execution and scheduler devices, supporting various execution patterns. It includes a push key for moving messages from a process's execution outbox into another execution's schedule.
ARIO Token: The native cryptocurrency of the AR.IO Network that powers the world's first Permanent Cloud. With a fixed supply of 1 billion tokens, ARIO serves multiple functions: powering ecosystem operations, facilitating transactions, and incentivizing network participation. It enables critical operations like gateway staking, ArNS domain registrations, and protocol rewards distribution. The token follows a non-inflationary model where network revenue is recycled to fund rewards rather than minting new tokens. Each ARIO is divisible into 1,000,000 micro-ARIO (µARIO) units to support a wide range of transactions.
Armstrong: The smallest unit of the AO Token. One AO is equal to one trillion (1,000,000,000,000) Armstrongs. When specifying token quantities in AO transactions, the amount is typically expressed in Armstrongs rather than AO tokens.
Winston: The smallest unit of the AR Token (Arweave). One AR is equal to one trillion (1,000,000,000,000) Winstons. This denomination is similar to satoshis in Bitcoin and is used for precise calculations and transactions within the Arweave network.
micro-ARIO: The smallest unit of the ARIO Token. One ARIO is equal to one million (1,000,000) micro-ARIO. This subunit allows for precise calculations and transactions within the AR.IO Network, enabling granular token operations for services like gateway staking and ArNS domain registrations.
Action: A decentralized framework built on AO and Arweave designed for creating self-sovereign games, digital assets, and communities. Action introduces 'hyperobjects' (interoperable game assets like avatars, items, and environments) that can be used across different games and platforms. The ecosystem is powered by the ACTION token, which enables hyperobject creation, governance participation, and economic coordination. Action emphasizes originality, collaboration, and sustainability, with a portion of fees directed toward real-world environmental initiatives like rainforest protection.
ACTION Token: The native utility token of the Action ecosystem with a fixed supply of 10 billion tokens. It enables hyperobject creation, marketplace transactions, governance participation, and ecosystem growth. Unlike traditional tokens, ACTION is designed primarily for platform utility rather than speculation, with 75% of the supply allocated to community distribution. The token follows a non-inflationary model and incorporates treasury mechanisms to fund environmental and social initiatives through community governance.
Hyperobject: The foundational digital assets of the Action ecosystem that serve as building blocks for game, commerce, and creativity. Hyperobjects include avatars ('Action Figures'), game assets ('Action Items'), and game environments ('Worlds'). Unlike traditional NFTs, hyperobjects can be large applications due to AO's scalability, and they exhibit context-dependent behavior, adapting dynamically to the specific game or world in which they're used. Each hyperobject runs as a unique AO process with corresponding Arweave data representing its underlying assets.
Merkle Root: The result of hashing all transactions in a block, pairing those hashes, and hashing them again until a single hash remains. In blockchain systems like Arweave and AO, Merkle roots enable efficient verification of transaction data without requiring nodes to process every transaction individually. This technique significantly reduces the computational resources needed to verify blocks and is central to maintaining blockchain integrity. In the AO Core Protocol, HashPaths use Merkle tree principles to create a cryptographic history of all operations that led to a particular state, allowing verification of the entire computation chain with a single hash.
AO Yield: A customizable reward system in the AO ecosystem that allows users to earn tokens based on their Arweave (AR) holdings and deposited assets (like stETH and DAI). Users can allocate their yield to different options: PI (receiving diversified exposure through the Permaweb Index), AO (continuing to earn AO tokens), AR (converting yield to Arweave tokens), or specific fair launch projects. When allocated to ecosystem projects, the yield helps fund development while users receive project tokens in return. The system operates through trustless, audited contracts on Ethereum for bridged assets, with deposits remaining withdrawable at any time.
Permaweb Index: A fair launch funding mechanism for the permaweb ecosystem, launched on Pi Day (3.14). It introduces a new model for funding crypto projects where users can allocate their AO yield to support ecosystem development. The index is represented by the PI token, which provides diversified exposure to key permaweb assets: 33.3% AO, 33.3% Arweave (AR), and 33.3% fair launch projects. This structure enables builders to secure sustainable development funding while allowing community members to support projects they believe in, all while maintaining broad exposure to the ecosystem's growth through a single token.
ArDrive: A decentralized storage application built on Arweave that enables users to store and manage their files permanently. It offers a pay-once, store-forever model, eliminating the need for recurring subscription fees. ArDrive provides true data ownership, ensuring files remain accessible even if the company disappears, and supports both private and public data storage options. The platform is optimized for storing various types of data, including photography, digital art, and NFTs.
ArDrive CLI: A command-line interface tool for interacting with ArDrive, enabling users to upload, download, and manage files on the Arweave network programmatically. It provides a powerful way to automate file operations and integrate ArDrive functionality into scripts and applications.
ArDrive Turbo: An enhanced version of ArDrive that provides faster upload speeds and improved performance for large files. It optimizes the upload process by implementing advanced bundling and chunking techniques, making it more efficient for handling substantial amounts of data.
ArDrive Core: The fundamental library that provides the core functionality for interacting with ArDrive. It includes essential features for file management, data encryption, and Arweave integration, serving as the foundation for both the ArDrive web application and CLI tools.
Rug Pull: A malicious practice in cryptocurrency where project developers abandon the project and take investors' funds, or deliberately manipulate token value before selling their holdings, leaving other investors with worthless tokens. AO's Fair Launch model prevents rug pulls by eliminating pre-mining, pre-allocation, and insider advantages. Instead, projects receive funding through the Permaweb Index based on community-allocated yield, ensuring that token distribution is tied directly to community support and utility rather than speculative investment. This model removes the ability for developers to suddenly dump large token holdings, as tokens are earned gradually through actual development and community backing.
Legacynet: A development and testing network for the AO computer launched on February 27, 2024. It allows developers and early adopters to interact with the AO computer without fees, providing a sandbox environment for testing and building applications before deploying to mainnet. The network includes features like the aos console for process management and native community chat servers for developer collaboration. It serves as a crucial platform for exploring AO's hyper parallel computing capabilities and building towards mainnet deployment.
RandAO: A novel decentralized random number generation protocol built on AO that achieves robust security through an 'any honest' guarantee - ensuring randomness integrity as long as at least one participant remains honest. It integrates Verifiable Delay Functions (VDFs) with a commit-reveal scheme to prevent denial of service attacks and manipulation attempts while maintaining unpredictable output generation.
Verifiable Delay Function: A cryptographic primitive that requires a specified number of sequential steps to evaluate (and hence a theoretical amount of time), yet produces a unique output that can be efficiently verified. VDFs feature sequential computation that cannot be significantly accelerated even with parallel processing, while allowing for efficient verification of results. In RandAO, VDFs prevent denial of service attacks by ensuring that once commitments are made, the final entropy value is deterministically locked but requires time to calculate.
Commit Reveal Scheme: A decentralized protocol for generating verifiable random numbers where providers first commit to entropy values by submitting their hashes, then reveal the original values which are verified against the commitments. The naive implementation suffers from a critical any-malicious denial of service vulnerability that RandAO addresses through VDF integration.
$RNG Token: The native utility token of the RandAO ecosystem that captures the value of decentralized random number generation. All randomness requests must be paid for in $RNG tokens, establishing continuous demand. The token economy creates a marketplace where providers compete on service quality and earn rewards through both direct service payments and staking mechanisms.
Pietrzak's VDF: An elegant implementation of a Verifiable Delay Function using the RSW timelock puzzle. The construction is defined by an RSA modulus (N), input value (x), number of squaring operations (T), and number of checkpoints (n). It creates checkpoints that split verification into equal segments that can be verified independently, reducing verification time through parallelization.
RSW Timelock Puzzle: A cryptographic construction developed by Rivest, Shamir, and Wagner that requires sequential computation to solve, making it impossible to significantly accelerate even with parallel processing. In RandAO, it serves as the foundation for Pietrzak's VDF implementation, creating a deterministic time delay between commitment and revelation phases to prevent denial of service attacks in randomness generation.
Parallel Processing: A fundamental capability in AO where an arbitrary number of processes can execute simultaneously and independently. Unlike traditional blockchain networks where computation happens in a single global state, AO processes maintain independent states and can operate separately, sharing available computational resources efficiently. During idle times, resources are automatically reallocated to active processes, maximizing throughput and enabling scalability mechanics similar to traditional web2 systems. This parallel architecture, combined with holographic state management, forms the foundation of AO's hyper parallel computing capabilities.
Hyper Parallel Computing: The ability to execute multiple processes simultaneously and independently, allowing for parallel execution of tasks and improved overall system performance. This is a key feature of AO's architecture, enabling efficient utilization of computational resources and faster processing times.
Deflationary: A property of Arweave's tokenomics where the effective token supply decreases over time due to the storage endowment mechanism. As users pay for permanent storage, tokens are locked in the endowment, and due to the storage cost decline rate (historically averaging 38% per year), fewer tokens need to be released from the endowment than were initially deposited. This creates a natural token sink that reduces circulating supply proportionate to network usage, strengthening the token's value proposition without extracting value from users.
Endowment Simulation: A computational model that projects the behavior of Arweave's storage endowment over time based on variables like storage cost decline rate, token prices, and network usage. This tool helps users understand the long-term sustainability of the permanent storage economics system.
Non-value Extractive Mechanism: A tokenomic design where costs to users represent actual value (in Arweave's case, storage security) rather than rent extraction. This creates alignment between user interests and protocol economics, as users pay for reliability rather than arbitrary fees. This mechanism is fundamental to Arweave's Permanent Storage Economics model.
Permanent Storage Economics: The economic model underlying Arweave's permanent storage solution, where upfront payments create a Storage Endowment that generates returns to fund ongoing storage costs. This model, as a non-value extractive mechanism, treats storage costs not as extraction but as purchased security, with higher costs correlating to greater reliability and permanence. The system's sustainability is projected through the endowment simulation tool.
Storage Cost Decline Rate: The annual percentage decrease in data storage costs due to technological advancements. Historically averaging around 38% per year over the past 50 years (as of 2025), this rate directly impacts Arweave's tokenomics by reducing the number of tokens that need to be released from the Storage Endowment over time to maintain stored data.
Storage Endowment: A financial mechanism in Arweave where users pay upfront for long-term storage (approximately 200 years at current prices). These tokens are removed from circulation and held in the endowment, only to be released if/when needed to pay for ongoing storage costs. This creates a natural token sink that reduces circulating supply proportionate to network usage.
Token Sink: A mechanism in the Arweave ecosystem that removes tokens from circulation, effectively reducing the available supply. In Arweave's case, the Storage Endowment functions as a token sink, as tokens paid for storage are locked away for extended periods. This mechanism contributes to the deflationary nature of the AR token.
TabM: A unified data format in HyperBEAM standing for 'Type Annotated Binary Messages'. TabM serves as the intermediary representation that all message formats can convert to and from, enabling seamless communication between different encoding schemes. It includes type annotations that preserve the data types of fields (such as integers, strings, and binaries), allowing for accurate message transformation across the system. TabM is a core component of HyperBEAM's message processing pipeline, bridging the gap between various message formats while preserving data integrity.
Codecs: Modular components in HyperBEAM that convert between different message formats and the unified TabM format. Codecs solve interoperability challenges in distributed systems by using TabM as an intermediate format, reducing complexity from N² direct conversions to N codecs for N formats. This maintains data integrity and type safety while supporting various encodings (ANS-104, HTTP Signatures, JSON, flat formats). The architecture makes the system extensible - adding a new format only requires one codec that converts to/from TabM, rather than creating conversions for every existing format. This enables HyperBEAM to seamlessly communicate across different systems while maintaining consistent message semantics.
Flat Format: A simple key-value encoding scheme in HyperBEAM where data is represented as a series of 'key: value' pairs, each on a new line. This format is particularly used in configuration files and certain message types due to its simplicity and human readability. For example, a flat format message might look like 'Content-Type: text/plain\nAction: update\nValue: 123'. Unlike nested formats like JSON, flat formats have no hierarchy or nesting capabilities, making them easier to parse but less expressive. In HyperBEAM, flat formats are often used for configuration files and basic message headers where nested structures aren't necessary.
~patch@1.0: A HyperBEAM device that enables AO processes to expose specific parts of their internal state for direct HTTP access. It allows developers to make selected data immediately readable via a standardized URL path, improving performance for web frontends and data services by replacing slower dryrun calls with immediate state reading.
Permaweb Deploy: A Node.js command-line tool designed to streamline the deployment of web applications to the Arweave permaweb. It automates the process of uploading build folders using Turbo, creating manifests, and updating ArNS (Arweave Name Service) records. The tool supports both mainnet and testnet deployments and integrates easily with GitHub Actions for continuous deployment workflows.
~lua@5.3a: A HyperBEAM device (lua@5.3a) that enables dynamic, on-the-fly computation of process state using Lua scripts. It allows transformation of process state without altering the underlying data, supporting pipeline-style processing where state is passed as a base message and custom Lua functions are executed to return computed results.
Wayfinder SDK: A client-side routing and verification library for decentralized data access on the Arweave network. It provides intelligent gateway routing based on performance and availability, cryptographic data verification to ensure content integrity, and seamless access to permaweb content by automatically selecting optimal gateways and verifying data authenticity.
dryrun: A simulation of an operation without committing it to the blockchain, allowing you to test and validate interactions on process state before applying them permanently. This method is being deprecated for HyperBEAM processes in favor of the state patching mechanism (~patch@1.0 device) for better performance, as dryrun calls caused bottlenecks on Legacynet. Still useful for testing token balances, transfers, and other operations locally before execution.

---

# 2. ARIO Docs

Document Number: 2
Source: https://docs.ar.io/build/guides/arns-viewer
Words: 2599
Extraction Method: html

ArNS Viewer Overview This guide will walk you through creating a project that uses the AR.IO SDK to interact with ArNS names in a web environment. It provides all the steps and context needed to help you get up and running smoothly, allowing you to effectively use these technologies.We will be using ARNext, a new framework based on Next.js, to simplify deployment to the Arweave permaweb. ARNext provides flexibility for deploying seamlessly to Arweave using an ArNS name, an Arweave transaction ID, or traditional services like Vercel—all without requiring major code modifications. This means you can deploy the same project across different environments with minimal effort.The guide will focus on the following core functionalities of the AR.IO SDK:Retrieving a List of All Active ArNS Names: Learn how to use the SDK to get and display a list of active ArNS names.Querying Detailed Records for a Specific ArNS Name: Learn how to access detailed records for a specific ArNS name using its ANT (Arweave Name Token).Updating and Creating Records on an ArNS Name: Learn how to modify and add records to an ArNS name, showcasing the capabilities of ANT for dynamic web content.By the end of this guide, you will have a complete, functional project that not only demonstrates how to use the AR.IO SDK but also shows the ease and flexibility of deploying applications to the Arweave permaweb. Whether you are an experienced developer or just starting out, this guide will help you understand the key aspects of building and deploying on Arweave.Getting Started Prerequisites Node v20.17 or greater git Install ARNext ARNext is a brand new framework that is still in development. It supports installation using npx, and you will need the proper Node version for the installation to be successful.You can then move your terminal into that newly created folder with:or open the folder in an IDE like VSCode, and open a new terminal inside that IDE in order to complete the next steps.Sanity Check It is good practice when starting a new project to view it in localhost without any changes, to make sure everything is installed and working correctly. To do this, run:or, if you prefer yarn:By default, the project will be served on port 3000, so you can access it by navigating to localhost:3000 in any browser. You should see something that looks like this: With this complete, you are ready to move on to customizing for your own project.Install AR.IO SDK Next, install the AR.IO SDK.or Polyfills Polyfills are used to provide missing functionality in certain environments. For example, browsers do not have direct access to a computer's file system, but many JavaScript libraries are designed to work in both browser and Node.js environments. These libraries might include references to fs, the module used by Node.js to interact with the file system. Since fs is not available in browsers, we need a polyfill to handle these references and ensure the application runs properly in a browser environment.Installation The below command will install several packages as development dependencies, which should be sufficient to handle most polyfill needs for projects that interact with Arweave.or Next Config With the polyfill packages installed, we need to tell our app how to use them. In NextJS, which ARNext is built on, this is done in the next.config.js file in the root of the project. The default config file will look like this:This configuration allows the app to determine if it is being served via an Arweave transaction Id, or through a more traditional method. From here, we need to add in the additional configurations for resolving our polyfills. The updated next.config.js will look like this:With that, you are ready to start customizing your app.Strip Default Content The first step in building your custom app is to remove the default content and create a clean slate. Follow these steps:Update the Home Page Navigate to pages > index.js, which serves as the main home page.Delete everything in this file and replace it with the following placeholder:Remove Unused Pages The folder pages > posts > [id].js will not be used in this project. Delete the entire posts folder to keep the project organized and free of unnecessary files.Create Header Create a new components folder Inside that, create a Header.js file, leave it blank for now.Create Routes Create a new file at components > ArweaveRoutes.js to handle routing between pages. Leave it simple for now.Your project is now a blank slate, ready for your own custom design and functionality. This clean setup will make it easier to build and maintain your application as you move forward.Add Utilities There are a few functions that we might end up wanting to use in multiple different pages in our finished product. So we can put these in a separate file and export them, so that other pages can import them to use. Start by creating a utils folder in the root of the project, then create 2 files inside of it:auth.js: This will contain the functions required for connecting an Arweave wallet using ArConnect arweave.js: This is where we will put most of our AR.IO SDK functions for interacting with Arweave import { ARIO, ANT, ArconnectSigner } from "@ar.io/sdk/web";

/**
 * Initialize ArIO and fetch all ArNS records.
 * @returns {Promise<Object>} All ArNS records.
 */
export const fetchArNSRecords = async () => {
  const ario = ARIO.init();
  let allRecords = [];
  let hasMore = true;
  let cursor;

  // Paginates through all records to get the full registry.
  while (hasMore) {
    const response = await ario.getArNSRecords({
      limit: 1000, // You can adjust the limit as needed, max is 1000
      sortBy: "name",
      sortOrder: "asc",
      cursor: cursor,
    });

    allRecords = [...allRecords, ...response.items];
    cursor = response.nextCursor;
    hasMore = response.hasMore;
  }

  // console.log(allRecords);
  return allRecords;
};

/**
 * Initialize ANT with the given processId.
 * @param {string} processId - The processId.
 * @returns {Object} ANT instance.
 */
export const initANT = (processId) => {
  return ANT.init({ processId });
};

/**
 * Fetch detailed records, owner, and controllers for a given processId.
 * @param {string} contractTxId - The processId.
 * @returns {Promise<Object>} Detailed records, owner, and controllers.
 */
export const fetchRecordDetails = async (processId) => {
  const ant = initANT(processId);
  const detailedRecords = await ant.getRecords();
  const owner = await ant.getOwner();
  const controllers = await ant.getControllers();
  return { detailedRecords, owner, controllers };
};

/**
 * Set a new record in the ANT process.
 * @param {string} processId - The processId.
 * @param {string} subDomain - The subdomain for the record.
 * @param {string} transactionId - The transaction ID the record should resolve to.
 * @param {number} ttlSeconds - The Time To Live (TTL) in seconds.
 * @returns {Promise<Object>} Result of the record update.
 */
export const setANTRecord = async (
  processId,
  name,
  transactionId,
  ttlSeconds
) => {
  console.log(`Pid: ${processId}`);
  console.log(`name: ${name}`);
  console.log(`txId: ${transactionId}`);
  const browserSigner = new ArconnectSigner(window.arweaveWallet);
  const ant = ANT.init({ processId, signer: browserSigner });
  const result = await ant.setRecord({
    undername: name,
    transactionId,
    ttlSeconds,
  });
  console.log(result);
  return result;
};Build Home Page Header We want the Header component to contain a button for users to connect their wallet to the site, and display their wallet address when Connected. To do this, we will use the functions we exported from the utils > auth.js file, and pass in a state and set state function from each page rendering the header:import React from "react";
import { connectWallet, truncateAddress } from "../utils/auth";

/**
 * Header component for displaying the connect wallet button and navigation.
 * @param {Object} props - Component props.
 * @param {string} props.address - The connected wallet address.
 * @param {function} props.setAddress - Function to set the connected wallet address.
 */
const Header = ({ address, setAddress }) => {
  const handleConnectWallet = async () => {
    try {
      const walletAddress = await connectWallet();
      setAddress(walletAddress);
    } catch (error) {
      console.error("Failed to connect wallet:", error);
    }
  };

  return (
    <div className="header">
      <button className="connect-wallet" onClick={handleConnectWallet}>
        {address ? `Connected: ${truncateAddress(address)}` : "Connect Wallet"}
      </button>
    </div>
  );
};

export default Header;Grid Component Our home page is going to fetch a list of all ArNS names and display them. To make this display cleaner and more organized, we are going to create a component to display the names as a grid.Create a new file in components named RecordsGrid.js This will take an individual ArNS record and display it as a button that logs the record name when clicked. We will update this later to make the button act as a link to the more detailed record page after we build that, which is why we are importing Link from arnext Home Page Go back to pages > index.js and lets build out our home page. We want to fetch the list of ArNS names when the page loads, and then feed the list into the grid component we just created. Because there are so many names, we also want to include a simple search bar to filter out displayed names. We will also need several states in order to manage all of this info:"use client";
import { useEffect, useState } from "react";
import Header from "@/components/Header";
import { fetchArNSRecords } from "@/utils/arweave";
import RecordsGrid from "@/components/RecordsGrid";

export default function Home() {
  const [arnsRecords, setArnsRecords] = useState(null); // State for storing all ArNS records
  const [isProcessing, setIsProcessing] = useState(true); // State for processing indicator
  const [searchTerm, setSearchTerm] = useState("") // used to filter displayed results by search input
  const [address, setAddress] = useState(null); // State for wallet address
  

  useEffect(() => {
    const fetchRecords = async () => {
      const allRecords = await fetchArNSRecords();
      setArnsRecords(allRecords);
      setIsProcessing(false);
    };

    fetchRecords();
  }, []);

  return (
    <div>
      <Header address={address} setAddress={setAddress} />
      {isProcessing ? (
        "processing"
      ) : (
        <div>
          <h2>Search</h2>
          <input 
          type="text"
          value={searchTerm}
          className ="search-bar"
          onChange = {(e) => {setSearchTerm(e.target.value)}}
          />
        <RecordsGrid
          keys={arnsRecords
            .map((r) => r.name)
            .filter((key) => key.toLowerCase().includes(searchTerm?.toLowerCase()))}
        /></div>
      )}
    </div>
  );
} Names Page NextJS, and ARNext by extension, supports dynamic routing, allowing us to create dedicated pages for any ArNS name without needing to use query strings, which makes the sharable urls much cleaner and more intuitive. We can do this by creating a page file with the naming convention [variable].js. Since we want to make a page for specific ArNS names we will create a new folder inside the pages folder named names, and then a new file pages > names > [name].js.This will be our largest file so far, including different logic for the displayed content depending on if the connected wallet is authorized to make changes the the name. We also need to make the page see what the name being looked at is, based on the url. We can do this using the custom useParams function from ARNext.The finished page will look like this:import Header from "@/components/Header";
import { useParams, Link } from "arnext"; // Import from ARNext, not NextJS
import { useEffect, useState } from "react";
import { ARIO } from "@ar.io/sdk/web";
import { fetchRecordDetails, setANTRecord } from "@/utils/arweave";

export async function getStaticPaths() {
  return { paths: [], fallback: "blocking" };
}

export async function getStaticProps({ params }) {
  const { name } = params;
  return { props: { name } }; // No initial record, just returning name
}

export default function NamePage() {
  const { name } = useParams();
  const [nameState, setNameState] = useState("");
  const [nameRecord, setNameRecord] = useState(null); // Initialize record to null
  const [arnsRecord, setArnsRecord] = useState(null);
  const [resultMessage, setResultMessage] = useState("");
  const [address, setAddress] = useState(null); // State for wallet address

  useEffect(() => {
    if (name && name !== nameState) {
      setNameState(name);

      // Fetch the record dynamically whenever routeName changes
      const fetchRecord = async () => {
        console.log("fetching records");
        try {
          const ario = ARIO.init();
          const newRecord = await ario.getArNSRecord({ name });
          console.log(newRecord);
          setNameRecord(newRecord);
        } catch (error) {
          console.error("Failed to fetch record:", error);
          setRecord(null);
        }
      };

      fetchRecord();
    }
    if (nameRecord && nameRecord.processId) {
      const fetchArnsRecord = async () => {
        try {
          const arnsRecord = await fetchRecordDetails(nameRecord.processId);
          console.log(arnsRecord);
          setArnsRecord(arnsRecord);
        } catch (error) {
          console.error(error);
        }
      };
      fetchArnsRecord();
    }
  }, [nameState, nameRecord]);

  const handleUpdateRecord = async (key, txId) => {
    const result = await setANTRecord(nameRecord.processId, key, txId, 900)
  console.log(`result Message: ${result}`)
  console.log(result)
    setResultMessage(result.id)
  };

  if (nameRecord === null) {
    return (
      <div>
        <Header address={address} setAddress={setAddress} />
        <p>Loading...</p>
      </div>
    );
  }

  const owner = arnsRecord?.owner || "N/A";
  const controllers = arnsRecord?.controllers || [];

  return (
    <div>
      <Header address={address} setAddress={setAddress} />
      <div className="record-details">
        <h3>Record Details for {nameState}</h3>
        <div>
          {arnsRecord?.detailedRecords &&
            Object.keys(arnsRecord.detailedRecords).map((recordKey, index) => (
              <div key={index} className="record-txid">
                <strong>{recordKey}:</strong>{" "}
                <a
                  href={`https://arweave.net/${arnsRecord.detailedRecords[recordKey].transactionId}`}
                  target="_blank"
                  rel="noopener noreferrer"
                >
                  {arnsRecord.detailedRecords[recordKey].transactionId}
                </a>
              </div>
            ))}
        </div>
        <p>Owner: {owner}</p>
        <p>
          Controllers: {controllers.length > 0 ? controllers.join(", ") : "N/A"}
        </p>
        {owner === address && ( 
          <>
            {arnsRecord?.detailedRecords &&
              Object.keys(arnsRecord.detailedRecords).map(
                (recordKey, index) => (
                  <div key={index} className="record-update">
                    <label>
                      {recordKey}:
                      <input
                        type="text"
                        placeholder="Enter new TxID"
                        id={`input-${index}`}
                      />
                      <button
                        onClick={() => {
                          const inputElement = document.getElementById(`input-${index}`);
                          const inputValue = inputElement ? inputElement.value : "";
                          handleUpdateRecord(
                            recordKey === "@" ? "@" : `${recordKey}`,
                            inputValue
                          );
                        }}
                      >
                        Update
                      </button>
                    </label>
                  </div>
                )
              )}
            <div className="new-record">
              <input
                type="text"
                placeholder="New Subdomain"
                id={`new-subdomain-input`}
              />
              <input
                type="text"
                placeholder="New TxID"
                id={`new-txid-input`}
              />
              <button
                onClick={() => {
                  const subdomainElement = document.getElementById("new-subdomain-input");
                  const txIdElement = document.getElementById("new-txid-input");
            
                  const newSubdomainValue = subdomainElement ? subdomainElement.value : "";
                  const newTxIdValue = txIdElement ? txIdElement.value : "";
            
                  console.log(newSubdomainValue)
                  console.log(newTxIdValue)
                  handleUpdateRecord(newSubdomainValue, newTxIdValue);
                }}
              >
                Set New Record
              </button>
            </div>
          </>
        )}
        <Link href="/">
          <button>Back to list</button>
        </Link>

        {resultMessage && <p>Successfully updated with message ID: {resultMessage}</p>}
      </div>
    </div>
  );
} When this page loads, it gets the name being queried by using useParams and our custom getStaticPaths and getStaticProps functions. It then uses the AR.IO sdk to get the process Id of the ANT that controls the name, and queries the ANT for its info and detailed records list.Once the page has that info, it renders the ArNS name, its owner address, any addresses authorized to make changes, and every record that name contains. If the user has connected a wallet authorized to make changes, the page also renders input fields for each record for making those updates. It also provides the option to create an entirely new undername record.Finish the Grid Component Now that we have a path for our main page displays to link to, we can update the components > RecordsGrid.js file to include that link when clicked.View Project The ArNS viewer should be fully functional now. You can view it locally in your browser using the same steps as the initial Sanity Check Run yarn dev in your terminal Navigate to localhost:3000 in a browser CSS You will likely notice that everything functions correctly, but it doesnt look very nice. This is because we havent updated our css at all.The primary css file for this project is css > App.css. You can make whatever css rules here that you like to make the page look the way you want. Deploy With Turbo Once your app is looking the way you want it, you can deploy it to the permaweb using Turbo. For this, you will need an Arweave wallet with some Turbo Credits. Make sure you don't place your keyfile for the wallet inside the project directory, or you risk it getting uploaded to Arweave by mistake.In your terminal, run the command:Make sure to replace <path-to-your-wallet> with the actual path to your Arweave wallet. This will create a static build of your entire project, upload it to Arweave, and print out in the terminal all of the details of the upload.Find the section in the print out manifestResponse which will have a key named id. That will be the Arweave transaction id for your project.You can view a permanently deployed version of your project at https://arweave.net/<transaction-id> References Completed Project example: github Deployed Project: transaction id

---

# 3. ARIO Docs

Document Number: 3
Source: https://docs.ar.io/guides/arns-viewer
Words: 2599
Extraction Method: html

ArNS Viewer Overview This guide will walk you through creating a project that uses the AR.IO SDK to interact with ArNS names in a web environment. It provides all the steps and context needed to help you get up and running smoothly, allowing you to effectively use these technologies.We will be using ARNext, a new framework based on Next.js, to simplify deployment to the Arweave permaweb. ARNext provides flexibility for deploying seamlessly to Arweave using an ArNS name, an Arweave transaction ID, or traditional services like Vercel—all without requiring major code modifications. This means you can deploy the same project across different environments with minimal effort.The guide will focus on the following core functionalities of the AR.IO SDK:Retrieving a List of All Active ArNS Names: Learn how to use the SDK to get and display a list of active ArNS names.Querying Detailed Records for a Specific ArNS Name: Learn how to access detailed records for a specific ArNS name using its ANT (Arweave Name Token).Updating and Creating Records on an ArNS Name: Learn how to modify and add records to an ArNS name, showcasing the capabilities of ANT for dynamic web content.By the end of this guide, you will have a complete, functional project that not only demonstrates how to use the AR.IO SDK but also shows the ease and flexibility of deploying applications to the Arweave permaweb. Whether you are an experienced developer or just starting out, this guide will help you understand the key aspects of building and deploying on Arweave.Getting Started Prerequisites Node v20.17 or greater git Install ARNext ARNext is a brand new framework that is still in development. It supports installation using npx, and you will need the proper Node version for the installation to be successful.You can then move your terminal into that newly created folder with:or open the folder in an IDE like VSCode, and open a new terminal inside that IDE in order to complete the next steps.Sanity Check It is good practice when starting a new project to view it in localhost without any changes, to make sure everything is installed and working correctly. To do this, run:or, if you prefer yarn:By default, the project will be served on port 3000, so you can access it by navigating to localhost:3000 in any browser. You should see something that looks like this: With this complete, you are ready to move on to customizing for your own project.Install AR.IO SDK Next, install the AR.IO SDK.or Polyfills Polyfills are used to provide missing functionality in certain environments. For example, browsers do not have direct access to a computer's file system, but many JavaScript libraries are designed to work in both browser and Node.js environments. These libraries might include references to fs, the module used by Node.js to interact with the file system. Since fs is not available in browsers, we need a polyfill to handle these references and ensure the application runs properly in a browser environment.Installation The below command will install several packages as development dependencies, which should be sufficient to handle most polyfill needs for projects that interact with Arweave.or Next Config With the polyfill packages installed, we need to tell our app how to use them. In NextJS, which ARNext is built on, this is done in the next.config.js file in the root of the project. The default config file will look like this:This configuration allows the app to determine if it is being served via an Arweave transaction Id, or through a more traditional method. From here, we need to add in the additional configurations for resolving our polyfills. The updated next.config.js will look like this:With that, you are ready to start customizing your app.Strip Default Content The first step in building your custom app is to remove the default content and create a clean slate. Follow these steps:Update the Home Page Navigate to pages > index.js, which serves as the main home page.Delete everything in this file and replace it with the following placeholder:Remove Unused Pages The folder pages > posts > [id].js will not be used in this project. Delete the entire posts folder to keep the project organized and free of unnecessary files.Create Header Create a new components folder Inside that, create a Header.js file, leave it blank for now.Create Routes Create a new file at components > ArweaveRoutes.js to handle routing between pages. Leave it simple for now.Your project is now a blank slate, ready for your own custom design and functionality. This clean setup will make it easier to build and maintain your application as you move forward.Add Utilities There are a few functions that we might end up wanting to use in multiple different pages in our finished product. So we can put these in a separate file and export them, so that other pages can import them to use. Start by creating a utils folder in the root of the project, then create 2 files inside of it:auth.js: This will contain the functions required for connecting an Arweave wallet using ArConnect arweave.js: This is where we will put most of our AR.IO SDK functions for interacting with Arweave import { ARIO, ANT, ArconnectSigner } from "@ar.io/sdk/web";

/**
 * Initialize ArIO and fetch all ArNS records.
 * @returns {Promise<Object>} All ArNS records.
 */
export const fetchArNSRecords = async () => {
  const ario = ARIO.init();
  let allRecords = [];
  let hasMore = true;
  let cursor;

  // Paginates through all records to get the full registry.
  while (hasMore) {
    const response = await ario.getArNSRecords({
      limit: 1000, // You can adjust the limit as needed, max is 1000
      sortBy: "name",
      sortOrder: "asc",
      cursor: cursor,
    });

    allRecords = [...allRecords, ...response.items];
    cursor = response.nextCursor;
    hasMore = response.hasMore;
  }

  // console.log(allRecords);
  return allRecords;
};

/**
 * Initialize ANT with the given processId.
 * @param {string} processId - The processId.
 * @returns {Object} ANT instance.
 */
export const initANT = (processId) => {
  return ANT.init({ processId });
};

/**
 * Fetch detailed records, owner, and controllers for a given processId.
 * @param {string} contractTxId - The processId.
 * @returns {Promise<Object>} Detailed records, owner, and controllers.
 */
export const fetchRecordDetails = async (processId) => {
  const ant = initANT(processId);
  const detailedRecords = await ant.getRecords();
  const owner = await ant.getOwner();
  const controllers = await ant.getControllers();
  return { detailedRecords, owner, controllers };
};

/**
 * Set a new record in the ANT process.
 * @param {string} processId - The processId.
 * @param {string} subDomain - The subdomain for the record.
 * @param {string} transactionId - The transaction ID the record should resolve to.
 * @param {number} ttlSeconds - The Time To Live (TTL) in seconds.
 * @returns {Promise<Object>} Result of the record update.
 */
export const setANTRecord = async (
  processId,
  name,
  transactionId,
  ttlSeconds
) => {
  console.log(`Pid: ${processId}`);
  console.log(`name: ${name}`);
  console.log(`txId: ${transactionId}`);
  const browserSigner = new ArconnectSigner(window.arweaveWallet);
  const ant = ANT.init({ processId, signer: browserSigner });
  const result = await ant.setRecord({
    undername: name,
    transactionId,
    ttlSeconds,
  });
  console.log(result);
  return result;
};Build Home Page Header We want the Header component to contain a button for users to connect their wallet to the site, and display their wallet address when Connected. To do this, we will use the functions we exported from the utils > auth.js file, and pass in a state and set state function from each page rendering the header:import React from "react";
import { connectWallet, truncateAddress } from "../utils/auth";

/**
 * Header component for displaying the connect wallet button and navigation.
 * @param {Object} props - Component props.
 * @param {string} props.address - The connected wallet address.
 * @param {function} props.setAddress - Function to set the connected wallet address.
 */
const Header = ({ address, setAddress }) => {
  const handleConnectWallet = async () => {
    try {
      const walletAddress = await connectWallet();
      setAddress(walletAddress);
    } catch (error) {
      console.error("Failed to connect wallet:", error);
    }
  };

  return (
    <div className="header">
      <button className="connect-wallet" onClick={handleConnectWallet}>
        {address ? `Connected: ${truncateAddress(address)}` : "Connect Wallet"}
      </button>
    </div>
  );
};

export default Header;Grid Component Our home page is going to fetch a list of all ArNS names and display them. To make this display cleaner and more organized, we are going to create a component to display the names as a grid.Create a new file in components named RecordsGrid.js This will take an individual ArNS record and display it as a button that logs the record name when clicked. We will update this later to make the button act as a link to the more detailed record page after we build that, which is why we are importing Link from arnext Home Page Go back to pages > index.js and lets build out our home page. We want to fetch the list of ArNS names when the page loads, and then feed the list into the grid component we just created. Because there are so many names, we also want to include a simple search bar to filter out displayed names. We will also need several states in order to manage all of this info:"use client";
import { useEffect, useState } from "react";
import Header from "@/components/Header";
import { fetchArNSRecords } from "@/utils/arweave";
import RecordsGrid from "@/components/RecordsGrid";

export default function Home() {
  const [arnsRecords, setArnsRecords] = useState(null); // State for storing all ArNS records
  const [isProcessing, setIsProcessing] = useState(true); // State for processing indicator
  const [searchTerm, setSearchTerm] = useState("") // used to filter displayed results by search input
  const [address, setAddress] = useState(null); // State for wallet address
  

  useEffect(() => {
    const fetchRecords = async () => {
      const allRecords = await fetchArNSRecords();
      setArnsRecords(allRecords);
      setIsProcessing(false);
    };

    fetchRecords();
  }, []);

  return (
    <div>
      <Header address={address} setAddress={setAddress} />
      {isProcessing ? (
        "processing"
      ) : (
        <div>
          <h2>Search</h2>
          <input 
          type="text"
          value={searchTerm}
          className ="search-bar"
          onChange = {(e) => {setSearchTerm(e.target.value)}}
          />
        <RecordsGrid
          keys={arnsRecords
            .map((r) => r.name)
            .filter((key) => key.toLowerCase().includes(searchTerm?.toLowerCase()))}
        /></div>
      )}
    </div>
  );
} Names Page NextJS, and ARNext by extension, supports dynamic routing, allowing us to create dedicated pages for any ArNS name without needing to use query strings, which makes the sharable urls much cleaner and more intuitive. We can do this by creating a page file with the naming convention [variable].js. Since we want to make a page for specific ArNS names we will create a new folder inside the pages folder named names, and then a new file pages > names > [name].js.This will be our largest file so far, including different logic for the displayed content depending on if the connected wallet is authorized to make changes the the name. We also need to make the page see what the name being looked at is, based on the url. We can do this using the custom useParams function from ARNext.The finished page will look like this:import Header from "@/components/Header";
import { useParams, Link } from "arnext"; // Import from ARNext, not NextJS
import { useEffect, useState } from "react";
import { ARIO } from "@ar.io/sdk/web";
import { fetchRecordDetails, setANTRecord } from "@/utils/arweave";

export async function getStaticPaths() {
  return { paths: [], fallback: "blocking" };
}

export async function getStaticProps({ params }) {
  const { name } = params;
  return { props: { name } }; // No initial record, just returning name
}

export default function NamePage() {
  const { name } = useParams();
  const [nameState, setNameState] = useState("");
  const [nameRecord, setNameRecord] = useState(null); // Initialize record to null
  const [arnsRecord, setArnsRecord] = useState(null);
  const [resultMessage, setResultMessage] = useState("");
  const [address, setAddress] = useState(null); // State for wallet address

  useEffect(() => {
    if (name && name !== nameState) {
      setNameState(name);

      // Fetch the record dynamically whenever routeName changes
      const fetchRecord = async () => {
        console.log("fetching records");
        try {
          const ario = ARIO.init();
          const newRecord = await ario.getArNSRecord({ name });
          console.log(newRecord);
          setNameRecord(newRecord);
        } catch (error) {
          console.error("Failed to fetch record:", error);
          setRecord(null);
        }
      };

      fetchRecord();
    }
    if (nameRecord && nameRecord.processId) {
      const fetchArnsRecord = async () => {
        try {
          const arnsRecord = await fetchRecordDetails(nameRecord.processId);
          console.log(arnsRecord);
          setArnsRecord(arnsRecord);
        } catch (error) {
          console.error(error);
        }
      };
      fetchArnsRecord();
    }
  }, [nameState, nameRecord]);

  const handleUpdateRecord = async (key, txId) => {
    const result = await setANTRecord(nameRecord.processId, key, txId, 900)
  console.log(`result Message: ${result}`)
  console.log(result)
    setResultMessage(result.id)
  };

  if (nameRecord === null) {
    return (
      <div>
        <Header address={address} setAddress={setAddress} />
        <p>Loading...</p>
      </div>
    );
  }

  const owner = arnsRecord?.owner || "N/A";
  const controllers = arnsRecord?.controllers || [];

  return (
    <div>
      <Header address={address} setAddress={setAddress} />
      <div className="record-details">
        <h3>Record Details for {nameState}</h3>
        <div>
          {arnsRecord?.detailedRecords &&
            Object.keys(arnsRecord.detailedRecords).map((recordKey, index) => (
              <div key={index} className="record-txid">
                <strong>{recordKey}:</strong>{" "}
                <a
                  href={`https://arweave.net/${arnsRecord.detailedRecords[recordKey].transactionId}`}
                  target="_blank"
                  rel="noopener noreferrer"
                >
                  {arnsRecord.detailedRecords[recordKey].transactionId}
                </a>
              </div>
            ))}
        </div>
        <p>Owner: {owner}</p>
        <p>
          Controllers: {controllers.length > 0 ? controllers.join(", ") : "N/A"}
        </p>
        {owner === address && ( 
          <>
            {arnsRecord?.detailedRecords &&
              Object.keys(arnsRecord.detailedRecords).map(
                (recordKey, index) => (
                  <div key={index} className="record-update">
                    <label>
                      {recordKey}:
                      <input
                        type="text"
                        placeholder="Enter new TxID"
                        id={`input-${index}`}
                      />
                      <button
                        onClick={() => {
                          const inputElement = document.getElementById(`input-${index}`);
                          const inputValue = inputElement ? inputElement.value : "";
                          handleUpdateRecord(
                            recordKey === "@" ? "@" : `${recordKey}`,
                            inputValue
                          );
                        }}
                      >
                        Update
                      </button>
                    </label>
                  </div>
                )
              )}
            <div className="new-record">
              <input
                type="text"
                placeholder="New Subdomain"
                id={`new-subdomain-input`}
              />
              <input
                type="text"
                placeholder="New TxID"
                id={`new-txid-input`}
              />
              <button
                onClick={() => {
                  const subdomainElement = document.getElementById("new-subdomain-input");
                  const txIdElement = document.getElementById("new-txid-input");
            
                  const newSubdomainValue = subdomainElement ? subdomainElement.value : "";
                  const newTxIdValue = txIdElement ? txIdElement.value : "";
            
                  console.log(newSubdomainValue)
                  console.log(newTxIdValue)
                  handleUpdateRecord(newSubdomainValue, newTxIdValue);
                }}
              >
                Set New Record
              </button>
            </div>
          </>
        )}
        <Link href="/">
          <button>Back to list</button>
        </Link>

        {resultMessage && <p>Successfully updated with message ID: {resultMessage}</p>}
      </div>
    </div>
  );
} When this page loads, it gets the name being queried by using useParams and our custom getStaticPaths and getStaticProps functions. It then uses the AR.IO sdk to get the process Id of the ANT that controls the name, and queries the ANT for its info and detailed records list.Once the page has that info, it renders the ArNS name, its owner address, any addresses authorized to make changes, and every record that name contains. If the user has connected a wallet authorized to make changes, the page also renders input fields for each record for making those updates. It also provides the option to create an entirely new undername record.Finish the Grid Component Now that we have a path for our main page displays to link to, we can update the components > RecordsGrid.js file to include that link when clicked.View Project The ArNS viewer should be fully functional now. You can view it locally in your browser using the same steps as the initial Sanity Check Run yarn dev in your terminal Navigate to localhost:3000 in a browser CSS You will likely notice that everything functions correctly, but it doesnt look very nice. This is because we havent updated our css at all.The primary css file for this project is css > App.css. You can make whatever css rules here that you like to make the page look the way you want. Deploy With Turbo Once your app is looking the way you want it, you can deploy it to the permaweb using Turbo. For this, you will need an Arweave wallet with some Turbo Credits. Make sure you don't place your keyfile for the wallet inside the project directory, or you risk it getting uploaded to Arweave by mistake.In your terminal, run the command:Make sure to replace <path-to-your-wallet> with the actual path to your Arweave wallet. This will create a static build of your entire project, upload it to Arweave, and print out in the terminal all of the details of the upload.Find the section in the print out manifestResponse which will have a key named id. That will be the Arweave transaction id for your project.You can view a permanently deployed version of your project at https://arweave.net/<transaction-id> References Completed Project example: github Deployed Project: transaction id

---

# 4. Register an IP Asset on Arweave - ARIO Docs

Document Number: 4
Source: https://docs.ar.io/guides/story
Words: 1932
Extraction Method: html

Registering Story Protocol IP Assets with Arweave Metadata using Turbo Utilize the speed and reliability of ArDrive Turbo to store metadata for Story Protocol IP Assets permanently on Arweave.Story Protocol enables the registration and management of intellectual property (IP) on-chain. A crucial part of this process involves linking metadata to your IP Assets. While various storage solutions exist, Arweave offers permanent, decentralized storage, making it an ideal choice for valuable IP metadata.This guide demonstrates how to use the ArDrive Turbo SDK to efficiently upload IP Asset metadata to Arweave and register it with the Story Protocol TypeScript SDK.Prerequisites Before you begin, ensure you have the following:Node.js: Version 18 or later. Download from nodejs.org.npm/pnpm/yarn: A compatible package manager.Arweave Wallet: A wallet.json file. Generate one using tools like the Wander browser extension. Keep this file secure and do not commit it to version control.Turbo Credits: Your Arweave wallet must be funded with Turbo credits to pay for uploads. Top up at https://turbo-topup.com.Story Protocol Account: An Ethereum-compatible private key (WALLET_PRIVATE_KEY) and an RPC Provider URL (RPC_PROVIDER_URL) for the desired Story Protocol network (e.g., Aeneid testnet) stored in a .env file.TypeScript Environment: You'll need to execute TypeScript code, so make sure you have ts-node installed globally (npm install -g ts-node) or as a dev dependency.Setup 1. Install Dependencies First, set up a new project directory and install the necessary SDKs:Then install the required dependencies:2. Project Setup Create the following files in your project:.env file (in the project root):Place your Arweave wallet.json file in the project root.Create a tsconfig.json file in the project root:3. Initialize SDK Clients Create a configuration file to set up and export both the Turbo and Story clients:import { TurboFactory, TurboAuthenticatedClient } from "@ardrive/turbo-sdk";
import { StoryClient, StoryConfig } from "@story-protocol/core-sdk";
import { http } from "viem";
import { Account, privateKeyToAccount, Address } from "viem/accounts";
import fs from 'fs';
import path from 'path';
import 'dotenv/config';

// --- Environment Variable Loading ---
const privateKeyEnv = process.env.WALLET_PRIVATE_KEY;
const rpcProviderUrlEnv = process.env.RPC_PROVIDER_URL;
const walletPath = path.resolve(process.cwd(), 'wallet.json'); // Assumes wallet.json is in the project root

// --- Validations ---
if (!privateKeyEnv) {
  throw new Error("WALLET_PRIVATE_KEY is not set in the .env file");
}
if (!rpcProviderUrlEnv) {
  throw new Error("RPC_PROVIDER_URL is not set in the .env file");
}
if (!fs.existsSync(walletPath)) {
    throw new Error(`Arweave wallet file not found at ${walletPath}. Please ensure wallet.json exists in the project root.`);
}

// --- ArDrive Turbo Client Setup ---
function parseWallet(filePath: string): any {
    try {
        const walletData = fs.readFileSync(filePath, 'utf8');
        return JSON.parse(walletData);
    } catch (error) {
        console.error(`Error reading or parsing wallet file at ${filePath}:`, error);
        throw new Error(`Failed to load Arweave wallet. Ensure ${filePath} exists and is valid JSON.`);
    }
}

const arweaveWallet = parseWallet(walletPath);

export const turboClient: TurboAuthenticatedClient = TurboFactory.authenticated({
    privateKey: arweaveWallet,
});
console.log("ArDrive Turbo Client initialized.");

// --- Story Protocol Client Setup ---
const storyPrivateKey: Address = `0x${privateKeyEnv}`;
const storyAccount: Account = privateKeyToAccount(storyPrivateKey);

const storyConfig: StoryConfig = {
  account: storyAccount,
  transport: http(rpcProviderUrlEnv),
  chainId: "aeneid", // Adjust chainId if necessary
};

export const storyClient = StoryClient.newClient(storyConfig);
console.log("Story Client initialized.");Make sure to create the utils directory first:Now, let's create a script to register an IP asset. This involves three steps:Define metadata for the IP itself and the NFT representing ownership Upload metadata to Arweave using Turbo Register the IP on Story Protocol Create the following script file:import { storyClient, turboClient } from "./utils/clients";
import { createHash } from "crypto";
import { Address } from "viem";
import type { UploadResult } from "@ardrive/turbo-sdk";

// Helper function to upload JSON to Arweave via Turbo
async function uploadJSONToArweave(jsonData: any, description: string): Promise<UploadResult> {
    const dataBuffer = Buffer.from(JSON.stringify(jsonData));
    console.log(`Uploading ${description} (${dataBuffer.byteLength} bytes) to Arweave via Turbo...`);

    const tags = [
        { name: "Content-Type", value: "application/json" },
        { name: "App-Name", value: "ArDrive-Story-Tutorial" } // Example tag
    ];

    try {
        // Use Turbo to upload the file buffer
        const result = await turboClient.uploadFile(dataBuffer, { tags });
        console.log(`${description} uploaded successfully: Transaction ID ${result.id}`);
        return result;
    } catch (error) {
        console.error(`Error uploading ${description} to Arweave:`, error);
        throw new Error(`Arweave upload failed for ${description}.`);
    }
}

async function register() {
  // --- Step 1: Define IP Metadata ---
  const ipMetadata = {
    title: "My Arweave-Powered IP",
    description: "An example IP asset with metadata stored permanently on Arweave via Turbo.",
    // Add other required fields like image, creators, etc.
    // Example creator:
    creators: [
      { name: "Your Name/Org", address: storyClient.account.address, contributionPercent: 100 },
    ],
  };
  console.log("IP Metadata defined.");

  const nftMetadata = {
    name: "Ownership NFT for My Arweave IP",
    description: "This NFT represents ownership of the IP Asset whose metadata is on Arweave.",
    // Add other fields like image
  };
  console.log("NFT Metadata defined.");

  // --- Step 2: Upload Metadata to Arweave ---
  const ipUploadResult = await uploadJSONToArweave(ipMetadata, "IP Metadata");
  const nftUploadResult = await uploadJSONToArweave(nftMetadata, "NFT Metadata");

  // Use arweave.net URLs instead of ar:// protocol
  const ipMetadataArweaveURI = `https://arweave.net/${ipUploadResult.id}`;
  const nftMetadataArweaveURI = `https://arweave.net/${nftUploadResult.id}`;

  console.log(`IP Metadata Arweave URI: ${ipMetadataArweaveURI}`);
  console.log(`NFT Metadata Arweave URI: ${nftMetadataArweaveURI}`);

  // Calculate metadata hashes (required by Story Protocol)
  const ipMetadataHash = `0x${createHash("sha256")
    .update(JSON.stringify(ipMetadata))
    .digest("hex")}`;
  const nftMetadataHash = `0x${createHash("sha256")
    .update(JSON.stringify(nftMetadata))
    .digest("hex")}`;

  console.log(`IP Metadata Hash: ${ipMetadataHash}`);
  console.log(`NFT Metadata Hash: ${nftMetadataHash}`);

  // --- Step 3: Register IP on Story Protocol ---
  console.log("Registering IP Asset on Story Protocol...");

  // Choose an SPG NFT contract (Story Protocol Governed NFT)
  // Use a public testnet one or create your own (see Story docs)
  const spgNftContract: Address = "0xc32A8a0FF3beDDDa58393d022aF433e78739FAbc"; // Aeneid testnet example

  try {
    const response = await storyClient.ipAsset.mintAndRegisterIp({
      spgNftContract: spgNftContract,
      ipMetadata: {
        ipMetadataURI: ipMetadataArweaveURI,      // URI pointing to Arweave
        ipMetadataHash: ipMetadataHash as Address, // Content hash
        nftMetadataURI: nftMetadataArweaveURI,     // URI pointing to Arweave
        nftMetadataHash: nftMetadataHash as Address // Content hash
      },
      txOptions: { waitForTransaction: true }, // Wait for confirmation
    });

    console.log(
      `Successfully registered IP Asset!`
    );
    console.log(`  Transaction Hash: ${response.txHash}`);
    console.log(`  IP ID: ${response.ipId}`);
    console.log(`  Story Explorer Link: https://aeneid.explorer.story.foundation/ipa/${response.ipId}`); // Adjust explorer link for different networks
    console.log(`  IP Metadata (Arweave): ${ipMetadataArweaveURI}`);
    console.log(`  NFT Metadata (Arweave): ${nftMetadataArweaveURI}`);

  } catch (error) {
    console.error("Error registering IP Asset on Story Protocol:", error);
  }
}

// Execute the register function
register().catch(console.error);Run the Registration Script To execute the script and register your IP Asset:This will:Upload your IP metadata to Arweave permanently Upload your NFT metadata to Arweave permanently Register an IP Asset on Story Protocol pointing to these Arweave URLs Once an IP Asset is registered, you can attach license terms and allow others to mint license tokens. Create a new script for this:import { storyClient } from "./utils/clients";
import { Address } from "viem";

// Assume these values are known for the IP Asset you want to license
const LICENSOR_IP_ID: Address = "0x..."; // Replace with the actual IP ID of the asset
const LICENSE_TERMS_ID: string = "..."; // Replace with the specific terms ID attached to the IP Asset
const RECEIVER_ADDRESS: Address = "0x..."; // Address to receive the license token(s)

async function mintLicense() {
  console.log(`Minting license token(s) for IP ID ${LICENSOR_IP_ID} under terms ${LICENSE_TERMS_ID}...`);

  try {
    const response = await storyClient.license.mintLicenseTokens({
      licenseTermsId: LICENSE_TERMS_ID,
      licensorIpId: LICENSOR_IP_ID,
      receiver: RECEIVER_ADDRESS,
      amount: 1, // Number of license tokens to mint
      // Optional parameters:
      // maxMintingFee: BigInt(0), // Set if the terms have a fee; 0 disables check if no fee expected
      // maxRevenueShare: 100, // Default check for revenue share percentage
      txOptions: { waitForTransaction: true },
    });

    console.log(
      `Successfully minted license token(s)!`
    );
    console.log(`  Transaction Hash: ${response.txHash}`);
    console.log(`  License Token ID(s): ${response.licenseTokenIds}`);

  } catch (error) {
    console.error("Error minting license token(s):", error);
  }
}

// Execute the function (after updating the constants above)
// mintLicense().catch(console.error);Before running this script:Replace LICENSOR_IP_ID with the actual IP ID obtained from your registration Replace LICENSE_TERMS_ID with the ID of license terms attached to that IP Replace RECEIVER_ADDRESS with the address to receive the license token Uncomment the function call at the bottom Then run:Finally, let's create a script to register a derivative work based on an existing IP, also using Arweave for metadata storage:import { storyClient, turboClient } from "./utils/clients";
import { createHash } from "crypto";
import { Address } from "viem";
import type { UploadResult } from "@ardrive/turbo-sdk";
import { DerivativeData } from "@story-protocol/core-sdk";

// Helper function to upload JSON to Arweave via Turbo (same as in registerIpWithArweave.ts)
async function uploadJSONToArweave(jsonData: any, description: string): Promise<UploadResult> {
    const dataBuffer = Buffer.from(JSON.stringify(jsonData));
    console.log(`Uploading ${description} (${dataBuffer.byteLength} bytes) to Arweave via Turbo...`);

    const tags = [
        { name: "Content-Type", value: "application/json" },
        { name: "App-Name", value: "ArDrive-Story-Tutorial" }
    ];

    try {
        const result = await turboClient.uploadFile(dataBuffer, { tags });
        console.log(`${description} uploaded successfully: Transaction ID ${result.id}`);
        return result;
    } catch (error) {
        console.error(`Error uploading ${description} to Arweave:`, error);
        throw new Error(`Arweave upload failed for ${description}.`);
    }
}

// --- Information about the Parent IP and License ---
const PARENT_IP_ID: Address = "0x..."; // Replace with the actual Parent IP ID
const LICENSE_TERMS_ID: string = "..."; // Replace with the License Terms ID to derive under

async function registerDerivative() {
  // --- Step 1: Define Derivative Metadata ---
  const derivativeIpMetadata = {
    title: "My Derivative Work (Arweave Metadata)",
    description: "A remix/adaptation based on a parent IP, metadata on Arweave.",
    // Add other required fields (image, creators matching the derivative creator, etc.)
  };

  const derivativeNftMetadata = {
    name: "Ownership NFT for My Derivative Work",
    description: "NFT for the derivative IP, metadata on Arweave.",
    // Add other fields
  };

  // --- Step 2: Upload Derivative Metadata to Arweave ---
  console.log("Uploading derivative metadata to Arweave via Turbo...");
  const derivIpUploadResult = await uploadJSONToArweave(derivativeIpMetadata, "Derivative IP Metadata");
  const derivNftUploadResult = await uploadJSONToArweave(derivativeNftMetadata, "Derivative NFT Metadata");

  // Use arweave.net URLs instead of ar:// protocol
  const derivIpMetadataArweaveURI = `https://arweave.net/${derivIpUploadResult.id}`;
  const derivNftMetadataArweaveURI = `https://arweave.net/${derivNftUploadResult.id}`;

  const derivIpMetadataHash = `0x${createHash("sha256")
    .update(JSON.stringify(derivativeIpMetadata))
    .digest("hex")}`;
  const derivNftMetadataHash = `0x${createHash("sha256")
    .update(JSON.stringify(derivativeNftMetadata))
    .digest("hex")}`;

  console.log(`Derivative IP Metadata Arweave URI: ${derivIpMetadataArweaveURI}`);
  console.log(`Derivative NFT Metadata Arweave URI: ${derivNftMetadataArweaveURI}`);

  // --- Step 3: Register Derivative on Story Protocol ---
  // Prepare Derivative Data for Story Protocol
  const derivData: DerivativeData = {
    parentIpIds: [PARENT_IP_ID],
    licenseTermsIds: [LICENSE_TERMS_ID],
  };

  console.log("Registering Derivative IP Asset on Story Protocol...");

  // Use the same SPG NFT contract or your own
  const spgNftContract: Address = "0xc32A8a0FF3beDDDa58393d022aF433e78739FAbc"; // Aeneid testnet example

  try {
    const response = await storyClient.ipAsset.mintAndRegisterIpAndMakeDerivative({
      spgNftContract: spgNftContract,
      derivData: derivData, // Link to parent IP and license terms
      ipMetadata: { // Metadata for the *new* derivative IP
        ipMetadataURI: derivIpMetadataArweaveURI,      // Arweave URI
        ipMetadataHash: derivIpMetadataHash as Address, // Content hash
        nftMetadataURI: derivNftMetadataArweaveURI,     // Arweave URI
        nftMetadataHash: derivNftMetadataHash as Address // Content hash
      },
      txOptions: { waitForTransaction: true },
    });

    console.log(
      `Successfully registered Derivative IP Asset!`
    );
    console.log(`  Transaction Hash: ${response.txHash}`);
    console.log(`  Derivative IP ID: ${response.ipId}`);
    console.log(`  Derivative Token ID: ${response.tokenId}`);
    console.log(`  Story Explorer Link: https://aeneid.explorer.story.foundation/ipa/${response.ipId}`);
    console.log(`  Derivative Metadata (Arweave): ${derivIpMetadataArweaveURI}`);

  } catch (error) {
    console.error("Error registering derivative IP Asset on Story Protocol:", error);
  }
}

// Before running this script:
// 1. Replace PARENT_IP_ID with a real IP ID you have access to
// 2. Replace LICENSE_TERMS_ID with the actual license terms ID
// Then uncomment the line below to execute
// registerDerivative().catch(console.error);Before running this script:Replace PARENT_IP_ID with the actual parent IP ID Replace LICENSE_TERMS_ID with the license terms ID that permits derivatives Uncomment the function execution at the bottom Run:Conclusion By leveraging the ArDrive Turbo SDK, you can seamlessly integrate permanent Arweave storage into your Story Protocol workflow. Uploading metadata with Turbo ensures fast, reliable, and cost-effective data persistence for your valuable IP Assets, whether they are root IPs or complex derivatives with licensing relationships.This tutorial demonstrated a complete workflow:Setting up a project structure with all required dependencies Creating a utility module for client initialization Registering original IP Assets with metadata stored on Arweave Minting license tokens for IP Assets Creating and registering derivative works For further details on Story Protocol concepts like licensing, derivatives, or specific SDK functions, refer to the Story Protocol Documentation.

---

# 5. Minimal Svelte Starter Kit  Cooking with the Permaweb

Document Number: 5
Source: https://cookbook.arweave.net/kits/svelte/minimal.html
Words: 566
Extraction Method: html

Minimal Svelte Starter Kit This guide will walk you through in a step by step flow to configure your development environment to build and deploy a permaweb application.Prerequisites Know typescript NodeJS v18 or greater Know Svelte - https://svelte.dev Know git and common terminal commands Development Dependencies TypeScript esbuild w3 Steps Create Project mkdir myproject
cd myproject
npm init -y
npm install -D svelte esbuild typescript esbuild-svelte tinro svelte-preprocess mkdir myproject
cd myproject
yarn init -y
yarn add -D svelte esbuild typescript esbuild-svelte tinro svelte-preprocess Create buildscript.js import fs from "fs";
import esbuild from "esbuild";
import esbuildSvelte from "esbuild-svelte";
import sveltePreprocess from "svelte-preprocess";

//make sure the directoy exists before stuff gets put into it
if (!fs.existsSync("./dist/")) {
    fs.mkdirSync("./dist/");
}
esbuild
    .build({
        entryPoints: [`./src/main.ts`],
        bundle: true,
        outdir: `./dist`,
        mainFields: ["svelte", "browser", "module", "main"],
        // logLevel: `info`,
        splitting: true,
        write: true,
        format: `esm`,
        plugins: [
            esbuildSvelte({
                preprocess: sveltePreprocess(),
            }),
        ],
    })
    .catch((error, location) => {
        console.warn(`Errors: `, error, location);
        process.exit(1);
    });

//use a basic html file to test with
fs.copyFileSync("./index.html", "./dist/index.html");Modify package.json Set type to module, add a build script {
  "type": "module"
  ...
  "scripts": {
    "build": "node buildscript.js"
  }
} Create src directory and some src files mkdir src
touch src/main.ts
touch src/app.svelte
touch src/counter.svelte
touch src/about.svelte Main.ts import App from "./app.svelte";

new App({
    target: document.body,
});app.svelte Hash Routing You will notice the router.mode.hash() setting in the script session, this is important to configure your application to use hash based routing, which will enable url support when running that application on a path, like https://[gateway]/[TX] counter.svelte <script lang="ts">
    let count = 0;

    function inc() {
        count += 1;
    }
</script>
<h1>Hello Permaweb</h1>
<button on:click="{inc}">Inc</button>
<p>Count: {count}</p> about.svelte <h1>About Page</h1>
<p>Minimal About Page</p>
<a href="/">Home</a> Add index.html <!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <link rel="icon" type="image/svg+xml" href="/vite.svg" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Vite + Svelte + TS</title>
  </head>
  <body>
    <div id="app"></div>
    <script type="module" src="./main.js"></script>
  </body>
</html> Deploy Permanently Generate Wallet We need the arweave package to generate a wallet npm install --save arweave yarn add arweave -D then run this command in the terminal node -e "require('arweave').init({}).wallets.generate().then(JSON.stringify).then(console.log.bind(console))" > wallet.json Fund Wallet You will need to fund your wallet with ArDrive Turbo credits. To do this, enter ArDrive and import your wallet. Then, you can purchase turbo credits for your wallet.Setup Permaweb-Deploy npm install --global permaweb-deploy yarn global add permaweb-deploy Update vite.config.ts import { defineConfig } from 'vite'
import { svelte } from '@sveltejs/vite-plugin-svelte'

export default defineConfig({
  plugins: [svelte()],
  base: './'
}) Update package.json {
  ...
  "scripts": {
    ...
    "deploy": "DEPLOY_KEY=$(base64 -i wallet.json) permaweb-deploy --ant-process << ANT-PROCESS >> --deploy-folder build"
  }
  ...
} Replace << ANT-PROCESS >> with your ANT process id.Run build Now it is time to generate a build, run npm run build yarn build Run deploy Finally we are good to deploy our first Permaweb Application npm run deploy yarn deploy ERROR If you receive an error Insufficient funds, make sure you remembered to fund your deployment wallet with ArDrive Turbo credits.Response You should see a response similar to the following:Deployed TxId [<<tx-id>>] to ANT [<<ant-process>>] using undername [<<undername>>] Your Svelte app can be found at https://arweave.net/<< tx-id >>.SUCCESS You should now have a Svelte Application on the Permaweb! Great Job!Summary This is a minimal version of publishing a Svelte application on the permaweb, but you may want more features, like hot-reloading and tailwind, etc. Check out hypar for a turnkey starter kit. HypAR

---

# 6. wasm6410 - HyperBEAM - Documentation

Document Number: 6
Source: https://hyperbeam.arweave.net/build/devices/wasm64-at-1-0.html
Words: 538
Extraction Method: html

Device: ~wasm64@1.0 Overview The ~wasm64@1.0 device enables the execution of 64-bit WebAssembly (WASM) code within the HyperBEAM environment. It provides a sandboxed environment for running compiled code from various languages (like Rust, C++, Go) that target WASM.Core Concept: WASM Execution This device allows AO processes to perform complex computations defined in WASM modules, which can be written in languages like Rust, C++, C, Go, etc., and compiled to WASM.The device manages the lifecycle of a WASM instance associated with the process state.Key Functions (Keys) These keys are typically used within an execution stack (managed by dev_stack) for an AO process.init Action: Initializes the WASM environment for the process. It locates the WASM image (binary), starts a WAMR instance, and stores the instance handle and helper functions (for reading/writing WASM memory) in the process's private state (priv/...).Inputs (Expected in Process Definition or init Message):[Prefix]/image: The Arweave Transaction ID of the WASM binary, or the WASM binary itself, or a message containing the WASM binary in its body.[Prefix]/Mode: (Optional) Specifies execution mode (WASM (default) or AOT if allowed by node config).Outputs (Stored in priv/):[Prefix]/instance: The handle to the running WAMR instance.[Prefix]/write: A function to write data into the WASM instance's memory.[Prefix]/read: A function to read data from the WASM instance's memory.[Prefix]/import-resolver: A function used to handle calls from the WASM module back to the AO environment (imports).compute Action: Executes a function within the initialized WASM instance. It retrieves the target function name and parameters from the incoming message or process definition and calls the WASM instance via hb_beamr.Inputs (Expected in Process State or Incoming Message):priv/[Prefix]/instance: The handle obtained during init.function or body/function: The name of the WASM function to call.parameters or body/parameters: A list of parameters to pass to the WASM function.Outputs (Stored in results/):results/[Prefix]/type: The result type returned by the WASM function.results/[Prefix]/output: The actual result value returned by the WASM function.import Action: Handles calls originating from the WASM module (imports). The default implementation (default_import_resolver) resolves these calls by treating them as sub-calls within the AO environment, allowing WASM code to invoke other AO device functions or access process state via the hb_ao:resolve mechanism.Inputs (Provided by hb_beamr): Module name, function name, arguments, signature.Response: Returns the result of the resolved AO call back to the WASM instance.snapshot Action: Captures the current memory state of the running WASM instance. This is used for checkpointing and restoring process state.Inputs:priv/[Prefix]/instance.Outputs: A message containing the raw binary snapshot of the WASM memory state, typically tagged with [Prefix]/State.normalize (Internal Helper) Action: Ensures a consistent state representation for computation, primarily by loading a WASM instance from a snapshot ([Prefix]/State) if a live instance (priv/[Prefix]/instance) isn't already present. This allows resuming execution from a cached state.terminate Action: Stops and cleans up the running WASM instance associated with the process.Inputs:priv/[Prefix]/instance.Usage within dev_stack The ~wasm64@1.0 device is almost always used as part of an execution stack configured in the Process Definition Message and managed by dev_stack. dev_stack ensures that init is called on the first pass, compute on subsequent passes, and potentially snapshot or terminate as needed.# Example Process Definition Snippet
Execution-Device: [`stack@1.0`](./source-code/dev_stack.md)
Execution-Stack: "[`scheduler@1.0`](./source-code/dev_scheduler.md)", "wasm64@1.0"
WASM-Image: <WASMImageTxID> This setup allows AO processes to leverage the computational power and language flexibility offered by WebAssembly in a decentralized, verifiable manner.wasm module

---

# 7. useWayfinder - ARIO Docs

Document Number: 7
Source: https://docs.ar.io/wayfinder/react/use-wayfinder
Words: 891
Extraction Method: html

Overview The useWayfinder hook provides access to the complete Wayfinder instance from the React context. This hook gives you full control over all Wayfinder methods and is ideal for advanced usage scenarios where you need access to multiple Wayfinder capabilities.Signature Usage Basic Usage Advanced Usage with Event Listeners import { useWayfinder } from '@ar.io/wayfinder-react'
import { useEffect, useState } from 'react'

function AdvancedComponent() {
  const { wayfinder } = useWayfinder()
  const [events, setEvents] = useState([])

  useEffect(() => {
    // Listen to routing events
    const handleRoutingSuccess = (event) => {
      setEvents((prev) => [
        ...prev,
        {
          type: 'routing-success',
          data: event,
          timestamp: Date.now(),
        },
      ])
    }

    const handleRoutingFailed = (error) => {
      setEvents((prev) => [
        ...prev,
        {
          type: 'routing-failed',
          data: error,
          timestamp: Date.now(),
        },
      ])
    }

    const handleVerificationSuccess = (event) => {
      setEvents((prev) => [
        ...prev,
        {
          type: 'verification-success',
          data: event,
          timestamp: Date.now(),
        },
      ])
    }

    // Subscribe to events
    wayfinder.emitter.on('routing-succeeded', handleRoutingSuccess)
    wayfinder.emitter.on('routing-failed', handleRoutingFailed)
    wayfinder.emitter.on('verification-succeeded', handleVerificationSuccess)

    // Cleanup
    return () => {
      wayfinder.emitter.off('routing-succeeded', handleRoutingSuccess)
      wayfinder.emitter.off('routing-failed', handleRoutingFailed)
      wayfinder.emitter.off('verification-succeeded', handleVerificationSuccess)
    }
  }, [wayfinder])

  return (
    <div>
      <h3>Wayfinder Events</h3>
      <ul>
        {events.map((event, index) => (
          <li key={index}>
            {event.type} - {new Date(event.timestamp).toLocaleTimeString()}
          </li>
        ))}
      </ul>
    </div>
  )
} Custom Request with Overrides import { useWayfinder } from '@ar.io/wayfinder-react'
import { useState } from 'react'
import {
  StaticRoutingStrategy,
  HashVerificationStrategy,
} from '@ar.io/wayfinder-core'

function CustomRequestComponent({ txId }) {
  const { wayfinder } = useWayfinder()
  const [data, setData] = useState(null)
  const [loading, setLoading] = useState(false)
  const [error, setError] = useState(null)

  const fetchWithCustomSettings = async () => {
    setLoading(true)
    setError(null)

    try {
      // Override routing and verification for this specific request
      const response = await wayfinder.request(`ar://${txId}`, {
        routingSettings: {
          strategy: new StaticRoutingStrategy({
            gateway: 'https://arweave.net',
          }),
        },
        verificationSettings: {
          enabled: true,
          strict: true,
          strategy: new HashVerificationStrategy({
            trustedGateways: ['https://arweave.net'],
          }),
        },
      })

      const text = await response.text()
      setData(text)
    } catch (err) {
      setError(err)
    } finally {
      setLoading(false)
    }
  }

  return (
    <div>
      <button onClick={fetchWithCustomSettings} disabled={loading}>
        {loading
          ? 'Fetching with custom settings...'
          : 'Fetch with Custom Settings'}
      </button>

      {error && <div className="error">Error: {error.message}</div>}
      {data && <pre>{data}</pre>}
    </div>
  )
} Provider Context Error The hook throws an error if used outside of a WayfinderProvider:Proper Error Handling import { useWayfinder } from '@ar.io/wayfinder-react'
import { useState, useCallback } from 'react'

function RobustComponent() {
  const { wayfinder } = useWayfinder()
  const [error, setError] = useState(null)
  const [data, setData] = useState(null)
  const [loading, setLoading] = useState(false)

  const handleRequest = useCallback(
    async (txId) => {
      setLoading(true)
      setError(null)

      try {
        const response = await wayfinder.request(`ar://${txId}`)
        const result = await response.text()
        setData(result)
      } catch (err) {
        setError(err)

        // Log different error types
        if (err.name === 'TimeoutError') {
          console.error('Request timed out')
        } else if (err.name === 'VerificationError') {
          console.error('Data verification failed')
        } else if (err.name === 'NetworkError') {
          console.error('Network error occurred')
        } else {
          console.error('Unknown error:', err)
        }
      } finally {
        setLoading(false)
      }
    },
    [wayfinder],
  )

  const clearError = useCallback(() => {
    setError(null)
  }, [])

  return (
    <div>
      {error && (
        <div className="error-banner">
          <p>Error: {error.message}</p>
          <button onClick={clearError}>Dismiss</button>
          <button onClick={() => handleRequest('retry')}>Retry</button>
        </div>
      )}

      {loading && <div>Loading...</div>}
      {data && <pre>{data}</pre>}
    </div>
  )
} Performance Considerations Memoization The Wayfinder instance is automatically memoized in the provider, but you should memoize callbacks that use it:Event Listener Cleanup Always clean up event listeners to prevent memory leaks:TypeScript Support Typed Usage import { useWayfinder } from '@ar.io/wayfinder-react'
import { WayfinderContextValue } from '@ar.io/wayfinder-react'
import { Wayfinder, WayfinderEvent } from '@ar.io/wayfinder-core'

interface ComponentProps {
  txId: string
  onSuccess?: (data: string) => void
  onError?: (error: Error) => void
}

const TypedComponent: React.FC<ComponentProps> = ({
  txId,
  onSuccess,
  onError,
}) => {
  const context: WayfinderContextValue = useWayfinder()
  const wayfinder: Wayfinder = context.wayfinder

  const handleFetch = async (): Promise<void> => {
    try {
      const response = await wayfinder.request(`ar://${txId}`)
      const data = await response.text()
      onSuccess?.(data)
    } catch (error) {
      onError?.(error as Error)
    }
  }

  // Type-safe event handling
  const handleRoutingEvent = (
    event: WayfinderEvent['routing-succeeded'],
  ): void => {
    console.log('Selected gateway:', event.selectedGateway)
  }

  return <button onClick={handleFetch}>Fetch Data</button>
} Custom Hook with TypeScript import { useWayfinder } from '@ar.io/wayfinder-react'
import { useState, useCallback } from 'react'

interface UseWayfinderDataResult {
  data: string | null
  loading: boolean
  error: Error | null
  fetchData: (txId: string) => Promise<void>
  clearData: () => void
}

function useWayfinderData(): UseWayfinderDataResult {
  const { wayfinder } = useWayfinder()
  const [data, setData] = useState<string | null>(null)
  const [loading, setLoading] = useState<boolean>(false)
  const [error, setError] = useState<Error | null>(null)

  const fetchData = useCallback(
    async (txId: string): Promise<void> => {
      setLoading(true)
      setError(null)

      try {
        const response = await wayfinder.request(`ar://${txId}`)
        const result = await response.text()
        setData(result)
      } catch (err) {
        setError(err as Error)
      } finally {
        setLoading(false)
      }
    },
    [wayfinder],
  )

  const clearData = useCallback((): void => {
    setData(null)
    setError(null)
  }, [])

  return {
    data,
    loading,
    error,
    fetchData,
    clearData,
  }
}

// Usage
function MyComponent() {
  const { data, loading, error, fetchData, clearData } = useWayfinderData()

  return (
    <div>
      <button onClick={() => fetchData('transaction-id')}>Fetch</button>
      <button onClick={clearData}>Clear</button>
      {loading && <div>Loading...</div>}
      {error && <div>Error: {error.message}</div>}
      {data && <pre>{data}</pre>}
    </div>
  )
} Testing Mocking the Hook When to Use Use useWayfinder when you need:Full Wayfinder API access: Access to all methods like request(), resolveUrl(), and event emitters Event monitoring: Listening to routing, verification, or other Wayfinder events Custom request configurations: Overriding routing or verification settings per request Advanced integrations: Building complex components that need multiple Wayfinder capabilities Custom abstractions: Creating your own hooks or utilities that wrap Wayfinder functionality For simpler use cases, consider:useWayfinderRequest for basic data fetching useWayfinderUrl for URL resolution with loading states

---

# 8. Create React App Starter Kit  Cooking with the Permaweb

Document Number: 8
Source: https://cookbook.arweave.net/kits/react/create-react-app.html
Words: 701
Extraction Method: html

Create React App Starter Kit This guide will walk you through in a step by step flow to configure your development environment to build and deploy a permaweb react application.Prerequisites Basic Typescript Knowledge (Not Mandatory) - [https://www.typescriptlang.org/docs/](Learn Typescript) NodeJS v16.15.0 or greater - [https://nodejs.org/en/download/](Download NodeJS) Knowledge of ReactJS - [https://reactjs.org/](Learn ReactJS) Know git and common terminal commands Development Dependencies TypeScript NPM or Yarn Package Manager Steps Create Project If you are not familiar with typescript you can exclude the extra check --template typescript npx create-react-app permaweb-create-react-app --template typescript yarn create react-app permaweb-create-react-app --template typescript Change into the Project Directory cd permaweb-create-react-app Install react-router-dom You have to install this package to manage routing between different pages npm install react-router-dom --save yarn add react-router-dom -D Run the App Now we need to check if everything is working before jumping into next step, run npm start yarn start This will start a new development server locally on your machine. By default it uses `PORT 3000`, if this PORT is already in use it may ask you to switch to another available PORT in Terminal Modify the package.json to contain the following config Setup Routing Now modify the application and add a new route such as an about page, first create 2 more.tsx files. (if you have exluceded the extra check --template typescript, then your component file extension should be .jsx or .js) HomePage.tsx About.tsx import { Link } from "react-router-dom";

function About() {
    return (
    <div>
            Welcome to the About page!
            <Link to={"/"}>
                <div>Home</div>
            </Link>
        </div>
    );
}

export default About;Modify App.tsx We need to update the App.tsx to manage the different pages Hash Routing Note that we are wrapping the routes in a HashRouter and using the react-router-dom Link component to build links. This is important on the permaweb in its current state, it will ensure the routes work properly because applications are served on a path like https://[gateway]/[TX] Deploy Permanently Generate Wallet Existing Wallet This step will generate a new, empty, Arweave wallet. If you already have an existing Arweave wallet you may provide its keyfile and skip this step.We need the arweave package to generate a wallet npm install --save arweave yarn add arweave -D then run this command in the terminal node -e "require('arweave').init({}).wallets.generate().then(JSON.stringify).then(console.log.bind(console))" > wallet.json It is very important to make sure that your wallet file is not included in any folder you want uploaded to Arweave.Setup Turbo We need Turbo to deploy our app to the Permaweb.Fund Wallet You will need to fund your wallet with ArDrive Turbo credits. To do this, enter ArDrive and import your wallet. Then, you can purchase turbo credits for your wallet.Setup Permaweb-Deploy npm install --global permaweb-deploy yarn global add permaweb-deploy Fund Your Wallet Turbo uses Turbo Credits to upload data to Arweave. You can purchase Turbo Credits with a variety of fiat currencies or crypto tokens. Below is an example for funding your wallet with 10 USD. It will open a browser window to complete the purchase using Stripe.npm install @ardrive/turbo-sdk
turbo top-up --wallet-file wallet.json --currency USD --value 10 Be sure to replace wallet.json with the path to your Arweave wallet.Update package.json {
  ...
  "scripts": {
    ...
    "deploy": "turbo upload-folder --folder-path ./build --wallet-file wallet.json > latest-manifest.json"
  }
  ...
} This will upload your build folder to the permaweb, and save all of the details of the upload to a file named "latest-manifest.json". That way, you'll have a reference for the manifest TxId to use later.Run build Now it is time to generate a build, run npm run build yarn build Run deploy Finally we are good to deploy our first Permaweb Application npm run deploy yarn deploy ERROR If you receive an error Insufficient funds, make sure you remembered to fund your deployment wallet with ArDrive Turbo credits.Response You should see a response similar to the following:Deployed TxId [<<tx-id>>] to ANT [<<ant-process>>] using undername [<<undername>>] Your React app can be found at https://arweave.net/<< tx-id >>.SUCCESS You should now have a React Application on the Permaweb! Great Job!Summary This is a Create React App version of publishing a React app on the permaweb. You may discover new ways to deploy an app on the permaweb or checkout other starter kits in this guide!

---

# 9. TEE Nodes - HyperBEAM - Documentation

Document Number: 9
Source: https://hyperbeam.arweave.net/run/tee-nodes.html
Words: 681
Extraction Method: html

Trusted Execution Environment (TEE) Recommended Setup Use HyperBEAM OS for the easiest TEE deployment with pre-configured AMD SEV-SNP support. Note: HB-OS is typically used for TEE operations, but is not necessary for router registration.Overview HyperBEAM supports Trusted Execution Environments (TEEs) through the ~snp@1.0 device, enabling secure, verifiable computation on remote machines. TEEs provide hardware-level isolation and cryptographic attestation that allows users to verify their code is running in a protected environment exactly as intended, even on untrusted hardware.The ~snp@1.0 device generates and validates attestation reports that prove:Code is running inside a genuine AMD SEV-SNP TEE The execution environment hasn't been tampered with Specific software components (firmware, kernel, initramfs) match trusted hashes Debug mode is disabled for security Configuration Files Configuration can be set in either config.json (JSON) or config.flat (flat) format. For full details and examples of both formats, see Configuration Reference.The examples below use JSON for clarity.When to use HB-OS Operation Use HB-OS?Purpose TEE Node (SNP) Recommended Secure, attested computation (hardware isolation) Router Registration Optional Registering/joining a router (TEE not required) If you are registering or running a router, you can do so without HB-OS.If you want to run a TEE node, HB-OS or an equivalent TEE setup is recommended for convenience and security.Quick Start: TEE Node with HyperBEAM OS Prerequisites AMD EPYC processor with SEV-SNP support (Milan generation or newer) Host system with SEV-SNP enabled in BIOS Setup TEE Node # Clone and build TEE-enabled HyperBEAM
# (Only needed for TEE nodes if you choose HB-OS)
git clone https://github.com/permaweb/hb-os.git && cd hb-os
./run init && ./run setup_host && ./run build_base_image && ./run build_guest_image

# Launch TEE-protected node
./run start The VM boots with dm-verity protection, measured boot, and automatic attestation report generation.Using the SNP Device Generate Attestation Report Request an attestation report from a TEE node:curl https://your-tee-node.com/~snp@1.0/generate Returns a signed attestation report containing:
- Nonce: Unique identifier preventing replay attacks
- Address: Node's ephemeral public key (only exists inside TEE)
- Measurement: Cryptographic hash of the execution environment
- Report: AMD SEV-SNP hardware attestation with certificate chain Verify Attestation Report The verification process validates:
1. Nonce integrity: Ensures report freshness and prevents replay
2. Signature validity: Confirms the report was signed by the claimed address
3. Address authenticity: Verifies the signing key exists only in the TEE
4. Debug disabled: Ensures no debugging capabilities that could compromise security
5. Trusted software: Validates firmware, kernel, and initramfs hashes match approved versions
6. Measurement accuracy: Confirms the reported environment matches actual execution
7. Hardware attestation: Verifies AMD's cryptographic signature on the report Configuration Trusted Software Hashes (config.json example) Configure which software components are trusted by setting snp_trusted in your node options:"snp_trusted": [
  // Trusted software hashes here
] Custom Trust Validation Implement custom trust policies by specifying an is-trusted-device:curl -X POST https://your-node.com/~snp@1.0/verify \
  -H "is-trusted-device: my-custom-validator@1.0" \
  -d '{"report": "...", "target": "self"}' Security Considerations SEV-SNP capable CPU: AMD EPYC Milan or newer Firmware support: Recent AMD firmware with SEV-SNP enabled Memory encryption: SME (Secure Memory Encryption) recommended RMP table: Sufficient memory reserved for Reverse Map Page Table Attestation Tools HyperBEAM OS includes several attestation utilities:get_report: Generate attestation reports with custom data verify_report: Validate attestation report signatures sev_feature_info: Check host SEV-SNP capabilities idblock_generator: Create signed VM configuration blocks Integration Examples Router Registration with TEE (Advanced, config.json example) If you want to register a TEE-protected router node, use the following configuration (see also the router registration guide):{
  "operator": "trustless",
  "initialized": "permanent",
  "snp_trusted": [ /* ... */ ],
  "on": {
    "request": {
      "device": "p4@1.0",
      "ledger-device": "lua@5.3a",
      "pricing-device": "simple-pay@1.0",
      "ledger-path": "/ledger~node-process@1.0",
      "module": ""
    },
    "response": {
      "device": "p4@1.0",
      "ledger-device": "lua@5.3a",
      "pricing-device": "simple-pay@1.0",
      "ledger-path": "/ledger~node-process@1.0",
      "module": ""
    }
  },
  "p4_non_chargable_routes": [
    {"template": "/.*~node-process@1.0/.*"},
    {"template": "/.*~greenzone@1.0/.*"},
    {"template": "/.*~router@1.0/.*"},
    {"template": "/.*~meta@1.0/.*"},
    {"template": "/schedule"},
    {"template": "/push"},
    {"template": "/~hyperbuddy@1.0/.*"}
  ],
  "node_process_spawn_codec": "ans104@1.0",
  "node_processes": {
    "ledger": {
      "device": "process@1.0",
      "execution-device": "lua@5.3a",
      "scheduler-device": "scheduler@1.0",
      "authority-match": 1,
      "admin": "",
      "token": "",
      "module": "",
      "authority": ""
    }
  },
  "router_opts": {
    "offered": [ /* ... */ ]
  },
  "green_zone_peer_location": "",
  "green_zone_peer_id": "",
  "p4_recipient": ""
} TEE attestation TEE-protected computation Trusted software validation HyperBEAM OS Repository See the router registration guide for non-TEE router setup.Configuration Reference

---

# 10. React Starter Kit wvite  ArDrive  Cooking with the Permaweb

Document Number: 10
Source: https://cookbook.arweave.net/kits/react/turbo.html
Words: 493
Extraction Method: html

React Starter Kit w/vite & ArDrive This guide will walk you through in a step by step flow to configure your development environment to build and deploy a permaweb react application.Prerequisites Basic Typescript Knowledge (Not Mandatory) - [https://www.typescriptlang.org/docs/](Learn Typescript) NodeJS v16.15.0 or greater - [https://nodejs.org/en/download/](Download NodeJS) Knowledge of ReactJS - [https://reactjs.org/](Learn ReactJS) Know git and common terminal commands Development Dependencies TypeScript NPM or Yarn Package Manager Steps Create React App npm create vite my-arweave-app --template react-ts
cd my-arweave-app
npm install yarn create vite my-arweave-app --template react-ts
cd my-arweave-app
yarn Add React Router DOM npm install react-router-dom yarn add react-router-dom We need to use the hash-router to create a working app on arweave.Page Components touch src/Home.tsx src/About.tsx src/Home.tsx import { Link } from "react-router-dom";

function Home() {
    return (
        <div>
            Welcome to the Permaweb!
            <Link to={"/about/"}>
                <div>About</div>
            </Link>
        </div>
    );
}

export default Home;src/About.tsx import { Link } from "react-router-dom";

function About() {
    return (
        <div>
            Welcome to the About page!
            <Link to={"/"}>
                <div>Home</div>
            </Link>
        </div>
    );
}

export default About;Modify App.tsx We need to update the App.tsx to manage different pages import { HashRouter } from "react-router-dom";
import { Routes, Route } from "react-router-dom";

import Home from "./Home";
import About from "./About";

function App() {
    return (
        <HashRouter>
            <Routes>
                <Route path={"/"} element={<Home />} />
                <Route path={"/about/"} element={<About />} />
            </Routes>
        </HashRouter>
    );
}

export default App;Modify index.css Alter the body selector body {
  margin: 0;
  padding-top: 200px;
  display: flex;
  flex-direction: column;
  place-items: center;
  min-width: 100%;
  min-height: 100vh;
} Run the project npm run dev yarn dev Building React App Modify vite.config.ts import { defineConfig } from 'vite'
import react from '@vitejs/plugin-react'

// https://vitejs.dev/config/
export default defineConfig({
  base: "",
  plugins: [react()],
}) Build App yarn build Deploy Permanently Generate Wallet We need the arweave package to generate a wallet npm install --save arweave yarn add arweave -D then run this command in the terminal node -e "require('arweave').init({}).wallets.generate().then(JSON.stringify).then(console.log.bind(console))" > wallet.json Fund Wallet You will need to fund your wallet with ArDrive Turbo credits. To do this, enter ArDrive and import your wallet. Then, you can purchase turbo credits for your wallet.Setup Permaweb-Deploy npm install --global permaweb-deploy yarn global add permaweb-deploy Update package.json {
  ...
  "scripts": {
    ...
    "deploy": "DEPLOY_KEY=$(base64 -i wallet.json) permaweb-deploy --ant-process << ANT-PROCESS >> "
  }
  ...
} Replace << ANT-PROCESS >> with your ANT process id.Run build Now it is time to generate a build, run npm run build yarn build Run deploy Finally we are good to deploy our first Permaweb Application npm run deploy yarn deploy ERROR If you receive an error Insufficient funds, make sure you remembered to fund your deployment wallet with ArDrive Turbo credits.Response You should see a response similar to the following:Deployed TxId [<<tx-id>>] to ANT [<<ant-process>>] using undername [<<undername>>] Your React app can be found at https://arweave.net/<< tx-id >>.SUCCESS You should now have a React Application on the Permaweb! Great Job!Congrats!You just published a react application on the Permaweb! This app will be hosted forever!

---

# 11. ARIO Docs

Document Number: 11
Source: https://docs.ar.io/wayfinder/getting-started
Words: 590
Extraction Method: html

Getting Started with Wayfinder Wayfinder provides decentralized and verified access to data stored on Arweave. This guide will help you get started with the core concepts and basic usage.Installation Choose the package that fits your project:Core Library (JavaScript/TypeScript) React Components Quick Start Basic Usage The simplest way to get started is with the default configuration:React Integration For React applications, use the wayfinder-react package:import { LocalStorageGatewaysProvider, NetworkGatewaysProvider } from '@ar.io/wayfinder-core'
import { WayfinderProvider, useWayfinder } from '@ar.io/wayfinder-react'

// Wrap your app with the provider
function App() {
  return (
    <WayfinderProvider
      gatewaysProvider={new LocalStorageGatewaysProvider({
        gatewaysProvider: new NetworkGatewaysProvider({
          ario: ARIO.mainnet()
        })
      })};
      routingSettings={{
        strategy: new FastestPingRoutingStrategy({ timeoutMs: 500 }),
      }}
    >
      <MyComponent />
    </WayfinderProvider>
  )
}

function YourComponent() {
  const txId = 'your-transaction-id'; // Replace with actual txId

  // Use custom hooks for URL resolution and data fetching
  const request = useWayfinderRequest();

  // store the fetched data
  const [data, setData] = useState<any>(null);
  const [dataLoading, setDataLoading] = useState(false);
  const [dataError, setDataError] = useState<Error | null>(null);

  useEffect(() => {
    (async () => {
      try {
        setDataLoading(true);
        setDataError(null);
        // fetch the data for the txId using wayfinder
        const response = await request(`ar://${txId}`, {
          verificationSettings: {
            enabled: true, // enable verification on the request
            strict: true, // don't use the data if it's not verified
          },
        });
        const data = await response.arrayBuffer(); // or response.json() if you want to parse the data as JSON
        setData(data);
      } catch (error) {
        setDataError(error as Error);
      } finally {
        setDataLoading(false);
      }
    })();
  }, [request, txId]);

  return (
    <div>
      {dataLoading && <p>Loading data...</p>}
      {dataError && <p>Error loading data: {dataError.message}</p>}
      <pre>{data}</pre>
    </div>
  );
} Available Strategies Routing Strategies ← Swipe to see more → Strategy Description Use Case FastestPingRoutingStrategy Selects gateway with lowest latency Performance-critical applications PreferredWithFallbackRoutingStrategy Tries preferred gateway first, falls back to others When you have a trusted primary gateway RoundRobinRoutingStrategy Distributes requests evenly across gateways Load balancing and fair distribution RandomRoutingStrategy Randomly selects from available gateways Simple load distribution ← Swipe to see more → Verification Strategies Verification strategies may be dependent on the gateway being used having the
data indexed locally. A gateway cannot verify data it doesn't have access to
or hasn't indexed yet.Advanced Configuration For production applications, you'll want to configure gateway providers, routing strategies, and verification:import {
  Wayfinder,
  NetworkGatewaysProvider,
  FastestPingRoutingStrategy,
  HashVerificationStrategy,
} from '@ar.io/wayfinder-core'
import { ARIO } from '@ar.io/sdk'

const wayfinder = new Wayfinder({
  // Discover gateways from the AR.IO Network
  gatewaysProvider: new SimpleCacheGatewaysProvider({
    gatewaysProvider: new NetworkGatewaysProvider({
      ario: ARIO.mainnet(),
      limit: 10,
      sortBy: 'operatorStake',
      sortOrder: 'desc',
    }),
  }),

  // Use fastest ping routing strategy
  routingSettings: {
    strategy: new FastestPingRoutingStrategy({
      timeoutMs: 500,
    }),
    events: {
      onRoutingSucceeded: (event) => {
        console.log('Selected gateway:', event.selectedGateway)
      },
    },
  },

  // Enable data verification
  verificationSettings: {
    enabled: true,
    strategy: new HashVerificationStrategy({
      trustedGateways: ['https://arweave.net'],
    }),
    events: {
      onVerificationSucceeded: (event) => {
        console.log('Verification passed for:', event.txId)
      },
      onVerificationFailed: (event) => {
        console.log('Verification failed for:', event.txId)
      },
    },
  },

  // Enable telemetry
  telemetrySettings: {
    enabled: true,
    clientName: 'my-app',
    clientVersion: '1.0.0',
    sampleRate: 0.1, // 10% sampling
  },
}) Core Concepts Gateway Providers Gateway providers discover and manage the list of available AR.IO gateways:NetworkGatewaysProvider: Fetches gateways from the AR.IO Network StaticGatewaysProvider: Uses a predefined list of gateways SimpleCacheGatewaysProvider: Caches gateway lists for performance in-memory LocalStorageGatewaysProvider Caches gateway lists for performance in window.localStorage Routing Strategies Routing strategies determine which gateway to use for each request:FastestPingRoutingStrategy: Selects the gateway with lowest latency PreferredWithFallbackRoutingStrategy: Tries a preferred gateway first RoundRobinRoutingStrategy: Distributes requests evenly RandomRoutingStrategy: Randomly selects gateways Verification Strategies Verification strategies ensure data integrity:HashVerificationStrategy: Verifies data against trusted gateway hashes SignatureVerificationStrategy: Validates Arweave transaction signatures DataRootVerificationStrategy: Verifies against transaction data roots

---

# 12. Fetching Transaction Data  Cooking with the Permaweb

Document Number: 12
Source: https://cookbook.arweave.net/guides/http-api.html
Words: 563
Extraction Method: html

Fetching Transaction Data While indexing services allow querying of transaction metadata they don't provide access to the transaction data itself. This is because caching transaction data and indexing metadata have different resource requirements. Indexing services primarily rely on compute resources to perform queries on a database while transaction data is better suited to deployment on a Content Delivery Network (CDN) to optimize storage and bandwidth.A Transaction data caching service is offered by most gateways though a set of HTTP endpoints. Any HTTP client/package can be used to request transaction data from these endpoints. For example Axios or Fetch for JavaScript, Guzzle for PHP, etc.If you wanted to bypass a transaction data caching service and get data directly from the Arweave peers/nodes you could, but it's a lot of work!Transaction data is stored on Arweave as a contiguous sequence of 256KB chunks, from the very beginning of the network until the current block. This format is optimized to support the SPoRA mining mechanism miners participate in to prove they are storing Arweave data.Retrieve a list of peers from a well known peer.Ask the peer for the chunk offsets which contain your transactions data.Ask the peer to for the chunks. If the peer provides the chunks, combine them back into their original format.(If the peer does not have the chunks) walk the peer list asking for the chunks.For each peer you visit, check their peer list and add peers not already in your list.Repeat from step 3 until you have all of the chunks.This is a fairly large amount of work to perform each time you want to retrieve data from the Arweave network. Imagine if you were trying to display a timeline of tweets like https://public-square.arweave.net does. The user experience would be terrible with long load times and spinners. Because data on Arweave is permanent, it's safe to cache in its original form to make retrieval of transaction data much quicker and easier.The following is how to access cached transaction data in the arweave.net Transaction data caching service.Get cached TX data https://arweave.net/TX_ID const res = await axios.get(`https://arweave.net/sHqUBKFeS42-CMCvNqPR31yEP63qSJG3ImshfwzJJF8`)
console.log(res) Click to view example result  Each Arweave peer/node also exposes some HTTP endpoints which are often replicated gateways. You can read more about Arweave peer's HTTP endpoints here.Get raw transaction https://arweave.net/raw/TX_ID const result = await fetch('https://arweave.net/raw/rLyni34aYMmliemI8OjqtkE_JHHbFMb24YTQHGe9geo')
  .then(res => res.json())
  console.log(JSON.stringify(result)) Click to view example result {
  "manifest": "arweave/paths",
  "version": "0.1.0",
  "index": {
    "path": "index.html"
  },
  "paths": {
    "index.html": {
      "id": "FOPrEoqqk184Bnk9KrnQ0MTZFOM1oXb0JZjJqhluv78"
    }
  }
}  Get by field https://arweave.net/tx/TX_ID/FIELD Available fields: id | last_tx | owner | target | quantity | data | reward | signature const result = await fetch('https://arweave.net/sHqUBKFeS42-CMCvNqPR31yEP63qSJG3ImshfwzJJF8/data')
  .then(res => res.json())
  console.log(JSON.stringify(result)) Click to view example result {
  "ticker":"ANT-PENDING",
  "name":"pending",
  "owner":"NlNd_PcajvxAkOweo7rZHJKiIJ7vW1WXt9vb6CzGmC0",
  "controller":"NlNd_PcajvxAkOweo7rZHJKiIJ7vW1WXt9vb6CzGmC0",
  "evolve":null,
  "records": {
    "@":"As-g0fqvO_ALZpSI8yKfCZaFtnmuwWasY83BQ520Duw"
  },
  "balances":{"NlNd_PcajvxAkOweo7rZHJKiIJ7vW1WXt9vb6CzGmC0":1}
}  Get Wallet Balance The returned balance is in Winston. To get balance in $AR, divide the balance by 1000000000000 https://arweave.net/wallet/ADDRESS/balance const res = await axios.get(`https://arweave.net/wallet/NlNd_PcajvxAkOweo7rZHJKiIJ7vW1WXt9vb6CzGmC0/balance`)
console.log(res)
console.log(res.data / 1000000000000)

6638463438702 // Winston
6.638463438702 // $AR Get transaction status https://arweave.net/tx/TX_ID/status TIP This endpoint only supports native Arweave transactions. Transactions must be confirmed before getting a successful response.const result = await fetch('https://arweave.net/tx/EiRSQExb5HvSynpn0S7_dDnwcws1AJMxoYx4x7nWoho/status').then(res => res.json())
  console.log(JSON.stringify(result)) Click to view example result {
  "block_height":1095552,"block_indep_hash":"hyhLEyOw5WcIhZxq-tlnxhnEFgKChKHFrMoUdgIg2Sw0WoBMbdx6uSJKjxnQWon3","number_of_confirmations":10669
}  Get network information const res = await axios.get('https://arweave.net/info')
console.log(res.data) Click to view example result {
    "network": "arweave.N.1",
    "version": 5,
    "release": 53,
    "height": 1106211,
    "current": "bqPU_7t-TdRIxgsja0ftgEMNnlGL6OX621LPJJzYP12w-uB_PN4F7qRYD-DpIuRu",
    "blocks": 1092577,
    "peers": 13922,
    "queue_length": 0,
    "node_state_latency": 0
}

---

# 13. Create Vue Starter Kit  Cooking with the Permaweb

Document Number: 13
Source: https://cookbook.arweave.net/kits/vue/create-vue.html
Words: 707
Extraction Method: html

Create Vue Starter Kit This guide will provide step-by-step instructions to configure your development environment and build a permaweb Vue application.Prerequisites Basic Typescript Knowledge (Not Mandatory) - Learn Typescript NodeJS v16.15.0 or greater - Download NodeJS Knowledge of Vue.js (preferably Vue 3) - Learn Vue.js Know git and common terminal commands Development Dependencies TypeScript (Optional) NPM or Yarn Package Manager Steps Create Project The following command installs and launches create-vue, the official scaffolding tool for Vue projects.npm init vue@latest yarn create vue During the process, you'll be prompted to select optional features such as TypeScript and testing support. I recommend selecting the Vue Router with yes, the rest can be selected as per your preference.✔ Project name: … <your-project-name>
✔ Add TypeScript? … No / Yes
✔ Add JSX Support? … No / Yes
✔ Add Vue Router for Single Page Application development? … No / *Yes*
✔ Add Pinia for state management? … No / Yes
✔ Add Vitest for Unit testing? … No / Yes
✔ Add Cypress for both Unit and End-to-End testing? … No / Yes
✔ Add ESLint for code quality? … No / Yes
✔ Add Prettier for code formatting? … No / Yes Change into the Project Directory cd <your-project-name> Install Dependencies npm install yarn Setup Router Vue Router is the official router for Vue.js and seamlessly integrates with Vue. To make it work with Permaweb, switch from a browser history router to a hash router as the URL cannot be sent to the server. Change createWebHistory to createWebHashHistory in your src/router/index.ts or src/router/index.js file.import { createRouter, createWebHashHistory } from "vue-router";
import HomeView from "../views/HomeView.vue";

const router = createRouter({
    history: createWebHashHistory(import.meta.env.BASE_URL),
    routes: [
        {
            path: "/",
            name: "home",
            component: HomeView,
        },
        {
            path: "/about",
            name: "about",
            component: () => import("../views/AboutView.vue"),
        },
    ],
});

export default router;Setup Build Configure the build process in the vite.config.ts or vite.config.js file. To serve Permaweb apps from a sub-path (https://[gateway]/[TX_ID]), update the base property to./ in the config file.export default defineConfig({
  base: './',
  ...
}) Run the App Before moving forward, it is crucial to verify that everything is working correctly. Run a check to ensure smooth progress.npm run dev yarn dev it will start a new development server locally on your machine by default it uses `PORT 5173`. If this PORT is already in use it may increase the PORT number by 1 (`PORT 5174`) and try again. Deploy Permanently Generate Wallet We need the arweave package to generate a wallet npm install --save arweave yarn add arweave -D then run this command in the terminal node -e "require('arweave').init({}).wallets.generate().then(JSON.stringify).then(console.log.bind(console))" > wallet.json Fund Wallet You will need to fund your wallet with ArDrive Turbo credits. To do this, enter ArDrive and import your wallet. Then, you can purchase turbo credits for your wallet.Setup Permaweb-Deploy npm install --global permaweb-deploy yarn global add permaweb-deploy Fund Your Wallet Turbo uses Turbo Credits to upload data to Arweave. You can purchase Turbo Credits with a variety of fiat currencies or crypto tokens. Below is an example for funding your wallet with 10 USD. It will open a browser window to complete the purchase using Stripe.npm install @ardrive/turbo-sdk
turbo top-up --wallet-file wallet.json --currency USD --value 10 Be sure to replace wallet.json with the path to your Arweave wallet.Update package.json {
  ...
  "scripts": {
    ...
    "deploy": "DEPLOY_KEY=$(base64 -i wallet.json) permaweb-deploy --ant-process << ANT-PROCESS >> --deploy-folder build"
  }
  ...
} Replace << ANT-PROCESS >> with your ANT process id.Run build Now it is time to generate a build, run npm run build yarn build Run deploy Finally we are good to deploy our first Permaweb Application npm run deploy yarn deploy ERROR If you receive an error Insufficient funds, make sure you remembered to fund your deployment wallet with ArDrive Turbo credits.Response You should see a response similar to the following:Deployed TxId [<<tx-id>>] to ANT [<<ant-process>>] using undername [<<undername>>] Your Vue app can be found at https://arweave.net/<< tx-id >>.SUCCESS You should now have a Vue Application on the Permaweb! Great Job!Summary This guide provides a simple step-by-step method to publish a Vue.js app on the Permaweb using Create Vue. If you need additional features Tailwind, consider exploring alternative starter kits listed in the guide to find a suitable solution for your requirements.

---

# 14. ARIO Docs

Document Number: 14
Source: https://docs.ar.io/wayfinder/core
Words: 298
Extraction Method: html

Wayfinder The @ar.io/wayfinder-core library provides intelligent gateway routing and data verification for accessing Arweave data through the AR.IO network. It's the foundational package that powers all other Wayfinder tools.What is Wayfinder?Wayfinder Core is a JavaScript/TypeScript library that:Intelligently Routes Requests: Automatically selects the best AR.IO gateway for each request Verifies Data Integrity: Cryptographically verifies that data hasn't been tampered with Handles Failures Gracefully: Automatically retries with different gateways when requests fail Provides Observability: Emits events and telemetry for monitoring and debugging Works Everywhere: Compatible with browsers, Node.js, and edge environments Installation Basic Configuration Advanced Configuration With Routing Strategy With Data Verification Full Configuration Example import {
  Wayfinder,
  NetworkGatewaysProvider,
  FastestPingRoutingStrategy,
  HashVerificationStrategy,
} from '@ar.io/wayfinder-core'
import { ARIO } from '@ar.io/sdk'

const wayfinder = new Wayfinder({
  // Gateway discovery
  gatewaysProvider: new NetworkGatewaysProvider({
    ario: ARIO.mainnet(),
    limit: 10,
    sortBy: 'operatorStake',
  }),

  // Routing configuration
  routingSettings: {
    strategy: new FastestPingRoutingStrategy({
      timeoutMs: 500,
      cacheResultsMs: 30000,
    }),
    events: {
      onRoutingSucceeded: (event) => {
        console.log('Selected gateway:', event.selectedGateway)
      },
      onRoutingFailed: (error) => {
        console.error('Routing failed:', error.message)
      },
    },
  },

  // Data verification
  verificationSettings: {
    enabled: true,
    strategy: new HashVerificationStrategy({
      trustedGateways: ['https://arweave.net'],
    }),
    strict: false,
    events: {
      onVerificationSucceeded: (event) => {
        console.log('Verification passed:', event.txId)
      },
      onVerificationFailed: (error) => {
        console.warn('Verification failed:', error.message)
      },
    },
  },

  // Telemetry (optional)
  telemetrySettings: {
    enabled: true,
    serviceName: 'my-application',
    clientName: 'my-app',
    clientVersion: '1.0.0',
    sampleRate: 0.1,
  },

  // Custom logger (optional)
  logger: {
    debug: (message, ...args) =>
      console.debug(`[WAYFINDER] ${message}`, ...args),
    info: (message, ...args) => console.info(`[WAYFINDER] ${message}`, ...args),
    warn: (message, ...args) => console.warn(`[WAYFINDER] ${message}`, ...args),
    error: (message, ...args) =>
      console.error(`[WAYFINDER] ${message}`, ...args),
  },
}) request(): How to fetch Arweave data using Wayfinder resolveUrl(): Use dynamic URLs for transaction IDs, ArNS names, etc.Gateway Providers: Understand gateway discovery options Routing Strategies: Explore different routing algorithms Verification Strategies: Learn about data integrity verification Telemetry: Set up monitoring and observability

---

# 15. Overview - HyperBEAM - Documentation

Document Number: 15
Source: https://hyperbeam.arweave.net/build/devices/hyperbeam-devices.html
Words: 687
Extraction Method: html

HyperBEAM Devices In AO-Core and its implementation HyperBEAM, Devices are modular components responsible for processing and interpreting Messages. They define the specific logic for how computations are performed, data is handled, or interactions occur within the AO ecosystem.Think of Devices as specialized engines or services that can be plugged into the AO framework. This modularity is key to AO's flexibility and extensibility.Purpose of Devices Define Computation: Devices dictate how a message's instructions are executed. One device might run WASM code, another might manage process state, and yet another might simply relay data.Enable Specialization: Nodes running HyperBEAM can choose which Devices to support, allowing them to specialize in certain tasks (e.g., high-compute tasks, storage-focused tasks, secure TEE operations).Promote Modularity: New functionalities can be added to AO by creating new Devices, without altering the core protocol.Distribute Workload: Different Devices can handle different parts of a complex task, enabling parallel processing and efficient resource utilization across the network.Device Naming and Versioning Devices are typically referenced using a name and version, like ~<name>@<version> (e.g., ~process@1.0). The tilde (~) often indicates a primary, user-facing device, while internal or utility devices might use a dev_ prefix in the source code (e.g., dev_router).Versioning indicates the specific interface and behavior of the device. Changes to a device that break backward compatibility usually result in a version increment.Familiar Examples HyperBEAM includes many preloaded devices that provide core functionality. Some key examples include:~meta@1.0: Configures the node itself (hardware specs, supported devices, payment info).~process@1.0: Manages persistent, shared computational states (like traditional smart contracts, but more flexible).~scheduler@1.0: Handles the ordering and execution of messages within a process.~wasm64@1.0: Executes WebAssembly (WASM) code, allowing for complex computations written in languages like Rust, C++, etc.~lua@5.3a: Executes Lua scripts.~relay@1.0: Forwards messages between AO nodes or to external HTTP endpoints.~json@1.0: Provides access to JSON data structures.~message@1.0: Manages message state and processing.~patch@1.0: Applies state updates directly to a process, often used for migrating or managing process data.Beyond the Basics Devices aren't limited to just computation or state management. They can represent more abstract concepts:Security Devices (~snp@1.0, dev_codec_httpsig): Handle tasks related to Trusted Execution Environments (TEEs) or message signing, adding layers of security and verification.Payment/Access Control Devices (~p4@1.0, ~faff@1.0): Manage metering, billing, or access control for node services.Workflow/Utility Devices (dev_cron, dev_stack, dev_monitor): Coordinate complex execution flows, schedule tasks, or monitor process activity.Using Devices Devices are typically invoked via GET requests. The path specifies which Device should interpret the subsequent parts of the path or the request body.# Example: Execute the 'now' key on the process device for a specific process
/<procId>~process@1.0/now

# Example: Relay a GET request via the relay device
/~relay@1.0/call?method=GET&path=https://example.com The specific functions or 'keys' available for each Device are documented individually. See the Devices section for details on specific built-in devices. The Potential of Devices The modular nature of AO Devices opens up vast possibilities for future expansion and innovation. The current set of preloaded and community devices is just the beginning. As the AO ecosystem evolves, we can anticipate the development of new devices catering to increasingly specialized needs:Specialized Hardware Integration: Devices could be created to interface directly with specialized hardware accelerators like GPUs (for AI/ML tasks such as running large language models), TPUs, or FPGAs, allowing AO processes to leverage high-performance computing resources securely and verifiably.Advanced Cryptography: New devices could implement cutting-edge cryptographic techniques, such as zero-knowledge proofs (ZKPs) or fully homomorphic encryption (FHE), enabling enhanced privacy and complex computations on encrypted data.Cross-Chain & Off-Chain Bridges: Devices could act as secure bridges to other blockchain networks or traditional Web2 APIs, facilitating seamless interoperability and data exchange between AO and the wider digital world.AI/ML Specific Devices: Beyond raw GPU access, specialized devices could offer higher-level AI/ML functionalities, like optimized model inference engines or distributed training frameworks.Domain-Specific Logic: Communities or organizations could develop devices tailored to specific industries or use cases, such as decentralized finance (DeFi) primitives, scientific computing libraries, or decentralized identity management systems.The Device framework ensures that AO can adapt and grow, incorporating new technologies and computational paradigms without requiring fundamental changes to the core protocol. This extensibility is key to AO's long-term vision of becoming a truly global, decentralized computer.

---

# 16. Module dev_routererl - HyperBEAM - Documentation

Document Number: 16
Source: https://hyperbeam.arweave.net/build/devices/source-code/dev_router.html
Words: 1355
Extraction Method: html

Module dev_router.erl A device that routes outbound messages from the node to their
appropriate network recipients via HTTP.Description All messages are initially
routed to a single process per node, which then load-balances them
between downstream workers that perform the actual requests.The routes for the router are defined in the routes key of the Opts,
as a precidence-ordered list of maps. The first map that matches the
message will be used to determine the route.Multiple nodes can be specified as viable for a single route, with the Choose key determining how many nodes to choose from the list (defaulting
to 1). The Strategy key determines the load distribution strategy,
which can be one of Random, By-Base, or Nearest. The route may also
define additional parallel execution parameters, which are used by the hb_http module to manage control of requests.The structure of the routes should be as follows:Node?: The node to route the message to.
       Nodes?: A list of nodes to route the message to.
       Strategy?: The load distribution strategy to use.
       Choose?: The number of nodes to choose from the list.
       Template?: A message template to match the message against, either as a
                  map or a path regex.Function Index add_route_test/0*  apply_route/3* Apply a node map's rules for transforming the path of the message.apply_routes/3* Generate a uri key for each node in a route.binary_to_bignum/1* Cast a human-readable or native-encoded ID to a big integer.by_base_determinism_test/0* Ensure that By-Base always chooses the same node for the same
hashpath.choose/5* Implements the load distribution strategies if given a cluster.choose_1_test/1*  choose_n_test/1*  device_call_from_singleton_test/0*  do_apply_route/3*  dynamic_route_provider_test/0*  dynamic_router/0*  dynamic_router_test_/0* Example of a Lua module being used as the route_provider for a
HyperBEAM node.dynamic_routing_by_performance/0*  dynamic_routing_by_performance_test_/0* Demonstrates routing tables being dynamically created and adjusted
according to the real-time performance of nodes.explicit_route_test/0*  extract_base/2* Extract the base message ID from a request message.field_distance/2* Calculate the minimum distance between two numbers
(either progressing backwards or forwards), assuming a
256-bit field.find_target_path/2* Find the target path to route for a request message.generate_hashpaths/1*  generate_nodes/1*  get_routes_test/0*  info/1 Exported function for getting device info, controls which functions are
exposed via the device API.info/3 HTTP info response providing information about this device.load_routes/1* Load the current routes for the node.local_dynamic_router/0*  local_dynamic_router_test_/0* Example of a Lua module being used as the route_provider for a
HyperBEAM node.local_process_route_provider/0*  local_process_route_provider_test_/0*  lowest_distance/1* Find the node with the lowest distance to the given hashpath.lowest_distance/2*  match/3 Find the first matching template in a list of known routes.match_routes/3*  match_routes/4*  preprocess/3 Preprocess a request to check if it should be relayed to a different node.register/3 Register function that allows telling the current node to register
a new route with a remote router node.request_hook_reroute_to_nearest_test/0* Test that the preprocess/3 function re-routes a request to remote
peers via ~relay@1.0, according to the node's routing table.route/2 Find the appropriate route for the given message.route/3  route_provider_test/0*  route_regex_matches_test/0*  route_template_message_matches_test/0*  routes/3 Device function that returns all known routes.simulate/4*  simulation_distribution/2*  simulation_occurences/2*  strategy_suite_test_/0*  template_matches/3* Check if a message matches a message template or path regex.unique_nodes/1*  unique_test/1*  weighted_random_strategy_test/0*  within_norms/3*  Function Details add_route_test/0 * add_route_test() -> any() apply_route/3 * apply_route(Msg, Route, Opts) -> any() Apply a node map's rules for transforming the path of the message.
Supports the following keys:
- opts: A map of options to pass to the request.
- prefix: The prefix to add to the path.
- suffix: The suffix to add to the path.
- replace: A regex to replace in the path.apply_routes/3 * apply_routes(Msg, R, Opts) -> any() Generate a uri key for each node in a route.binary_to_bignum/1 * binary_to_bignum(Bin) -> any() Cast a human-readable or native-encoded ID to a big integer.by_base_determinism_test/0 * by_base_determinism_test() -> any() Ensure that By-Base always chooses the same node for the same
hashpath.choose/5 * choose(N, X2, Hashpath, Nodes, Opts) -> any() Implements the load distribution strategies if given a cluster.choose_1_test/1 * choose_1_test(Strategy) -> any() choose_n_test/1 * choose_n_test(Strategy) -> any() device_call_from_singleton_test/0 * device_call_from_singleton_test() -> any() do_apply_route/3 * do_apply_route(X1, R, Opts) -> any() dynamic_route_provider_test/0 * dynamic_route_provider_test() -> any() dynamic_router/0 * dynamic_router() -> any() dynamic_router_test_/0 * dynamic_router_test_() -> any() Example of a Lua module being used as the route_provider for a
HyperBEAM node. The module utilized in this example dynamically adjusts the
likelihood of routing to a given node, depending upon price and performance.
also include preprocessing support for routing dynamic_routing_by_performance/0 * dynamic_routing_by_performance() -> any() dynamic_routing_by_performance_test_/0 * dynamic_routing_by_performance_test_() -> any() Demonstrates routing tables being dynamically created and adjusted
according to the real-time performance of nodes. This test utilizes the dynamic-router script to manage routes and recalculate weights based on the
reported performance.explicit_route_test/0 * explicit_route_test() -> any() extract_base/2 * extract_base(RawPath, Opts) -> any() Extract the base message ID from a request message. Produces a single
binary ID that can be used for routing decisions.field_distance/2 * field_distance(A, B) -> any() Calculate the minimum distance between two numbers
(either progressing backwards or forwards), assuming a
256-bit field.find_target_path/2 * find_target_path(Msg, Opts) -> any() Find the target path to route for a request message.generate_hashpaths/1 * generate_hashpaths(Runs) -> any() generate_nodes/1 * generate_nodes(N) -> any() get_routes_test/0 * get_routes_test() -> any() info/1 info(X1) -> any() Exported function for getting device info, controls which functions are
exposed via the device API.info/3 info(Msg1, Msg2, Opts) -> any() HTTP info response providing information about this device load_routes/1 * load_routes(Opts) -> any() Load the current routes for the node. Allows either explicit routes from
the node message's routes key, or dynamic routes generated by resolving the route_provider message.local_dynamic_router/0 * local_dynamic_router() -> any() local_dynamic_router_test_/0 * local_dynamic_router_test_() -> any() Example of a Lua module being used as the route_provider for a
HyperBEAM node. The module utilized in this example dynamically adjusts the
likelihood of routing to a given node, depending upon price and performance.local_process_route_provider/0 * local_process_route_provider() -> any() local_process_route_provider_test_/0 * local_process_route_provider_test_() -> any() lowest_distance/1 * lowest_distance(Nodes) -> any() Find the node with the lowest distance to the given hashpath.lowest_distance/2 * lowest_distance(Nodes, X) -> any() match/3 match(Base, Req, Opts) -> any() Find the first matching template in a list of known routes. Allows the
path to be specified by either the explicit path (for internal use by this
module), or route-path for use by external devices and users.match_routes/3 * match_routes(ToMatch, Routes, Opts) -> any() match_routes/4 * match_routes(ToMatch, Routes, Keys, Opts) -> any() preprocess/3 preprocess(Msg1, Msg2, Opts) -> any() Preprocess a request to check if it should be relayed to a different node.register(M1, M2, Opts) -> any() Register function that allows telling the current node to register
a new route with a remote router node. This function should also be idempotent.
so that it can be called only once.request_hook_reroute_to_nearest_test/0 * request_hook_reroute_to_nearest_test() -> any() Test that the preprocess/3 function re-routes a request to remote
peers via ~relay@1.0, according to the node's routing table.route/2 route(Msg, Opts) -> any() Find the appropriate route for the given message. If we are able to
resolve to a single host+path, we return that directly. Otherwise, we return
the matching route (including a list of nodes under nodes) from the list of
routes.If we have a route that has multiple resolving nodes, check
the load distribution strategy and choose a node. Supported strategies:All: Return all nodes (default).
         Random: Distribute load evenly across all nodes, non-deterministically.
        By-Base: According to the base message's hashpath.
      By-Weight: According to the node's <code>weight</code> key.
        Nearest: According to the distance of the node's wallet address to the
                 base message's hashpath.By-Base will ensure that all traffic for the same hashpath is routed to the
same node, minimizing work duplication, while Random ensures a more even
distribution of the requests.Can operate as a ~router@1.0 device, which will ignore the base message,
routing based on the Opts and request message provided, or as a standalone
function, taking only the request message and the Opts map.route/3 route(X1, Msg, Opts) -> any() route_provider_test/0 * route_provider_test() -> any() route_regex_matches_test/0 * route_regex_matches_test() -> any() route_template_message_matches_test/0 * route_template_message_matches_test() -> any() routes/3 routes(M1, M2, Opts) -> any() Device function that returns all known routes.simulate/4 * simulate(Runs, ChooseN, Nodes, Strategy) -> any() simulation_distribution/2 * simulation_distribution(SimRes, Nodes) -> any() simulation_occurences/2 * simulation_occurences(SimRes, Nodes) -> any() strategy_suite_test_/0 * strategy_suite_test_() -> any() template_matches/3 * template_matches(ToMatch, Template, Opts) -> any() Check if a message matches a message template or path regex.unique_nodes/1 * unique_nodes(Simulation) -> any() unique_test/1 * unique_test(Strategy) -> any() weighted_random_strategy_test/0 * weighted_random_strategy_test() -> any() within_norms/3 * within_norms(SimRes, Nodes, TestSize) -> any()

---

# 17. Module dev_p4erl - HyperBEAM - Documentation

Document Number: 17
Source: https://hyperbeam.arweave.net/build/devices/source-code/dev_p4.html
Words: 550
Extraction Method: html

Module dev_p4.erl The HyperBEAM core payment ledger.Description This module allows the operator to
specify another device that can act as a pricing mechanism for transactions
on the node, as well as orchestrating a payment ledger to calculate whether
the node should fulfil services for users.The device requires the following node message settings in order to function:p4_pricing-device: The device that will estimate the cost of a request.p4_ledger-device: The device that will act as a payment ledger.The pricing device should implement the following keys:<code>GET /estimate?type=pre|post&body=[...]&request=RequestMessage</code><code>GET /price?type=pre|post&body=[...]&request=RequestMessage</code> The body key is used to pass either the request or response messages to the
device. The type key is used to specify whether the inquiry is for a request
(pre) or a response (post) object. Requests carry lists of messages that will
be executed, while responses carry the results of the execution. The price key may return infinity if the node will not serve a user under any
circumstances. Else, the value returned by the price key will be passed to
the ledger device as the amount key.A ledger device should implement the following keys:<code>POST /credit?message=PaymentMessage&request=RequestMessage</code><code>POST /charge?amount=PriceMessage&request=RequestMessage</code><code>GET /balance?request=RequestMessage</code> The type key is optional and defaults to pre. If type is set to post,
the charge must be applied to the ledger, whereas the pre type is used to
check whether the charge would succeed before execution.Function Index balance/3 Get the balance of a user in the ledger.faff_test/0* Simple test of p4's capabilities with the faff@1.0 device.hyper_token_ledger/0*  hyper_token_ledger_test_/0* Ensure that Lua scripts can be used as pricing and ledger devices.is_chargable_req/2* The node operator may elect to make certain routes non-chargable, using
the routes syntax also used to declare routes in router@1.0.non_chargable_route_test/0* Test that a non-chargable route is not charged for.request/3 Estimate the cost of a transaction and decide whether to proceed with
a request.response/3 Postprocess the request after it has been fulfilled.test_opts/1*  test_opts/2*  test_opts/3*  Function Details balance/3 balance(X1, Req, NodeMsg) -> any() Get the balance of a user in the ledger.faff_test/0 * faff_test() -> any() Simple test of p4's capabilities with the faff@1.0 device.hyper_token_ledger/0 * hyper_token_ledger() -> any() hyper_token_ledger_test_/0 * hyper_token_ledger_test_() -> any() Ensure that Lua scripts can be used as pricing and ledger devices. Our
scripts come in two components:
1. A process script which is executed as a persistent local-process on the
node, and which maintains the state of the ledger. This process runs hyper-token.lua as its base, then adds the logic of hyper-token-p4.lua to it. This secondary script implements the charge function that p4@1.0 will call to charge a user's account.
2. A client script, which is executed as a p4@1.0 ledger device, which
uses ~push@1.0 to send requests to the ledger process.is_chargable_req/2 * is_chargable_req(Req, NodeMsg) -> any() The node operator may elect to make certain routes non-chargable, using
the routes syntax also used to declare routes in router@1.0.non_chargable_route_test/0 * non_chargable_route_test() -> any() Test that a non-chargable route is not charged for.request/3 request(State, Raw, NodeMsg) -> any() Estimate the cost of a transaction and decide whether to proceed with
a request. The default behavior if pricing-device or p4_balances are
not set is to proceed, so it is important that a user initialize them.response/3 response(State, RawResponse, NodeMsg) -> any() Postprocess the request after it has been fulfilled.test_opts/1 * test_opts(Opts) -> any() test_opts/2 * test_opts(Opts, PricingDev) -> any() test_opts/3 * test_opts(Opts, PricingDev, LedgerDev) -> any()

---

# 18. JoiningRunning a Router - HyperBEAM - Documentation

Document Number: 18
Source: https://hyperbeam.arweave.net/run/joining-running-a-router.html
Words: 906
Extraction Method: html

Router Networks: Joining vs Running Router networks in HyperBEAM have two distinct roles that are often confused:Two Different Concepts Joining a router = Registering your worker node with an existing router to receive work Running a router = Operating a router that manages and distributes work to other nodes When to use HB-OS Operation Use HB-OS?Purpose TEE Node (SNP) Recommended Secure, attested computation (hardware isolation) Router Registration Optional Registering/joining a router (TEE not required) You can join or run a router without HB-OS.If you want to run a TEE node, HB-OS or an equivalent TEE setup is recommended for convenience and security.Configuration Files: config.json vs config.flat Configuration can be set in either config.json (JSON syntax) or config.flat (flat syntax). The examples below use JSON for clarity, but you can use either format depending on your deployment. The syntax differs:config.json uses standard JSON structure (see examples below) config.flat uses key-value pairs Joining a Router Network (Worker Node) Most users want to join an existing router to offer computational services. This does NOT require HB-OS or TEE unless you specifically want TEE security.1. Prepare Your Configuration (config.json example) Use the following configuration as a template for your worker node:{
    // ─── Initial Configuration ─────────────────────────────────────────────────
    // Lock this configuration so it cannot be changed again
    "operator": "trustless",
    "initialized": "permanent",

    // ─── SNP-Based TEE Attestation Parameters ──────────────────────────────────
    // These values let the TEE verify its own environment—and any other VM
    // instantiated from the same image—before granting access.
    "snp_trusted": [],

    // ─── Request/Response Processing Configuration ─────────────────────────────
    // Defines how requests and responses are processed through the p4 device
    "on": {
        "request": {
            "device": "p4@1.0",
            "ledger-device": "lua@5.3a",
            "pricing-device": "simple-pay@1.0",
            "ledger-path": "/ledger~node-process@1.0",
            "module": ""        // Automatically injected
        },
        "response": {
            "device": "p4@1.0",
            "ledger-device": "lua@5.3a",
            "pricing-device": "simple-pay@1.0",
            "ledger-path": "/ledger~node-process@1.0",
            "module": ""        // Automatically injected
        }
    },

    // ─── Non-Chargeable Routes Configuration ──────────────────────────────────
    // Routes that should not incur charges when accessed through p4
    "p4_non_chargable_routes": [
        { "template": "/.*~node-process@1.0/.*" },
        { "template": "/.*~greenzone@1.0/.*" },
        { "template": "/.*~router@1.0/.*" },
        { "template": "/.*~meta@1.0/.*" },
        { "template": "/schedule" },
        { "template": "/push" },
        { "template": "/~hyperbuddy@1.0/.*" }
    ],

    // ─── Node Process Spawn Configuration ─────────────────────────────────────
    // Codec used for spawning new node processes
    "node_process_spawn_codec": "ans104@1.0",

    // ─── Node Process Definitions ─────────────────────────────────────────────
    // Configuration for individual node processes
    "node_processes": {
        "ledger": {
            "device": "process@1.0",
            "execution-device": "lua@5.3a",
            "scheduler-device": "scheduler@1.0",
            "authority-match": 1,
            "admin": "",                   // Automatically injected
            "token": "",                   // Automatically injected
            "module": "",                  // Automatically injected
            "authority": ""                // Automatically injected
        }
    },

    // ─── Router Registration Options ──────────────────────────────────────────
    // Configuration for how processes register with the router
    "router_opts": {
        "offered": [
            // {
            //     "registration-peer": {},            // Automatically injected
            //     "template": "/*~process@1.0/*",   // The routes that the node will register with
            //     "prefix": "",                       // Automatically injected
            //     "price": 4500000                    // Registration fee in smallest units
            // }
        ]
    },

    // ─── Greenzone Registration Options ────────────────────────────────────────
    // Configuration for how processes register with the greenzone
    "green_zone_peer_location": "",         // Automatically injected
    "green_zone_peer_id": "",               // Automatically injected

    // ─── P4 Recipient ──────────────────────────────────────────────────────────
    // The Address of the node that will receive the P4 messages
    "p4_recipient": ""                      // Automatically injected
} Perform the following API calls in order:Meta Info Post:Endpoint: ~meta@1.0/info POST Example:
Join Green Zone:Endpoint: ~greenzone@1.0/join GET Become Green Zone Member:Endpoint: ~greenzone@1.0/become GET Register as Router:Endpoint: ~router@1.0/register GET 3. Verify Registration Check your node's status in the network Confirm green zone membership Test routing functionality 4. Troubleshooting If registration fails:
1. Verify all configuration parameters are correct
2. Check network connectivity to the node URL
3. Ensure proper headers are set in API requests
4. Review logs for specific error messages
5. Confirm green zone availability and accessibility Running Your Own Router (Advanced) If you want to operate a router that manages other worker nodes:Deploy the dynamic router Lua process to handle registrations Configure trusted software hashes for TEE validation (if using TEE) Set up load balancing and performance monitoring Manage worker node admissibility policies Example Router Configuration (config.json example) {
    // ─── Router Node Preprocessing Settings ───────────────────────────────────
    // Defines the router process and how it preprocesses incoming requests
    "on": {
        "request": {
            "device": "router@1.0",
            "path": "preprocess",
            "commit-request": true         // Enable request commitment for routing
        }
    },

    // ─── Route Provider Configuration ─────────────────────────────────────────
    // Specifies where to get routing information from the router node process
    "router_opts": {
        "provider": {
            "path": "/router~node-process@1.0/compute/routes~message@1.0"
        },
        "registrar": {
            "path": "/router~node-process@1.0"
        },
        "registrar-path": "schedule"
    },

    // ─── Relay Configuration ──────────────────────────────────────────────────
    // Allow the relay to commit requests when forwarding
    "relay_allow_commit_request": true,

    // ─── Router Node Process Configuration ────────────────────────────────────
    // Specifies the Lua-based router logic, weights for scoring, and admission check
    "node_processes": {
        "router": {
            "type": "Process",
            "device": "process@1.0",
            "execution-device": "lua@5.3a",
            "scheduler-device": "scheduler@1.0",
            "pricing-weight": 9,           // Weight for pricing in routing decisions
            "performance-weight": 1,       // Weight for performance in routing decisions
            "score-preference": 4,         // Preference scoring for route selection
            "performance-period": 2,       // Period for performance measurement
            "initial-performance": 1000,   // Initial performance score
            // Default admission policy (currently set to false)
            "is-admissible": {
                "path": "default",
                "default": "false"
            },
            "module": "",                  // Automatically injected
            "trusted-peer": "",            // Automatically injected
            "trusted": ""                  // Automatically injected
        }
    }
} Advanced Configuration Running a production router requires careful consideration of security, performance, and economic incentives. Most users should join existing routers rather than run their own.Further Exploration Examine the dev_router.erl source code for detailed implementation.Review the scripts/dynamic-router.lua for router-side logic.Review the available configuration options in hb_opts.erl related to routing (routes, strategies, etc.).Consult community channels for best practices on deploying production routers.

---

# 19. Running a HyperBEAM Node - HyperBEAM - Documentation

Document Number: 19
Source: https://hyperbeam.arweave.net/run/running-a-hyperbeam-node.html
Words: 841
Extraction Method: html

Running a HyperBEAM Node This guide provides the basics for running your own HyperBEAM node, installing dependencies, and connecting to the AO network.System Dependencies To successfully build and run a HyperBEAM node, your system needs several software dependencies installed.Install core dependencies using Homebrew:brew install cmake git pkg-config openssl ncurses Install core dependencies using apt:sudo apt-get update && sudo apt-get install -y --no-install-recommends \
    build-essential \
    cmake \
    git \
    pkg-config \
    ncurses-dev \
    libssl-dev \
    sudo \
    curl \
    ca-certificates Erlang/OTP HyperBEAM is built on Erlang/OTP. You need version OTP 27 installed (check the rebar.config or project documentation for specific version requirements, typically OTP 27).Installation methods:brew install erlang@27 sudo apt install erlang=1:27.* Download from erlang.org and follow the build instructions for your platform.Rebar3 Rebar3 is the build tool for Erlang projects.Installation methods:brew install rebar3 Get the rebar3 binary from the official website. Place the downloaded rebar3 file in your system's PATH (e.g., /usr/local/bin) and make it executable (chmod +x rebar3).Node.js Node.js might be required for certain JavaScript-related tools or dependencies. Node version 22+ is required.Installation methods:brew install node Rust Rust is needed if you intend to work with or build components involving WebAssembly (WASM) or certain Native Implemented Functions (NIFs) used by some devices (like ~snp@1.0).The recommended way to install Rust on all platforms is via rustup:curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
source "$HOME/.cargo/env" # Or follow the instructions provided by rustup Prerequisites for Running Before starting a node, ensure you have:Installed the system dependencies mentioned above.Cloned the HyperBEAM repository (git clone ...).Compiled the source code (rebar3 compile in the repo directory).An Arweave wallet keyfile (e.g., generated via Wander). The path to this file is typically set via the hb_key configuration option (see Configuring Your HyperBEAM Node).Starting a Basic Node The simplest way to start a HyperBEAM node for development or testing is using rebar3 from the repository's root directory:rebar3 shell This command:Starts the Erlang Virtual Machine (BEAM) with all HyperBEAM modules loaded.Initializes the node with default settings (from hb_opts.erl).Starts the default HTTP server (typically on port 8734), making the node accessible.Drops you into an interactive Erlang shell where you can interact with the running node.This basic setup is suitable for local development and exploring HyperBEAM's functionalities.HyperBEAM uses build profiles to enable optional features, often requiring extra dependencies. To run a node with specific profiles enabled, use rebar3 as ... shell:Available Profiles (Examples):genesis_wasm: Enables Genesis WebAssembly support.rocksdb: Enables the RocksDB storage backend.http3: Enables HTTP/3 support.Example Usage:# Start with RocksDB profile
rebar3 as rocksdb shell

# Start with RocksDB and Genesis WASM profiles
rebar3 as rocksdb, genesis_wasm shell Note: Choose profiles before starting the shell, as they affect compile-time options.Node Configuration HyperBEAM offers various configuration options (port, key file, data storage, logging, etc.). These are primarily set using a config.flat file and can be overridden by environment variables or command-line arguments.See the dedicated Configuring Your HyperBEAM Node guide for detailed information on all configuration methods and options.Verify Installation To quickly check if your node is running and accessible, you can send a request to its ~meta@1.0 device (assuming default port 8734):curl http://localhost:8734/~meta@1.0/info A JSON response containing node information indicates success.Running for Production (Mainnet) While you can connect to the main AO network using the rebar3 shell for testing purposes (potentially using specific configurations or helper functions like hb:start_mainnet/1 if available and applicable), the standard and recommended method for a stable production deployment (like running on the mainnet) is to build and run a release.1. Build the Release:From the root of the HyperBEAM repository, build the release package. You might include specific profiles needed for your mainnet setup (e.g., rocksdb if you intend to use it):# Build release with default profile
rebar3 release

# Or, build with specific profiles (example)
# rebar3 as rocksdb release This command compiles the project and packages it along with the Erlang Runtime System (ERTS) and all dependencies into a directory, typically _build/default/rel/hb.2. Configure the Release:Navigate into the release directory (e.g., cd _build/default/rel/hb). Ensure you have a correctly configured config.flat file here. See the configuration guide for details on setting mainnet parameters (port, key file location, store path, specific peers, etc.). Environment variables can also be used to override settings in the release's config.flat when starting the node.3. Start the Node:Use the generated start script (bin/hb) to run the node:# Start the node in the foreground (logs to console)
./bin/hb console

# Start the node as a background daemon
./bin/hb start

# Check the status
./bin/hb ping
./bin/hb status

# Stop the node
./bin/hb stop Consult the generated bin/hb script or Erlang/OTP documentation for more advanced start-up options (e.g., attaching a remote shell).Running as a release provides a more robust, isolated, and manageable way to operate a node compared to running directly from the rebar3 shell.Stopping the Node (rebar3 shell) To stop the node running within the rebar3 shell, press Ctrl+C twice or use the Erlang command q()..Configure Your Node: Deep dive into configuration options.TEE Nodes: Learn about running nodes in Trusted Execution Environments for enhanced security.Routers: Understand how to configure and run a router node.

---

# 20. joinNetwork - ARIO Docs

Document Number: 20
Source: https://docs.ar.io/ar-io-sdk/ario/gateways/join-network
Words: 200
Extraction Method: html

joinNetwork is a method on the ARIO class that joins a gateway to the ar.io network using its associated wallet.joinNetwork requires authentication.Parameters Example joinNetwork const fs = require("fs");
 const { ARIO, ArweaveSigner, ARIOToken } = require("@ar.io/sdk");

 async function main() {
 const jwk = JSON.parse(fs.readFileSync("KeyFile.json"));
 const ario = ARIO.init({
     signer: new ArweaveSigner(jwk),
 });

 const { id: txId } = await ario.joinNetwork(
     {
         qty: new ARIOToken(10_000).toMARIO(), // minimum operator stake allowed
         autoStake: true, // auto-stake operator rewards to the gateway
         allowDelegatedStaking: true, // allows delegated staking
         minDelegatedStake: new ARIOToken(100).toMARIO(), // minimum delegated stake allowed
         delegateRewardShareRatio: 10, // percentage of rewards to share with delegates (e.g. 10%)
         label: 'john smith', // min 1, max 64 characters
         note: 'The example gateway', // max 256 characters
         properties: 'FH1aVetOoulPGqgYukj0VE0wIhDy90WiQoV3U2PeY44', // Arweave transaction ID containing additional properties of the Gateway
         observerWallet: '0VE0wIhDy90WiQoV3U2PeY44FH1aVetOoulPGqgYukj', // wallet address of the observer, must match OBSERVER_WALLET on the observer
         fqdn: 'example.com', // fully qualified domain name - note: you must own the domain and set the OBSERVER_WALLET on your gateway to match `observerWallet`
         port: 443, // port number
         protocol: 'https', // only 'https' is supported
     },
     // optional additional tags
     { tags: [{ name: 'App-Name', value: 'My-Awesome-App' }] },
     );
 }

 main();

---

# 21. ARIO Network Testnet - ARIO Docs

Document Number: 21
Source: https://docs.ar.io/guides/testnet
Words: 885
Extraction Method: html

Testnet The AR.IO Network Testnet allows developers to test their applications and workflows using ARIO Network features such as ArNS Names before deploying to the mainnet. The ARIO Network Testnet offers a faucet for requesting testnet ARIO tokens (tARIO). The initial version of testnet only supports registering and resolving temporary ArNS names; however, enhancements such as temporary data uploads will be added in the future. We welcome feedback for improvements and other feature requests.Faucet Browser UI The ARIO Network Testnet Faucet is a service that allows developers to request testnet ARIO tokens (tARIO). It can be accessed in a browser by visiting ar://faucet.This is the recommended way to use the faucet. To use it:Select Testnet from the network dropdown Enter your wallet address Enter the an amount of tARIO tokens (max 10000) Complete the captcha challenge Click the "Request Tokens" button Onece complete, tARIO tokens will automatically be sent to your wallet Using Testnet Using the testnet is similar to using the mainnet, with a few key differences:Using the ARIO SDK When using the ARIO SDK, to interact with the AR.IO testnet - you can create your ARIO instance in one of two ways;Using the ARIO.tesntet() API By default, this instance will leverage cu.ardrive.io for process evaluation and the recommended way to interact with testnet.Using process with ARIO_TESTNET_PROCESS_ID By default, this instance will leverage community CUs managed by forward.Note: ANTs are network-agnostic, so no additional configuration is needed when working with them.Once configured, all SDK methods will operate on testnet instead of mainnet. For more details on configuration, see the ARIO Configuration documentation.Accessing ArNS Names To access ArNS names on testnet in a browser, you must use a gateway that is configured to operate on testnet instead of mainnet.The gateway ar-io.dev is configured to operate on the ARIO Network Testnet.Using arns.app with Testnet arns.app is the primary graphical dApp for purchasing and managing ArNS names. To configure arns.app to operate on testnet:Click the Connect button in the top right corner to connect your wallet After connecting, click on your user profile button (which replaces the Connect button) Go to Settings  Click on ArNS Registry Settings  On the right side of the screen, you'll see three buttons: Devnet, Testnet, and Mainnet Click on Testnet to switch the app to operate on the testnet  The app will now operate on testnet, allowing you to purchase and manage ArNS names using testnet tokens.Running your own Gateway with testnet In addition to ar-io.dev - you can also elect to run your own ARIO gateway that resolves names against testnet. To do so, you need to setup your gateway by following the steps in the Linux Setup Guide or the Windows Setup Guide.Once running, modify the .env to point ARIO testnet process id.Once set, restart your gateway and navigate to <your-gateway-url>/ar-io/info - you should see agYcCFJtrMG6cqMuZfskIkFTGvUPddICmtQSBIoPdiA as the process id. Your gateway will now resolve arns names stored on the ARIO tesntet process.Restrictions Testnet has a few primary purposes: to mimic mainnet functionality as close as possible, to provide a testing bed for upcoming network upgrades, and to provide a playground for users and developers to experiment. It is NOT intended for production purposes and should not be used as such.
Test ARIO (tARIO) tokens are just that - test tokens. They have no external value, may break, and have no guarantee of continued support. tARIO tokens have no relation to mainnet $ARIO and are not a proxy for any rewards. There is no supply cap on tARIO tokens.
While advanced notice will be provided whenever possible, testnet may go offline for maintenance. Likewise, test token balances and test ArNS names may be reset/nullified at any point to clean up the contract state or prepare for an upgrade.Advanced Integrating AR.IO Testnet in your client-side applications If you'd like to incorporate the AR.IO faucet into your application, you can programmatically retrieve access tokens - which allow your application to request testnet tokens for your users.To integrate:import { ARIO, ARIOToken } from '@ar.io/sdk'

// setup testnet client;
const testnet = ARIO.testnet()

// request the captcha URL for the token, which will require a human to solve
const captchaURL = await testnet.faucet.captchaURL()

// open the captcha URL in a browser;
const captchaWindow = window.open(
  captchaUrl.captchaUrl,
  '_blank',
  'width=600,height=600',
)

// The captcha URL includes a window.parent.postMessage event that is used to send the auth token to the parent window.
// You can store the auth token in localStorage and use it to claim tokens for the duration of the auth token's expiration (default 1 hour).
window.parent.addEventListener('message', async (event) => {
  if (event.data.type === 'ario-jwt-success') {
    localStorage.setItem('ario-jwt', event.data.token)
    localStorage.setItem('ario-jwt-expires-at', event.data.expiresAt)
    // close our captcha window
    captchaWindow?.close()
    // claim the tokens using the JWT token,
    const res = await testnet.faucet
      .claimWithAuthToken({
        authToken: event.data.token,
        recipient: await window.arweaveWallet.getActiveAddress(),
        quantity: new ARIOToken(100).toMARIO().valueOf(), // 100 ARIO
      })
      .then((res) => {
        alert('Successfully claimed 100 ARIO tokens! Transaction ID: ' + res.id)
      })
      .catch((err) => {
        alert(`Failed to claim tokens: ${err}`)
      })
  }
})

// you can re-use the JWT for up to 1 hour, allowing you to request tokens for multiple wallets without having to satisfy the catpcha multiple times
if (
  localStorage.getItem('ario-jwt-expires-at') &&
  Date.now() < parseInt(localStorage.getItem('ario-jwt-expires-at') ?? '0')
) {
  const res = await testnet.faucet.claimWithAuthToken({
    authToken: localStorage.getItem('ario-jwt') ?? '',
    recipient: await window.arweaveWallet.getActiveAddress(),
    quantity: new ARIOToken(100).toMARIO().valueOf(), // 100 ARIO
  })
}

---

# 22. Querying Transactions  Cooking with the Permaweb

Document Number: 22
Source: https://cookbook.arweave.net/concepts/queryTransactions.html
Words: 390
Extraction Method: html

Querying Transactions It isn't enough to store data permanently, for Arweave to be useful the data also needs to be discoverable and retrievable. This guide summarizes the different approaches to querying data on Arweave.GraphQL Over time, indexing services that implement a GraphQL interface have became the preferred method for querying transaction data on Arweave. An indexing service reads transaction and block headers as they are added to the network (usually from a full Arweave node which the service operates). Once read, the header info is inserted into a database where it can be indexed and efficiently queried. The indexing service uses this database to provide a GraphQL endpoint for clients to query.GraphQL has a few advantages that make it ideal for retrieving query data sets. It enables indexing services to create a single endpoint that can then be used to query all types data. The service is able to return multiple resources in a single request as opposed to making an HTTP request for each resource (like one would with a REST API). With GraphQL, clients can batch multiple requests in a single round-trip and specify exactly what data is needed which increases performance.The following GraphQL example queries all the transaction ids from a given owners wallet address that have a "Type" tag with a value of "manifest". For more information about tags, read the guide on Transaction Tags.const queryObject = {
    query:
    `{
        transactions (
            owners:["${address}"],
            tags: [
              {
                    name: "Type",
                    values: ["manifest"]
                }
            ]
        ) {
            edges {
                node {
                    id
                }
            }
        }
    }`
};
const results = await arweave.api.post('/graphql', queryObject);Public Indexing Services https://arweave.net/graphql https://arweave-search.goldsky.com/graphql Inspecting the Blocks Each piece of data uploaded to Arweave has its own unique transaction id and is included in a unique block which is then added to the blockchain. The data associated with each transaction is split up into 256KB chunks and appended sequentially to Arweave's dataset. It is possible to walk back, block by block, from the current block and inspect each one for the transaction id in question. Once found, the chunks offsets can be retrieved from the block and used to request chunks directly from an Arweave peer. This is the lowest level way to locate and read data on the network. Thankfully, less labor intensive approaches like GraphQL are available.Resources Querying Arweave Guide ar-gql package GraphQL Reference

---

# 23. Hello World (CLI)  Cooking with the Permaweb

Document Number: 23
Source: https://cookbook.arweave.net/getting-started/quick-starts/hw-cli.html
Words: 140
Extraction Method: html

Hello World (CLI) This guide walks you through the most simple way to get data on to the permaweb using a command-line interface (CLI).Requirements NodeJS open in new window LTS or greater Description Using a terminal/console window create a new folder called hw-permaweb-1.Setup cd hw-permaweb-1
npm init -y
npm install arweave ardrive-cli Generate a wallet npx -y @permaweb/wallet > ~/.demo-arweave-wallet.json Create a web page echo "<h1>Hello Permaweb</h1>" > index.html Upload using Ardrive CLI # Create a Drive
FOLDER_ID=$(npx ardrive create-drive -n public -w ~/.demo-arweave-wallet.json --turbo | jq -r '.created[] | select(.type == "folder") | .entityId')
# Upload file
TX_ID=$(npx ardrive upload-file -l index.html --content-type text/html -w ~/.demo-arweave-wallet.json --turbo -F ${FOLDER_ID} | jq -r '.created[] | select(.type == "file
") | .dataTxId')
# open file from ar.io gateway
open https://arweave.net/${TX_ID} Built with  ❤️  by the Arweave community. Learn more at  Arweave.org

---

# 24. Atomic Tokens  Cooking with the Permaweb

Document Number: 24
Source: https://cookbook.arweave.net/guides/atomic-tokens/intro.html
Words: 353
Extraction Method: html

Atomic Tokens ⚠️ Deprecation Notice This document is deprecated and may contain outdated information.What is an Atomic Token?Check out the concept Creating an Atomic Token INFORMATION For this example, we are using a SWT Contract Source that is already published on the network. x0ojRwrcHBmZP20Y4SY0mgusMRx-IYTjg5W8c3UFoNs - example.ts import Irys from '@irys/sdk'
import { WarpFactory } from 'warp-contracts'

async function main() {
  const wallet = JSON.parse(await import('fs')
    .then(fs => fs.readFileSync('./wallet.json', 'utf-8')))

  const irys = new Irys({ 'https://node2.irys.xyz', 'arweave', wallet })
  const warp = WarpFactory.forMainnet()

  const data = `<h1>Hello Permaweb!</h1>`
  const tags = [
    { name: 'Content-Type', value: 'text/html' },
    // ANS-110 Tags
    { name: 'Type', value: 'web-page' },
    { name: 'Title', value: 'My first permaweb page' },
    { name: 'Description', value: 'First permaweb page by Anon' },
    { name: 'Topic:Noob', value: 'Noob' },
    // SmartWeave Contract
    { name: 'App-Name', value: 'SmartWeaveContract' },
    { name: 'App-Version', value: '0.3.0' },
    { name: 'Contract-Src', value: 'x0ojRwrcHBmZP20Y4SY0mgusMRx-IYTjg5W8c3UFoNs' },
    {
      name: 'Init-State', value: JSON.stringify({
        balances: {
          'cHB6D8oNeXxbQCsKcmOyjUX3UkL8cc3FbJmzbaj3-Nc': 1000000
        },
        name: 'AtomicToken',
        ticker: 'ATOMIC-TOKEN',
        pairs: [],
        creator: 'cHB6D8oNeXxbQCsKcmOyjUX3UkL8cc3FbJmzbaj3-Nc',
        settings: [['isTradeable', true]]
      })
    }
  ]

  const { id } = await irys.upload(data, { tags })
  await warp.createContract.register(id, 'node2')
  console.log('Atomic Token: ', id)
}

main() In this example, we are creating a data-item and uploading the item to the bundler network service. Then we are registering our contract with the Warp sequencer. By using bundler to publish our data-item and registering with the Warp sequencer, our data is immediately available on the gateway service and our contract is immediately able to accept interactions.Run Example npm install @irys/sdk warp-contracts
npm install typescript ts-node
npx ts-node example.ts INFORMATION ANS-110 is an Asset Discovery Specification to allow for composability with the Permaweb Application ecosystem.Summary This is a simple example of deploying an Atomic Asset, for more detailed examples check out: https://atomic-assets.arweave.dev Working with Tokens SmartWeave Contracts can not hold AR the native coin of the Arweave Network. AR is used to purchase storage for data on the Arweave Network and it can be transferred from a source wallet to a target wallet on the Arweave network, but it can not be held in a SmartWeave contract.

---

# 25. Github Action  Cooking with the Permaweb

Document Number: 25
Source: https://cookbook.arweave.net/guides/deployment/github-action.html
Words: 618
Extraction Method: html

Github Action WARNING This guide is for educational purposes only, and you should use to learn options of how you might want to deploy your application. In this guide, we are trusting a 3rd party resource github owned by microsoft to protect our secret information, in their documentation they encrypt secrets in their store using libsodium sealed box, you can find more information about their security practices here. https://docs.github.com/en/actions/security-guides/encrypted-secrets Github Actions are CI/CD pipelines that allows developers to trigger automated tasks via events generated from the github workflow system. These tasks can be just about anything, in this guide we will show how you can use github actions to deploy your permaweb application to the permaweb using permaweb-deploy and ArNS.TIP This guide requires understanding of github actions, and you must have some Turbo Credits and an ArNS name. Go to https://ar.io/arns/ for more details on acquiring an ArNS name.WARNING This guide does not include testing or any other checks you may want to add to your production workflow.Prerequisites Before setting up GitHub Actions deployment, you'll need:An Arweave wallet with sufficient Turbo Credits for deployment An ArNS name that you own A built application (e.g., in a ./dist folder) Install permaweb-deploy Add permaweb-deploy as a development dependency to your project:npm install --save-dev permaweb-deploy Configure Deployment Script Add a deployment script to your package.json that builds your application and deploys it using permaweb-deploy:{
  "scripts": {
    "dev": "vuepress dev src",
    "build": "vuepress build src",
    "deploy": "npm run build && permaweb-deploy --arns-name YOUR_ARNS_NAME"
  }
} Replace YOUR_ARNS_NAME with your actual ArNS name (e.g., my-app).Advanced Configuration You can customize the deployment with additional options:{
  "scripts": {
    "deploy": "npm run build && permaweb-deploy --arns-name my-app --deploy-folder ./dist --undername @"
  }
} Available options:--arns-name (required): Your ArNS name --deploy-folder: Folder to deploy (default: ./dist) --undername: ANT undername to update (default: @) --ario-process: ARIO process (default: mainnet) Create GitHub Action Create a .github/workflows/deploy.yml file in your repository:name: Deploy to Permaweb

on:
  push:
    branches:
      - "main"

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version: 20.x
      - run: npm install
      - run: npm run deploy
        env:
          DEPLOY_KEY: ${{ secrets.DEPLOY_KEY }} Setup GitHub Secrets 1. Prepare Your Wallet First, encode your Arweave wallet as base64:base64 -i wallet.json Copy the output (it will be a long base64 string).2. Add Secret to GitHub Go to your repository on GitHub Navigate to Settings → Secrets and variables → Actions Click New repository secret Name: DEPLOY_KEY Value: Paste the base64 encoded wallet string Click Add secret Fund Your Wallet Ensure your deployment wallet has sufficient Turbo Credits. You can fund it using:# Check current balance
npx @ardrive/turbo-cli balance --wallet-file wallet.json

# Add credits (amount in Winston - 1 AR = 1,000,000,000,000 Winston)
npx @ardrive/turbo-cli top-up --value 500000000000 --wallet-file wallet.json Security Best Practices Use a dedicated wallet solely for deployments Keep minimal funds in the deployment wallet Never commit wallet files to your repository Regularly rotate deployment keys Test Your Deployment Local Testing Test your deployment locally before pushing:DEPLOY_KEY=$(base64 -i wallet.json) npm run deploy Verify Deployment After a successful GitHub Action run:Check the action logs for the deployment transaction ID Wait 10-20 minutes for ArNS propagation Visit your ArNS name: https://YOUR_ARNS_NAME.arweave.dev Troubleshooting Common Issues:Insufficient Credits: Ensure your wallet has enough Turbo Credits ArNS Propagation: Wait 10-20 minutes after deployment for changes to appear Build Failures: Ensure your build command works locally first Secret Issues: Verify the DEPLOY_KEY secret is properly set and base64 encoded Check Deployment Status:Monitor your deployments through:GitHub Actions logs ArNS resolver: https://arns.arweave.dev/resolve/YOUR_ARNS_NAME 🎉 You now have automated permaweb deployment with GitHub Actions!Your application will automatically deploy to the permaweb whenever you push to the main branch, and your ArNS name will point to the latest version.

---

# 26. Posting Transactions using arweave-js  Cooking with the Permaweb

Document Number: 26
Source: https://cookbook.arweave.net/guides/posting-transactions/arweave-js.html
Words: 375
Extraction Method: html

Posting Transactions using arweave-js Arweave native transactions can be posted directly to a node or gateway using the arweave-js package.Arweave scales though the use of transaction bundles. These bundles make it possible for each block to contain a nearly unlimited number of transactions. Without the use of bundles, Arweave blocks are limited 1000 transactions per block (with new blocks produced every ~2 minutes). If your use case exceeds this capacity you may experience dropped transactions. Under these circumstances please consider using irys.xyz or similar services to bundle your transactions.Installing the arweave-js Package To install arweave-js run npm install --save arweave yarn add arweave When working with NodeJS a minimum version of NodeJS 18 or higher is required.Initializing arweave-js Direct Layer 1 transactions are posted using the arweave-js library.import Arweave from 'arweave';
import fs from "fs";

// load the JWK wallet key file from disk
let key = JSON.parse(fs.readFileSync("walletFile.txt").toString());

// initialize an arweave instance
const arweave = Arweave.init({});Posting a wallet-to-wallet Transaction A basic transaction to move AR tokens from one wallet address to another.//  create a wallet-to-wallet transaction sending 10.5AR to the target address
let transaction = await arweave.createTransaction({
  target: '1seRanklLU_1VTGkEk7P0xAwMJfA7owA1JHW5KyZKlY',
  quantity: arweave.ar.arToWinston('10.5')
}, key);

// you must sign the transaction with your key before posting
await arweave.transactions.sign(transaction, key);

// post the transaction
const response = await arweave.transactions.post(transaction);Posting a Data Transaction This example illustrates how load a file from disk and create a transaction to store its data on the network. You can find the current price the network is charging at https://ar-fees.arweave.dev // load the data from disk
const imageData = fs.readFileSync(`iamges/myImage.png`);

// create a data transaction
let transaction = await arweave.createTransaction({
  data: imageData
}, key);

// add a custom tag that tells the gateway how to serve this data to a browser
transaction.addTag('Content-Type', 'image/png');

// you must sign the transaction with your key before posting
await arweave.transactions.sign(transaction, key);

// create an uploader that will seed your data to the network
let uploader = await arweave.transactions.getUploader(transaction);

// run the uploader until it completes the upload.
while (!uploader.isComplete) {
  await uploader.uploadChunk();
} Resources For an overview of all the ways you can post transactions, see the Posting Transactions section of the cookbook.For a more detailed description of all arweave-js 's features see the documentation on github

---

# 27. Querying Arweave with GraphQL  Cooking with the Permaweb

Document Number: 27
Source: https://cookbook.arweave.net/guides/querying-arweave/queryingArweave.html
Words: 828
Extraction Method: html

Querying Arweave with GraphQL Arweave provides a simple way of querying for transactions and filtering them by tags. Arweave GraphQL-compatible indexing services provide endpoints users can post GraphQL queries to, and also provide a playground for trying queries.GraphQL is a flexible query language that services can use to build a customized data schema for clients to query. GraphQL also allows clients to specify which elements of the available data structure they would like to see in the results.Public Indexing Services arweave.net graphql the original graphql endpoint, managed by ar.io goldsky search service a public service specifically optimized for search using a superset of the graphql syntax, managed by goldsky ar.io decentralized indexing A decentralized network for indexing services. Currently in testing with L1 transactions available.Executing a GraphQL Query To query arweave we’ll need to access it through an indexing service that supports GraphQL. Use one of the GraphQL playgrounds listed above to get started!Copy and paste in the following query query {
  transactions(tags: [{
    name: "App-Name",
    values: ["PublicSquare"]
  }]) 
  {
    edges {
      node {
        id
        tags {
          name
          value
        }
      }
    }
  }
} If you’re not familiar with GraphQL it can seem a little overwhelming at first but once you know the structure, it’s fairly easy to read and understand.query { <schema type> ( <filter criteria> ) { <data structure of the results> } } In the example query we pasted our <schema type> is transactions but we could also query for blocks. A full description of Arweave's GraphQL schema is written up in the Arweave GraphQL Guide. The guide refers to the filter criteria as “Query Structures” and the complete data structure definition of transactions and blocks as “Data Structures”.When it comes to the <data structure of the results>, the thing to note is that you can specify a subset of the complete data structure you’re interested in. For example, the complete data structure for a transactions schema is listed here.In our case we’re interested in the id and complete list of tags for any transaction matching our filter criteria.Hit the big “Play” button in the middle of the playground to run the query. You’ll notice we get back a list of transactions in the results data structure we specified in our original query.If you’re new to blockchains this is unexpected, we haven’t built anything, why do these results exist? It turns out, the “PublicSquare”: “App-Name” tag we’ve filtered for has been in use for a while.Arweave protocol's founder, Sam Williams, proposed the transaction format a few years ago in a github code snippet. Since then builders in the ecosystem have been building on and around it, experimenting, posting transactions with those tags.Back to querying Arweave. You’ll notice in the GraphQL results that there are no readable post messages, just tags and information about posts.This is because the GraphQL indexing service is concerned with indexing and retrieving header data for transactions and blocks but not their associated data.To get the data of a transaction we need to look it up using another HTTP endpoint.https://arweave.net/<transaction id> Copy and paste one of the id’s in your query results and modify the above link, appending the id. It should look something like this… https://arweave.net/eaUAvulzZPrdh6_cHwUYV473OhvCumqT3K7eWI8tArk The result of navigating to that URL in the browser (HTTP GET) would be retrieving the content of the post (stored in the transactions data). In this example it’s… Woah that's pretty cool 😎 (For a complete listing arweave HTTP endpoints visit the HTTP API documentation.) Posting a Query From JavaScript Posting a GraphQL query from javascript isn't much different than posting it in the playground.First install the arweave-js package for easy access to a GraphQL endpoint.npm install --save arweave Then enter a slightly more advanced version of the example query from above and await the results of posting it.import Arweave from 'arweave';

// initialize an arweave instance
const arweave = Arweave.init({});

// create a query that selects tx data the first 100 tx with specific tags
const queryObject = {
    query:
    `{
        transactions(
            first:100,
            tags: [
                {
                    name: "App-Name",
                    values: ["PublicSquare"]
                },
                {
                    name: "Content-Type",
                    values: ["text/plain"]
                }
            ]
        ) 
        {
            edges {
                node {
                    id
                    tags {
                        name
                        value
                    }
                }
            }
        }
    }`
};
const results = await arweave.api.post('/graphql', queryObject);Multiple Queries It is possible to post multiple queries in a single round-trip to the GraphQL endpoint. This example queries the name transaction (each as a separate query) for two wallet addresses using the now obsolete (replaced by ar-profile) but still permanent arweave-id protocol.query {
    account1: transactions(first: 1, owners:["89tR0-C1m3_sCWCoVCChg4gFYKdiH5_ZDyZpdJ2DDRw"],
        tags: [
            {
                name: "App-Name",
                values: ["arweave-id"]
            },
            {
                name: "Type",
                values: ["name"]
            }
        ]
    ) {
        edges {
            node {
                id
                    owner {
                    address
                }
            }
        }
    }
    account2: transactions(first: 1, owners:["kLx41ALBpTVpCAgymxPaooBgMyk9hsdijSF2T-lZ_Bg"],
        tags: [
            {
                name: "App-Name",
                values: ["arweave-id"]
            },
            {
                name: "Type",
                values: ["name"]
            }
        ]
    ) {
        edges {
            node {
                id
                    owner {
                    address
                }
            }
        }
    }
} Resources Arweave GQL Reference ArDB package ar-gql package Search Indexing Service

---

# 28. ar-gql  Cooking with the Permaweb

Document Number: 28
Source: https://cookbook.arweave.net/guides/querying-arweave/ar-gql.html
Words: 127
Extraction Method: html

ar-gql This package is a minimal layer on top of GraphQL, it supports parameterized queries with query variables. It also implements management of paged results.Installation To install `ar-gql run npm i ar-gql yarn add ar-gql Example import { arGql } from "ar-gql"

const argql = arGql()

(async () => {
    let results = await argql.run(`query( $count: Int ){
    transactions(
      first: $count, 
      tags: [
        {
          name: "App-Name",
          values: ["PublicSquare"]
        },
        {
          name: "Content-Type",
          values: ["text/plain"]
        },
      ]
    ) {
      edges {
        node {
          id
          owner {
            address
          }
          data {
            size
          }
          block {
            height
            timestamp
          }
          tags {
            name,
            value
          }
        }
      }
    }
  }`, {count: 1});
  console.log(results);
})();Resources ar-gql github page open in new window Built with  ❤️  by the Arweave community. Learn more at  Arweave.org

---

# 29. HyperBEAM - Documentation

Document Number: 29
Source: https://hyperbeam.arweave.net/
Words: 154
Extraction Method: html

A Decentralized Operating System.
Built on AO.What is hyperBEAM? Hyperbeam. Powering the decentralized supercomputer: AO.Access, build, and lease hardware for applications and services at any scale.Your gateway to AO, a decentralized supercomputer network built on top of Arweave. AO and Arweave power a cyberspace which guarantees the rights of users, outside of the control of any individual or group.  Communicate via asynchronous
message passing for unheard
of throughput.
 Get resilient compute in
your terminal with one
command.
What Do I Do With Hyperbeam?01 Monetize Your Hardware.Access a shared economy for hardware in the new cyberspace.
All while earning $AO    Offer compute to AO processes and
their users, earning fees in return.
  Run your own gateway.   Empower builders to launch trust-minimized, serverless WASM functions using built-in TEE integrations.
   Coming Soon: Offer support for GPUs.
  Sorry, your browser doesn’t support embedded video.
  Sorry, your browser doesn't support embedded video.
  Sorry, your browser doesn't support embedded video.

---

# 30. meta10 - HyperBEAM - Documentation

Document Number: 30
Source: https://hyperbeam.arweave.net/build/devices/meta-at-1-0.html
Words: 346
Extraction Method: html

Device: ~meta@1.0 Overview The ~meta@1.0 device provides access to metadata and configuration information about the local HyperBEAM node and the broader AO network.This device is essential for:Core Functions (Keys) info Retrieves or modifies the node's configuration message (often referred to as NodeMsg internally).GET /~meta@1.0/info Action: Returns the current node configuration message.Response: A message map containing the node's settings. Sensitive keys (like private wallets) are filtered out. Dynamically generated keys like the node's public address are added if a wallet is configured.POST /~meta@1.0/info Action: Updates the node's configuration message. Requires the request to be signed by the node's configured operator key/address.Request Body: A message map containing the configuration keys and values to update.Response: Confirmation message indicating success or failure.Note: Once a node's configuration is marked as initialized = permanent, it cannot be changed via this method.Key Configuration Parameters Managed by ~meta While the info key is the primary interaction point, the NodeMsg managed by ~meta holds crucial configuration parameters affecting the entire node's behavior, including (but not limited to):port: HTTP server port.priv_wallet / key_location: Path to the node's Arweave key file.operator: The address designated as the node operator (defaults to the address derived from priv_wallet).initialized: Status indicating if the node setup is temporary or permanent.preprocessor / postprocessor: Optional messages defining pre/post-processing logic for requests.routes: Routing table used by dev_router.store: Configuration for data storage.trace: Debug tracing options.p4_*: Payment configuration.faff_*: Access control lists.(Refer to hb_opts.erl for a comprehensive list of options.) Utility Functions (Internal/Module Level) The dev_meta.erl module also contains helper functions used internally or callable from other Erlang modules:is_operator(<RequestMsg>, <NodeMsg>) -> boolean(): Checks if the signer of RequestMsg matches the configured operator in NodeMsg.Pre/Post-Processing Hooks The ~meta device applies the node's configured preprocessor message before resolving the main request and the postprocessor message after obtaining the result, allowing for global interception and modification of requests/responses.Initialization Before a node can process general requests, it usually needs to be initialized. Attempts to access devices other than ~meta@1.0/info before initialization typically result in an error. Initialization often involves setting essential parameters like the operator key via a POST to info.meta module

---

# 31. relay10 - HyperBEAM - Documentation

Document Number: 31
Source: https://hyperbeam.arweave.net/build/devices/relay-at-1-0.html
Words: 366
Extraction Method: html

Device: ~relay@1.0 Overview The ~relay@1.0 device enables HyperBEAM nodes to send messages to external HTTP endpoints or other AO nodes.Core Concept: Message Forwarding This device acts as an HTTP client within the AO ecosystem. It allows a node or process to make outbound HTTP requests.Key Functions (Keys) call Action: Sends an HTTP request to a specified target and waits synchronously for the response.Inputs (from Request Message or Base Message M1):target: (Optional) A message map defining the request to be sent. Defaults to the original incoming request (Msg2 or M1).relay-path or path: The URL/path to send the request to.relay-method or method: The HTTP method (GET, POST, etc.).relay-body or body: The request body.requires-sign: (Optional, boolean) If true, the request message (target) will be signed using the node's key before sending. Defaults to false.http-client: (Optional) Specify a custom HTTP client module to use (defaults to node's configured relay_http_client).Response:{ok, <ResponseMessage>} where <ResponseMessage> is the full message received from the remote peer, or {error, Reason}.Example:GET /~relay@1.0/call?method=GET&path=https://example.com cast Action: Sends an HTTP request asynchronously. The device returns immediately after spawning a process to send the request; it does not wait for or return the response from the remote peer.Inputs: Same as call.Response:{ok, <<"OK">>}.preprocess Action: This function is designed to be used as a node's global preprocessor (configured via ~meta@1.0). When configured, it intercepts all incoming requests to the node and automatically rewrites them to be relayed via the call key. This effectively turns the node into a pure forwarding proxy, using its routing table (dev_router) to determine the destination.Response: A message structure that invokes /~relay@1.0/call with the original request as the target body.Use Cases Inter-Node Communication: Sending messages between HyperBEAM nodes.External API Calls: Allowing AO processes to interact with traditional web APIs.Routing Nodes: Nodes configured with the preprocess key act as dedicated routers/proxies.Client-Side Relaying: A local HyperBEAM instance can use ~relay@1.0 to forward requests to public compute nodes.When call or cast is invoked, the actual HTTP request dispatch is handled by hb_http:request/2. This function often utilizes the node's routing configuration (dev_router) to determine the specific peer/URL to send the request to, especially if the target path is an AO process ID or another internal identifier rather than a full external URL.relay module

---

# 32. scheduler10 - HyperBEAM - Documentation

Document Number: 32
Source: https://hyperbeam.arweave.net/build/devices/scheduler-at-1-0.html
Words: 672
Extraction Method: html

Device: ~scheduler@1.0 Overview The ~scheduler@1.0 device manages the queueing and ordering of messages targeted at a specific process (~process@1.0). It ensures that messages are processed according to defined scheduling rules.Core Concept: Message Ordering When messages are sent to an AO process (typically via the ~push@1.0 device or a POST to the process's /schedule endpoint), they are added to a queue managed by the Scheduler Device associated with that process. The scheduler ensures that messages are processed one after another in a deterministic order, typically based on arrival time and potentially other factors like message nonces or timestamps (depending on the specific scheduler implementation details).The ~process@1.0 device interacts with its configured Scheduler Device (which defaults to ~scheduler@1.0) primarily through the next key to retrieve the next message to be executed.Slot System Slots are a fundamental concept in the ~scheduler@1.0 device, providing a structured mechanism for organizing and sequencing computation.Sequential Ordering: Slots act as numbered containers (starting at 0) that hold specific messages or tasks to be processed in a deterministic order.State Tracking: The at-slot key in a process's state (or a similar internal field like current-slot within the scheduler itself) tracks execution progress, indicating which messages have been processed and which are pending. The slot function can be used to query this.Assignment Storage: Each slot contains an "assignment" - the cryptographically verified message waiting to be executed. These assignments are retrieved using the schedule function or internally via next.Schedule Organization: The collection of all slots for a process forms its "schedule".Application Scenarios:Scheduling Messages: When a message is posted to a process (e.g., via register), it's assigned to the next available slot.Status Monitoring: Clients can query a process's current slot (via the slot function) to check progress.Task Retrieval: Processes find their next task by requesting the next assignment via the next function, which implicitly uses the next slot number based on the current state.Distributed Consistency: Slots ensure deterministic execution order across nodes, crucial for maintaining consistency in AO.This slotting mechanism is central to AO processes built on HyperBEAM, allowing for deterministic, verifiable computation.Key Functions (Keys) These keys are typically accessed via the ~process@1.0 device, which delegates the calls to its configured scheduler.schedule (Handler for GET /<ProcessID>~process@1.0/schedule) Action: Retrieves the list of pending assignments (messages) for the process. May support cursor-based traversal for long schedules.Response: A message map containing the assignments, often keyed by slot number or message ID.register (Handler for POST /<ProcessID>~process@1.0/schedule) Action: Adds/registers a new message to the process's schedule. If this is the first message for a process, it might initialize the scheduler state.Request Body: The message to schedule.Response: Confirmation, potentially including the assigned slot or message ID.slot (Handler for GET /<ProcessID>~process@1.0/slot) Action: Queries the current or a specific slot number within the process's schedule.Response: Information about the requested slot, such as the current highest slot number.status (Handler for GET /<ProcessID>~process@1.0/status) Action: Retrieves status information about the scheduler for the process.Response: A status message.next (Internal Key used by ~process@1.0) Action: Retrieves the next assignment message from the schedule based on the process's current at-slot state.State Management: Requires the current process state (Msg1) containing the at-slot key.Response:{ok, #{ "body" => <NextAssignmentMsg>, "state" => <UpdatedProcessState> }} or {error, Reason} if no next assignment is found.Caching & Lookahead: The implementation uses internal caching (dev_scheduler_cache, priv/assignments) and potentially background lookahead workers to optimize fetching subsequent assignments.init (Internal Key) Action: Initializes the scheduler state for a process, often called when the process itself is initialized.checkpoint (Internal Key) Action: Triggers the scheduler to potentially persist its current state or perform other checkpointing operations.~process@1.0: The primary user of the scheduler, calling next to drive process execution.~push@1.0: Often used to add messages to the schedule via POST /schedule.dev_scheduler_cache: Internal module used for caching assignments locally on the node to reduce latency.Scheduling Unit (SU): Schedulers may interact with external entities (like Arweave gateways or dedicated SU nodes) to fetch or commit schedules, although ~scheduler@1.0 aims for a simpler, often node-local or SU-client model.~scheduler@1.0 provides the fundamental mechanism for ordered, sequential execution within the potentially asynchronous and parallel environment of AO.scheduler module

---

# 33. Core Capabilities - HyperBEAM - Documentation

Document Number: 33
Source: https://hyperbeam.arweave.net/build/hyperbeam-capabilities.html
Words: 685
Extraction Method: html

HyperBEAM: Your Decentralized Development Toolkit HyperBEAM is a versatile, multi-purpose tool that serves as the primary gateway to the AO Computer. It's not a single-purpose application, but rather a powerful, extensible engine—a "Swiss Army knife"—for developers building in the decentralized ecosystem.Designed to be modular, composable, and extensible, HyperBEAM lets you build anything from simple data transformations to complex, high-performance decentralized applications.Thinking in HyperBEAM While AO-Core establishes the foundational concepts of Messages, Devices, and Paths, building on HyperBEAM can be simplified to four key principles:Everything is a message. You can compute on any message by calling its keys by name. The device specified in the message determines how these keys are resolved. The default device, message@1.0, resolves keys to their literal values within the message.Paths are pipelines of messages. A path defines a sequence of 'request' messages to be executed. You can set a key in a message directly within the path using the &key=value syntax. Headers and parameters added after a ? are applied to all messages in the pipeline.Device-specific requests with ~x@y. The ~x@y syntax allows you to apply a request as if the base message had a different device. This provides a powerful way to execute messages using specific compute or storage logic defined by a device.Signed responses over HTTP. The final message in a pipeline is returned as an HTTP response. This response is signed against the hashpath that generated it, ensuring the integrity and verifiability of the computation.Ready to build an AO process?The serverless compute capability is a powerful application of HyperBEAM's modular design. To learn how to create and manage AO processes with WASM or Lua, please refer to the AO Processes Cookbook.Modularity: A System of Devices At its core, HyperBEAM is a modular system built on Devices. Each device is a specialized module responsible for a specific task. This modular architecture means you can think of HyperBEAM's functionality as a set of building blocks.Use Case: Imagine you need to create a serverless API that takes a number, runs a calculation, and returns a result.You would use the ~wasm64@1.0 or ~lua@5.3a devices to execute your calculation logic without needing to manage a server.If your API needs to return JSON, you can pipe the output to the ~json@1.0 device to ensure it's formatted correctly.Composability: Chaining Logic with URL Paths HyperBEAM's modular devices become even more powerful when combined. Its pathing routing mechanism leverages standard URLs to create powerful, composable pipelines. By constructing a URL, you can define a "path" of messages that are executed in sequence, with the output of one message becoming the input for the next.Use Case: Suppose you have a token process and want to calculate the total circulating supply without making the client download and compute all balances. You can construct a single URL that:Reads the latest state of the AO process.Pipes the state to a Lua script and calls the sum function, which sums the balances from the state.Formats the final result as a JSON object.The request would look something like this:/{process-id}~process@1.0/now/~lua@5.3a&module={module-id}/sum/serialize~json@1.0 This path chains together the operations, returning just the computed supply in a single, efficient request.Find the full example in the AO Process Cookbook Learn more about Pathing in HyperBEAM.Extensibility: Building Beyond the Core HyperBEAM is not a closed system. It is designed to be extended, allowing developers to add new functionality tailored to their specific needs.Build Custom Devices You can build and deploy your own devices in Erlang to introduce entirely new, high-level functionality to the network.Use Case: You could build a custom device that acts as a bridge to another blockchain's API, allowing your AO processes to interact with external systems seamlessly.Learn how to Build Your Own Device.Achieve Raw Performance with Native Code For the most demanding, performance-critical tasks, you can write Native Implemented Functions (NIFs) in low-level languages like C or Rust. These NIFs integrate directly with the Erlang VM, offering the highest possible performance.Use Case: If you were building a sophisticated cryptographic application, you could implement a new, high-speed hashing algorithm as a NIF to ensure maximum performance and security. This "raw" extensibility provides an escape hatch for ultimate control.

---

# 34. Intro to AO-Core - HyperBEAM - Documentation

Document Number: 34
Source: https://hyperbeam.arweave.net/build/introduction/what-is-ao-core.html
Words: 435
Extraction Method: html

What is AO-Core? Your browser does not support the video tag.AO-Core is a protocol and standard for distributed computation that forms the foundation of the AO Computer. Inspired by and built upon concepts from the Erlang language, AO-Core embraces the actor model for concurrent, distributed systems. It defines a minimal, generalized model for decentralized computation built around standard web technologies like HTTP.Think of it as a way to interpret the Arweave permaweb not just as static storage, but as a dynamic, programmable, and infinitely scalable computing environment. Unlike traditional blockchain systems, AO-Core defines a flexible, powerful computation protocol that enables a wide range of applications beyond just running Lua programs.Core Concepts AO-Core revolves around three fundamental components:                  Messages Modular Data Packets Messages are cryptographically linked, forming a verifiable computation graph.           Devices Extensible Execution Engines AO-Core introduces a modular architecture centered around Devices. These are pluggable components—typically implemented as modules—that define specific computational logic, such as executing WASM, managing state, or relaying data. Devices interpret and process messages, allowing for flexible and extensible computation. This design enables developers to extend the system by creating custom Devices to fit their specific needs, making the network highly adaptable and composable.                             Paths Composable Pipelines Paths in AO-Core are structures that link messages over time, creating a verifiable history of computations. They allow users to navigate the computation graph and access specific states or results. AO-Core leverages HashPaths —cryptographic fingerprints representing the sequence of operations leading to a specific message state—ensuring traceability and integrity. This pathing mechanism enables developers to compose complex, verifiable data pipelines and interact with processes and data in a flexible, trustless manner.Key Features AO-Core is inherently resilient, running across a global network of machines that eliminates any single point of failure. Its computations are permanent, immutably stored on Arweave so they can be recalled—or continued—at any time. The protocol remains permissionless, meaning anyone can participate. And it is trustless, with every state mathematically verifiable so no central authority is required.The Actor Model in AO Inspired by Erlang, AO-Core implements the actor model to provide a foundation for inherently concurrent, distributed, and scalable systems. In this model, computation is performed by independent actors (or processes). These actors communicate exclusively by passing messages to one another, and each can make local decisions, send more messages, and create new actors.Beyond Processes While AO Processes (smart contracts built using the AO-Core protocol) are a powerful application, AO-Core itself enables a much broader range of computational patterns:Serverless functions with trustless guarantees Hybrid applications combining smart contracts and serverless functionality Custom execution environments through new devices Composable systems using the path language

---

# 35. Pathing in HyperBEAM - HyperBEAM - Documentation

Document Number: 35
Source: https://hyperbeam.arweave.net/build/pathing-in-hyperbeam.html
Words: 844
Extraction Method: html

Pathing in HyperBEAM Overview Understanding how to construct and interpret paths in AO-Core is fundamental to working with HyperBEAM. This guide explains the structure and components of AO-Core paths, enabling you to effectively interact with processes and access their data.HyperBEAM Path Structure Let's examine a typical HyperBEAM endpoint piece-by-piece:https://forward.computer/<procId>~process@1.0/now Node URL (forward.computer) The HTTP response from this node includes a signature from the host's key. By accessing the ~snp@1.0 device, you can verify that the node is running in a genuine Trusted Execution Environment (TEE), ensuring computation integrity. You can replace forward.computer with any HyperBEAM TEE node operated by any party while maintaining trustless guarantees.Process Path (/<procId>~process@1.0) Every path in AO-Core represents a program. Think of the URL bar as a Unix-style command-line interface, providing access to AO's trustless and verifiable compute. Each path component (between / characters) represents a step in the computation. In this example, we instruct the AO-Core node to:Load a specific message from its caches (local, another node, or Arweave) Interpret it with the ~process@1.0 device The process device implements a shared computing environment with consistent state between users State Access (/now or /compute) Devices in AO-Core expose keys accessible via path components. Each key executes a function on the device:now: Calculates real-time process state compute: Serves the latest known state (faster than checking for new messages) Under the surface, these keys represent AO-Core messages. As we progress through the path, AO-Core applies each message to the existing state. You can access the full process state by visiting:/<procId>~process@1.0/now State Navigation You can browse through sub-messages and data fields by accessing them as keys. For example, if a process stores its interaction count in a field named cache, you can access it like this:/<procId>~process@1.0/compute/cache This shows the 'cache' of your process. Each response is:A message with a signature attesting to its correctness A hashpath describing its generation Transferable to other AO-Core nodes for uninterrupted execution Query Parameters and Type Casting Beyond path segments, HyperBEAM URLs can include query parameters that utilize a special type casting syntax. This allows specifying the desired data type for a parameter directly within the URL using the format key+type=value.Syntax: A + symbol separates the parameter key from its intended type (e.g., count+integer=42, items+list="apple",7).Mechanism: The HyperBEAM node identifies the +type suffix (e.g., +integer, +list, +map, +float, +atom, +resolve). It then uses internal functions (hb_singleton:maybe_typed and dev_codec_structured:decode_value) to decode and cast the provided value string into the corresponding Erlang data type before incorporating it into the message.Supported Types: Common types include integer, float, list, map, atom, binary (often implicit), and resolve (for path resolution). List values often follow the HTTP Structured Fields format (RFC 8941).This powerful feature enables the expression of complex data structures directly in URLs.Examples The following examples illustrate using HTTP paths with various AO-Core processes and devices. While these cover a few specific use cases, HyperBEAM's extensible nature allows interaction with any device or process via HTTP paths. For a deeper understanding, we encourage exploring the source code and experimenting with different paths.Example 1: Accessing Full Process State To get the complete, real-time state of a process identified by <procId>, use the /now path component with the ~process@1.0 device:GET /<procId>~process@1.0/now This instructs the AO-Core node to load the process and execute the now function on the ~process@1.0 device.Example 2: Navigating to Specific Process Data If a process maintains its state in a map and you want to access a specific field, like at-slot, using the faster /compute endpoint:GET /<procId>~process@1.0/compute/cache This accesses the compute key on the ~process@1.0 device and then navigates to the cache key within the resulting state map. Using this path, you will see the latest 'cache' of your process (the number of interactions it has received). Every piece of relevant information about your process can be accessed similarly, effectively providing a native API.(Note: This represents direct navigation within the process state structure. For accessing data specifically published via the ~patch@1.0 device, see the documentation on Exposing Process State, which typically uses the /cache/ path.) Example 3: Basic ~message@1.0 Usage Here's a simple example of using ~message@1.0 to create a message and retrieve a value:GET /~message@1.0&greeting="Hello"&count+integer=42/count Base:/ - The base URL of the HyperBEAM node.Root Device:~message@1.0 Query Params:greeting="Hello" (binary) and count+integer=42 (integer), forming the message #{ <<"greeting">> => <<"Hello">>, <<"count">> => 42 }.Path:/count tells ~message@1.0 to retrieve the value associated with the key count.Response: The integer 42.Example 4: Using the ~message@1.0 Device with Type Casting The ~message@1.0 device can be used to construct and query transient messages, utilizing type casting in query parameters.Consider the following URL:GET /~message@1.0&name="Alice"&age+integer=30&items+list="apple",1,"banana"&config+map=key1="val1";key2=true/[PATH] HyperBEAM processes this as follows:Base:/ - The base URL of the HyperBEAM node.Root Device:~message@1.0 Query Parameters (with type casting):name="Alice" -> #{ <<"name">> => <<"Alice">> } (binary) age+integer=30 -> #{ <<"age">> => 30 } (integer) items+list="apple",1,"banana" -> #{ <<"items">> => [<<"apple">>, 1, <<"banana">>] } (list) config+map=key1="val1";key2=true -> #{ <<"config">> => #{<<"key1">> => <<"val1">>, <<"key2">> => true} } (map) Initial Message Map: A combination of the above key-value pairs.Path Evaluation:If [PATH] is /items/1, the response is the integer 1.If [PATH] is /config/key1, the response is the binary <<"val1">>.

---

# 36. Troubleshooting - HyperBEAM - Documentation

Document Number: 36
Source: https://hyperbeam.arweave.net/run/reference/troubleshooting.html
Words: 330
Extraction Method: html

Node Operator Troubleshooting Guide This guide addresses common issues you might encounter when installing and running a HyperBEAM node.Installation Issues Erlang Installation Fails Symptoms: Errors during Erlang compilation or installation Solutions:Ensure all required dependencies are installed: sudo apt-get install -y libssl-dev ncurses-dev make cmake gcc g++ Try configuring with fewer options: ./configure --without-wx --without-debugger --without-observer --without-et Check disk space, as compilation requires several GB of free space Rebar3 Bootstrap Fails Symptoms: Errors when running ./bootstrap for Rebar3 Solutions:Verify Erlang is correctly installed: erl -eval 'erlang:display(erlang:system_info(otp_release)), halt().' Ensure you have the latest version of the repository: git fetch && git reset --hard origin/master Try manually downloading a precompiled Rebar3 binary HyperBEAM Issues HyperBEAM Won't Start Symptoms: Errors when running rebar3 shell or the HyperBEAM startup command Solutions:Check for port conflicts: Another service might be using the configured port Verify the wallet key file exists and is accessible Examine Erlang crash dumps for detailed error information Ensure all required dependencies are installed HyperBEAM Crashes During Operation Symptoms: Unexpected termination of the HyperBEAM process Solutions:Check system resources (memory, disk space) Examine Erlang crash dumps for details Reduce memory limits if the system is resource-constrained Check for network connectivity issues if connecting to external services Compute Unit Issues Compute Unit Won't Start Symptoms: Errors when running npm start in the CU directory Solutions:Verify Node.js is installed correctly: node -v Ensure all dependencies are installed: npm i Check that the wallet file exists and is correctly formatted Verify the .env file has all required settings Integration Issues HyperBEAM Can't Connect to Compute Unit Symptoms: Connection errors in HyperBEAM logs when trying to reach the CU Solutions:Verify the CU is running: curl http://localhost:6363 Ensure there are no firewall rules blocking the connection Verify network configuration if components are on different machines Getting Help If you're still experiencing issues after trying these troubleshooting steps:Check the GitHub repository for known issues Join the Discord community for support Open an issue on GitHub with detailed information about your problem

---

# 37. Configuring Your Machine - HyperBEAM - Documentation

Document Number: 37
Source: https://hyperbeam.arweave.net/run/configuring-your-machine.html
Words: 803
Extraction Method: html

Configuring Your HyperBEAM Node This guide details the various ways to configure your HyperBEAM node's behavior, including ports, storage, keys, and logging.Configuration (config.flat) The primary way to configure your HyperBEAM node is through a config.flat file located in the node's working directory or specified by the HB_CONFIG_LOCATION environment variable.This file uses a simple Key = Value. format (note the period at the end of each line).Example config.flat:% Set the HTTP port
port = 8080.

% Specify the Arweave key file
priv_key_location = "/path/to/your/wallet.json".

% Set the data store directory
% Note: Storage configuration can be complex. See below.
% store = [{local, [{root, <<"./node_data_mainnet">>}]}]. % Example of complex config, not for config.flat

% Enable verbose logging for specific modules
% debug_print = [hb_http, dev_router]. % Example of complex config, not for config.flat Below is a reference of commonly used configuration keys. Remember that config.flat only supports simple key-value pairs (Atoms, Strings, Integers, Booleans). For complex configurations (Lists, Maps), you must use environment variables or hb:start_mainnet/1.Core Configuration These options control fundamental HyperBEAM behavior.Option Type Default Description port Integer 8734 HTTP API port hb_config_location String "config.flat" Path to configuration file priv_key_location String "hyperbeam-key.json" Path to operator wallet key file mode Atom debug Execution mode (debug, prod) Server & Network Configuration These options control networking behavior and HTTP settings.Option Type Default Description host String "localhost" Choice of remote node for non-local tasks gateway String "https://arweave.net" Default gateway bundler_ans104 String "https://up.arweave.net:443" Location of ANS-104 bundler protocol Atom http2 Protocol for HTTP requests (http1, http2, http3) http_client Atom gun HTTP client to use (gun, httpc) http_connect_timeout Integer 5000 HTTP connection timeout in milliseconds http_keepalive Integer 120000 HTTP keepalive time in milliseconds http_request_send_timeout Integer 60000 HTTP request send timeout in milliseconds relay_http_client Atom httpc HTTP client for the relay device Security & Identity These options control identity and security settings.Option Type Default Description scheduler_location_ttl Integer 604800000 TTL for scheduler registration (7 days in ms) Caching & Storage These options control caching behavior. Note: Detailed storage configuration (store option) involves complex data structures and cannot be set via config.flat.Option Type Default Description cache_lookup_heuristics Boolean false Whether to use caching heuristics or always consult the local data store access_remote_cache_for_client Boolean false Whether to access data from remote caches for client requests store_all_signed Boolean true Whether the node should store all signed messages await_inprogress Atom/Boolean named Whether to await in-progress executions (false, named, true) Execution & Processing These options control how HyperBEAM executes messages and processes.Option Type Default Description scheduling_mode Atom local_confirmation When to inform recipients about scheduled assignments (aggressive, local_confirmation, remote_confirmation) compute_mode Atom lazy Whether to execute more messages after returning a result (aggressive, lazy) process_workers Boolean true Whether the node should use persistent processes client_error_strategy Atom throw What to do if a client error occurs wasm_allow_aot Boolean false Allow ahead-of-time compilation for WASM Device Management These options control how HyperBEAM manages devices.Option Type Default Description load_remote_devices Boolean false Whether to load devices from remote signers Debug & Development These options control debugging and development features.Option Type Default Description debug_stack_depth Integer 40 Maximum stack depth for debug printing debug_print_map_line_threshold Integer 30 Maximum lines for map printing debug_print_binary_max Integer 60 Maximum binary size for debug printing debug_print_indent Integer 2 Indentation for debug printing debug_print_trace Atom short Trace mode (short, false) short_trace_len Integer 5 Length of short traces debug_hide_metadata Boolean true Whether to hide metadata in debug output debug_ids Boolean false Whether to print IDs in debug output debug_hide_priv Boolean true Whether to hide private data in debug output Note: For the absolute complete and most up-to-date list, including complex options not suitable for config.flat, refer to the default_message/0 function in the hb_opts module source code.Overrides (Environment Variables & Args) You can override settings from config.flat or provide values if the file is missing using environment variables or command-line arguments.Using Environment Variables:Environment variables typically use an HB_ prefix followed by the configuration key in uppercase.HB_PORT=<port_number>: Overrides hb_port.Example: HB_PORT=8080 rebar3 shell HB_KEY=<path/to/wallet.key>: Overrides hb_key.Example: HB_KEY=~/.keys/arweave_key.json rebar3 shell HB_STORE=<directory_path>: Overrides hb_store.Example: HB_STORE=./node_data_1 rebar3 shell HB_PRINT=<setting>: Overrides hb_print. <setting> can be true (or 1), or a comma-separated list of modules/topics (e.g., hb_path,hb_ao,ao_result).Example: HB_PRINT=hb_http,dev_router rebar3 shell HB_CONFIG_LOCATION=<path/to/config.flat>: Specifies a custom location for the configuration file.Using erl_opts (Direct Erlang VM Arguments):You can also pass arguments directly to the Erlang VM using the -<key> <value> format within erl_opts. This is generally less common for application configuration than config.flat or environment variables.rebar3 shell --erl_opts "-hb_port 8080 -hb_key path/to/key.json" Order of Precedence:Command-line arguments (erl_opts).Settings in config.flat.Environment variables (HB_*).Default values from hb_opts.erl.Configuration in Releases When running a release build (see Running a HyperBEAM Node), configuration works similarly:A config.flat file will be present in the release directory (e.g., _build/default/rel/hb/config.flat). Edit this file to set your desired parameters for the release environment.Environment variables (HB_*) can still be used to override the settings in the release's config.flat when starting the node using the bin/hb script.

---

# 38. FAQ - HyperBEAM - Documentation

Document Number: 38
Source: https://hyperbeam.arweave.net/run/reference/faq.html
Words: 327
Extraction Method: html

Node Operator FAQ This page answers common questions about running and maintaining a HyperBEAM node.What is HyperBEAM?HyperBEAM is a client implementation of the AO-Core protocol written in Erlang. It serves as the node software for a decentralized operating system that allows operators to offer computational resources to users in the AO network.What are the system requirements for running HyperBEAM?Currently, HyperBEAM is primarily tested and documented for Ubuntu 22.04 and macOS. Other platforms will be added in future updates. For detailed requirements, see the System Requirements page.Can I run HyperBEAM in a container?While technically possible, running HyperBEAM in Docker containers or other containerization technologies is currently not recommended. The containerization approach may introduce additional complexity and potential performance issues. We recommend running HyperBEAM directly on the host system until container support is more thoroughly tested and optimized.How do I update HyperBEAM to the latest version?To update HyperBEAM:Pull the latest code from the repository (check Discord for the branch of Beta releases) Rebuild the application Restart the HyperBEAM service Specific update instructions will vary depending on your installation method.Can I run multiple HyperBEAM nodes on a single machine?Yes, you can run multiple HyperBEAM nodes on a single machine, but you'll need to configure them to use different ports and data directories to avoid conflicts. However, this is not recommended for production environments as each node should ideally have a unique IP address to properly participate in the network. Running multiple nodes on a single machine is primarily useful for development and testing purposes.Is there a limit to how many processes can run on a node?The practical limit depends on your hardware resources. Erlang is designed to handle millions of lightweight processes efficiently, but the actual number will be determined by:Available memory CPU capacity Network bandwidth Storage speed The complexity of your processes Where can I get help if I encounter issues?If you encounter issues:Check the Troubleshooting guide Search or ask questions on GitHub Issues Join the community on Discord

---

# 39. ao Specs  Cookbook

Document Number: 39
Source: https://cookbook_ao.arweave.net/concepts/specs.html
Words: 148
Extraction Method: html

Skip to content  ao Specs What is ao?The ao computer is the actor oriented machine that emerges from the network of nodes that adhere to its core data protocol, running on the Arweave network. This document gives a brief introduction to the protocol and its functionality, as well as its technical details, such that builders can create new implementations and services that integrate with it.The ao computer is a single, unified computing environment (a Single System Image), hosted on a heterogenous set of nodes in a distributed network. ao is designed to offer an environment in which an arbitrary number of parallel processes can be resident, coordinating through an open message passing layer. This message passing standard connects the machine's independently operating processes together into a 'web' -- in the same way that websites operate on independent servers but are conjoined into a cohesive, unified experience via hyperlinks.

---

# 40. Messages  Cookbook

Document Number: 40
Source: https://cookbook_ao.arweave.net/concepts/messages.html
Words: 310
Extraction Method: html

Messages The Message serves as the fundamental data protocol unit within ao, crafted from ANS-104 DataItems, thereby aligning with the native structure of Arweave. When engaged in a Process, a Message is structured as follows:This architecture merges the Assignment Type with the Message Type, granting the Process a comprehensive understanding of the Message's context for effective processing.When sending a message, here is a visual diagram of how the messages travels through the ao computer. The message workflow initiates with the MU (Messenger Unit), where the message's signature is authenticated. Following this, the SU (Scheduler Unit) allocates an Epoch and Nonce to the message, bundles the message with an Assignment Type, and dispatches it to Arweave. Subsequently, the aoconnect library retrieves the outcome from the CU (Compute Unit). The CU then calls for all preceding messages leading up to the current Message Id from the SU (Scheduler Unit), processes them to deduce the result. Upon completion, the computed result is conveyed back to aoconnect, which is integrated within client interfaces such as aos.Ethereum Signed Message If the Message ANS-104 DataItem was signed using Ethereum keys, then the value in the Owner and From fields will be the EIP-55 Ethereum address of the signer. For example: 0xfB6916095ca1df60bB79Ce92cE3Ea74c37c5d359.Summary Messages serve as the primary data protocol type for the ao network, leveraging ANS-104 Data-Items native to Arweave. Messages contain several fields including data content, origin, target, and cryptographic elements like signatures and nonces. They follow a journey starting at the Messenger Unit (MU), which ensures they are signed, through the Scheduler Unit (SU) that timestamps and sequences them, before being bundled and published to Arweave. The aoconnect library then reads the result from the Compute Unit (CU), which processes messages to calculate results and sends responses back through aoconnect, utilized by clients such as aos. The CU is the execution environment for these processes.

---

# 41. A whistle stop tour of Lua  Cookbook

Document Number: 41
Source: https://cookbook_ao.arweave.net/concepts/lua.html
Words: 870
Extraction Method: html

A whistle stop tour of Lua.Before we can explore ao in greater depth, let's take a moment to learn the basics of Lua: your companion for commanding aos processes.Lua is a simple language with few surprises. If you know Javascript, it will feel like a simplified, purer version. If you are learning from-scratch, it will seem like a tiny language that focuses on the important stuff: Clean computation with sane syntax.In this section we will cover the basics of Lua in just a few minutes. If you already know Lua, jump right through to the next chapter Jumping back into your aos process.For the purpose of this tutorial, we will be assuming that you have already completed the getting started guide. If not, complete that first.If you logged out of your process, you can always re-open it by running aos on your command line, optionally specifying your key file with --wallet [location].Basic Lua expressions.In the remainder of this primer we will quickly run through Lua's core features and syntax.Try out on the examples on your aos process as you go, or skip them if they are intuitive to you.Basic arithmetic: Try some basic arithmetic, like 5 + 3. After processing, you will see the result 8. +, -, *, /, and ^ all work as you might expect. % is the symbol that Lua uses for modulus.Setting variables: Type a = 10 and press enter. This sets the variable a to 10. By convention (not enforced by the language), global variables start with a capital letter in Lua (for example Handlers).Using variables: Now type a * 2. You will see 20 returned on the command line.String concatenation: Say hello to yourself by executing "Hello, " .. ao.id.INFO Note that while global variables conventionally start with a capital letter in Lua, this is not enforced by the language. For example, the ao module is a global variable that was intentionally lowercased for stylistic purposes.Experimenting with conditional statements.If-Else: Like most programming languages, Lua uses if-else blocks to conditionally execute code.In your aos process, type .editor and press enter. This will open an in-line text editor within your command-line interface.Once you are finished editing on your terminal, type .done on a new line and press enter. This will terminate edit mode and submit the expression to your process for evaluation.As a result, you will see that aos coolness is >9,000 cool. Good to know.if statements in Lua can also have additional elseif [condition] then blocks, making conditional execution hierarchies easier.Looping in Lua.There are a few different ways to loop in your code in Lua. Here are our favorites:While loops:Start by initializing your counter to zero by typing n = 0 and pressing enter.Then open the inline editor again with .editor.Type .done on a new line to execute the while loop. You can check the result of the loop by simply running n.For loops:Lua can also execute python-style for loops between a set of values. For example, use the .editor to enter the following code block:Request the new value of the variable by running n again.Getting functional.Define a function:Using the .editor once again, submit the following lines:Lua also has 'anonymous' or 'higher order' functions. These essentially allow you to use functions themselves as if they are normal data -- to be passed as arguments to other functions, etc. The following example defines an anonymous function and is equivalent to the above:Calling the function: Call the function with greeting("Earthling"). aos will return "Hello, Earthling".INFO Handlers in ao commonly utilize anonymous functions. When using Handlers.add(), the third argument is an anonymous function in the form function(msg) ... end. This is a key pattern you'll see frequently when working with ao processes.Defining deep objects with tables.Tables are Lua's only compound data structure. They map keys to values, but can also be used like traditional arrays.Create a simple table: Type ao_is = {"hyper", "parallel", "compute"} to create a simple table.Accessing the table's elements: Access an element with ao_is[2]. aos will return parallel. Note: Indices in Lua start from 1!Count a table's elements: The size of a table in Lua is found with the operator #. For example, running #ao_is will return 3.Set a named element: Type ao_is["cool"] = true to add a new named key to the table. Named elements can also be accessed with the . operator (e.g. ao_is.cool), but only if the key is a valid identifier - for other keys like "my key", use brackets.Lua Wats.aos uses Lua because it is a simple, clean language that most experienced programmers can learn very quickly, and is an increasingly popular first programming language, too, thanks to its use in video games like Roblox.Nonetheless, there are a few things about the language that are prone to trip up rookie Lua builders. Tastes may vary, but here is our exhaustive list of Lua wat s:Remember: Table indexing starts from 1 not 0!Remember: 'Not equals' is expressed with ~=, rather than != or similar.Remember: Objects in Lua are called 'tables', rather than their more common names.Let's go!With this in mind, you now know everything you need in order to build awesome decentralized processes with Lua! In the next chapter we will begin to build parallel processes with Lua and aos.

---

# 42. aos Brief Tour  Cookbook

Document Number: 42
Source: https://cookbook_ao.arweave.net/concepts/tour.html
Words: 390
Extraction Method: html

aos Brief Tour Welcome to a quick tour of aos! This tutorial will walk you through the key global functions and variables available in the aos environment, giving you a foundational understanding of how to interact with and utilize aos effectively.1. Introduction to Inbox What It Is: Inbox is a Lua table that stores all messages received by your process but not yet handled.How to Use: Check Inbox to see incoming messages. Iterate through Inbox[x] to process these messages.2. Sending Messages with Send(Message) Functionality: Send(Message) is a global function to send messages to other processes.Usage Example: Send({Target = "...", Data = "Hello, Process!"}) sends a message with the data "Hello, Process!" to a specified process.3. Creating Processes with Spawn(Module, Message) Purpose: Use Spawn(Module, Message) to create new processes.Example: Spawn("MyModule", {Data = "Start"}) starts a new process using "MyModule" with the provided message.4. Understanding Name and Owner Name: A string set during initialization, representing the process's name.Owner: Indicates the owner of the process. Changing this might restrict your ability to interact with your process.Important Note: Treat these as read-only to avoid issues.5. Utilizing Handlers What They Are: Handlers is a table of helper functions for creating message handlers.Usage: Define handlers in Handlers to specify actions for different incoming messages based on pattern matching.6. Data Representation with Dump Function: Dump converts any Lua table into a print-friendly format.How to Use: Useful for debugging or viewing complex table structures. Example: Dump(Inbox) prints the contents of Inbox.7. Leveraging Utils Module Contents: Utils contains a collection of functional utilities like map, reduce, and filter.Usage: Great for data manipulation and functional programming patterns in Lua. For example, Utils.map(myTable, function(x) return x * 2 end) to double the values in a table.8. Exploring the ao Core Library Description: ao is a core module that includes key functions for message handling and process management.Key Features: Includes functions for sending messages (send) and spawning processes (spawn), along with environment variables.Conclusion This brief tour introduces you to the primary globals and functionalities within the aos environment. With these tools at your disposal, you can create and manage processes, handle messages, and utilize Lua's capabilities to build efficient and responsive applications on the aos platform. Experiment with these features to get a deeper understanding and to see how they can be integrated into your specific use cases. Happy coding in aos!

---

# 43. Sending a Message to a Process  Cookbook

Document Number: 43
Source: https://cookbook_ao.arweave.net/guides/aoconnect/sending-messages.html
Words: 391
Extraction Method: html

Sending a Message to a Process A deep dive into the concept of Messages can be found in the ao Messages concept. This guide focuses on using ao connect to send a message to a process.Sending a message is the central way in which your app can interact with ao. A message is input to a process. There are 5 parts of a message that you can specify which are "target", "data", "tags", "anchor", and finally the messages "signature".Refer to your process module's source code or documentation to see how the message is used in its computation. The ao connect library will translate the parameters you pass it in the code below, construct a message, and send it.🎓 To Learn more about Wallets visit the Permaweb Cookbook Sending a Message in NodeJS Need a test wallet, use npx -y @permaweb/wallet > /path/to/wallet.json to create a wallet keyfile.jsimport { readFileSync } from "node:fs";

import { message, createDataItemSigner } from "@permaweb/aoconnect";

const wallet = JSON.parse(

  readFileSync("/path/to/arweave/wallet.json").toString(),

);

// The only 2 mandatory parameters here are process and signer

await message({

  /*

    The arweave TxID of the process, this will become the "target".

    This is the process the message is ultimately sent to.

  */

  process: "process-id",

  // Tags that the process will use as input.

  tags: [

    { name: "Your-Tag-Name-Here", value: "your-tag-value" },

    { name: "Another-Tag", value: "another-value" },

  ],

  // A signer function used to build the message "signature"

  signer: createDataItemSigner(wallet),

  /*

    The "data" portion of the message

    If not specified a random string will be generated

  */

  data: "any data",

})

  .then(console.log)

  .catch(console.error);Sending a Message in a browser New to building permaweb apps check out the Permaweb Cookbook jsimport { message, createDataItemSigner } from "@permaweb/aoconnect";

// The only 2 mandatory parameters here are process and signer

await message({

  /*

    The arweave TxID of the process, this will become the "target".

    This is the process the message is ultimately sent to.

  */

  process: "process-id",

  // Tags that the process will use as input.

  tags: [

    { name: "Your-Tag-Name-Here", value: "your-tag-value" },

    { name: "Another-Tag", value: "another-value" },

  ],

  // A signer function used to build the message "signature"

  signer: createDataItemSigner(globalThis.arweaveWallet),

  /*

    The "data" portion of the message.

    If not specified a random string will be generated

  */

  data: "any data",

})

  .then(console.log)

  .catch(console.error);If you would like to learn more about signers, click here

---

# 44. DataItem Signers  Cookbook

Document Number: 44
Source: https://cookbook_ao.arweave.net/guides/aoconnect/signers.html
Words: 286
Extraction Method: html

DataItem Signers Every message sent to AO MUST be signed, aoconnect provides a helper function for signing messages or spawning new processes. This helper function createDataItemSigner is provided for arweave wallets. But you can create your own Signer instance too.What is a Wallet/Keyfile?A wallet/keyfile is a public/private key pair that can be used to sign and encrypt data.What is an ao message/dataItem?You often see the terms message and dataItem used interchangeably in the documentation, a message is a data-protocol type in ao that uses the dataItem specification to describe the messages intent. A dataItem is defined in the ANS-104 bundle specification. A dataItem is the preferred format of storage for arweave bundles. A bundle is a collection of these signed dataItems. A message implements specific tags using the dataItem specification. When developers send messages to ao, they are publishing dataItems on arweave.🎓 To learn more about messages click here and to learn more about ANS-104 dataItems click here What is a signer?A signer is function that takes data, tags, anchor, target and returns an object of id, binary representing a signed dataItem. AO accepts arweave signers and ethereum signers. createDataItemSigner is a helper function that can take an arweave keyfile or a browser instance of an arweave wallet usually located in the global scope of the browser, when I user connects to a wallet using an extension or html app.Examples arweave keyfile NOTE: if you do not have a wallet keyfile you can create one using npx -y @permaweb/wallet > wallet.json arweave browser extension NOTE: This implementation works with ArweaveWalletKit, ArConnect, and Arweave.app ethereum key Summary Using the signer function developers can control how dataItems are signed without having to share the signing process with aoconnect.

---

# 45. Staking Blueprint  Cookbook

Document Number: 45
Source: https://cookbook_ao.arweave.net/guides/aos/blueprints/staking.html
Words: 423
Extraction Method: html

Staking Blueprint The Staking Blueprint is a predesigned template that helps you quickly build a staking system in ao. It is a great way to get started and can be customized to fit your needs.Prerequisites The Staking Blueprint requires the Token Blueprint to be loaded, first.Unpacking the Staking Blueprint Stakers: The Stakers array is used to store the staked tokens of the participants.Unstaking: The Unstaking array is used to store the unstaking requests of the participants.Stake Action Handler: The stake handler allows processes to stake tokens. When a process sends a message with the tag Action = "Stake", the handler will add the staked tokens to the Stakers array and send a message back to the process confirming the staking.Unstake Action Handler: The unstake handler allows processes to unstake tokens. When a process sends a message with the tag Action = "Unstake", the handler will add the unstaking request to the Unstaking array and send a message back to the process confirming the unstaking.Finalization Handler: The finalize handler allows processes to finalize the staking process. When a process sends a message with the tag Action = "Finalize", the handler will process the unstaking requests and finalize the staking process.How To Use:Open your preferred text editor.Open the Terminal.Start your aos process.Type in .load-blueprint staking Verify the Blueprint is Loaded:Type in Handlers.list to see the newly loaded handlers.What's in the Staking Blueprint:luaStakers = Stakers or {}

Unstaking = Unstaking or {}

-- Stake Action Handler

Handlers.stake = function(msg)

  local quantity = tonumber(msg.Tags.Quantity)

  local delay = tonumber(msg.Tags.UnstakeDelay)

  local height = tonumber(msg['Block-Height'])

  assert(Balances[msg.From] and Balances[msg.From] >= quantity, "Insufficient balance to stake")

  Balances[msg.From] = Balances[msg.From] - quantity

  Stakers[msg.From] = Stakers[msg.From] or {}

  Stakers[msg.From].amount = (Stakers[msg.From].amount or 0) + quantity

  Stakers[msg.From].unstake_at = height + delay

end

-- Unstake Action Handler

Handlers.unstake = function(msg)

  local quantity = tonumber(msg.Tags.Quantity)

  local stakerInfo = Stakers[msg.From]

  assert(stakerInfo and stakerInfo.amount >= quantity, "Insufficient staked amount")

  stakerInfo.amount = stakerInfo.amount - quantity

  Unstaking[msg.From] = {

      amount = quantity,

      release_at = stakerInfo.unstake_at

  }

end

-- Finalization Handler

local finalizationHandler = function(msg)

  local currentHeight = tonumber(msg['Block-Height'])

  -- Process unstaking

  for address, unstakeInfo in pairs(Unstaking) do

      if currentHeight >= unstakeInfo.release_at then

          Balances[address] = (Balances[address] or 0) + unstakeInfo.amount

          Unstaking[address] = nil

      end

  end

end

-- wrap function to continue handler flow

local function continue(fn)

  return function (msg)

    local result = fn(msg)

    if (result) == -1 then

      return 1

    end

    return result

  end

end

-- Registering Handlers

Handlers.add("stake",

  continue(Handlers.utils.hasMatchingTag("Action", "Stake")), Handlers.stake)

Handlers.add("unstake",

  continue(Handlers.utils.hasMatchingTag("Action", "Unstake")), Handlers.unstake)

-- Finalization handler should be called for every message

Handlers.add("finalize", function (msg) return -1 end, finalizationHandler)

---

# 46. CRED Utils Blueprint  Cookbook

Document Number: 46
Source: https://cookbook_ao.arweave.net/guides/aos/blueprints/cred-utils.html
Words: 661
Extraction Method: html

CRED Utils Blueprint CRED is now deprecated CRED was a token used during ao's legacynet phase to reward early developers. It is no longer earnable or redeemable.The CRED Utils Blueprint is a predesigned template that helps you quickly check your CRED balance in ao legacynet.Unpacking the CRED Utils Blueprint The CRED Metatable CRED.balance: Evaluating CRED.balance will print your process's last known balance of your CRED. If you have never fetched your CRED balance before, it will be fetched automatically. If you think your CRED has recently changed, consider running CRED.update first.CRED.process: Evaluating CRED.process will print the process ID of the CRED token issuer.CRED.send: Invoking CRED.send(targetProcessId, amount) like a function will transfer CRED from your ao process to another ao process.targetProcessId: string: the 43-character process ID of the recipient.amount: integer: The quantity of CRED units to send. 1 CRED === 1000 CRED units.CRED.update: Evaluating CRED.update will fetch your latest CRED balance by sending a message to the CRED issuer process. The UpdateCredBalance handler (see below) will ingest the response message.Handler Definitions Credit Handler: The CRED_Credit handler allows the CRED issuer process (and aos) to automatically notify you when your CRED balance increase.Debit Handler: The CRED_Debit handler allows the CRED issuer process (and aos) to automatically notify you when your CRED balance decreases.Update Balance Handler: The UpdateCredBalance handler ingests the response to any CRED.update requests.How To Use the Blueprint Open the Terminal.Start your aos process.Type in .load-blueprint credUtils Type in CRED.balance What's in the CRED Utils Blueprint:See the aos source code on GitHub for the blueprint shipped in the latest version of aos.luaCRED_PROCESS = "Sa0iBLPNyJQrwpTTG-tWLQU-1QeUAJA73DdxGGiKoJc"

_CRED = { balance = "Your CRED balance has not been checked yet. Updating now." }

local credMeta = {

    __index = function(t, key)

        -- sends CRED balance request

        if key == "update" then

            Send({ Target = CRED_PROCESS, Action = "Balance", Tags = { ["Target"] = ao.id } })

            return "Balance update requested."

            -- prints local CRED balance, requests it if not set

        elseif key == "balance" then

            if _CRED.balance == "Your CRED balance has not been checked yet. Updating now." then

                Send({ Target = CRED_PROCESS, Action = "Balance", Tags = { ["Target"] = ao.id } })

            end

            return _CRED.balance

            -- prints CRED process ID

        elseif key == "process" then

            return CRED_PROCESS

            -- tranfers CRED

        elseif key == "send" then

            return function(target, amount)

                -- ensures amount is string

                amount = tostring(amount)

                print("sending " .. amount .. "CRED to " .. target)

                Send({ Target = CRED_PROCESS, Action = "Transfer", ["Recipient"] = target, ["Quantity"] = amount })

            end

        else

            return nil

        end

    end

}

CRED = setmetatable({}, credMeta)

-- Function to evaluate if a message is a balance update

local function isCredBalanceMessage(msg)

    if msg.From == CRED_PROCESS and msg.Tags.Balance then

        return true

    else

        return false

    end

end

-- Function to evaluate if a message is a Debit Notice

local function isDebitNotice(msg)

    if msg.From == CRED_PROCESS and msg.Tags.Action == "Debit-Notice" then

        return true

    else

        return false

    end

end

-- Function to evaluate if a message is a Credit Notice

local function isCreditNotice(msg)

    if msg.From == CRED_PROCESS and msg.Tags.Action == "Credit-Notice" then

        return true

    else

        return false

    end

end

local function formatBalance(balance)

    -- Ensure balance is treated as a string

    balance = tostring(balance)

    -- Check if balance length is more than 3 to avoid unnecessary formatting

    if #balance > 3 then

        -- Insert dot before the last three digits

        balance = balance:sub(1, -4) .. "." .. balance:sub(-3)

    end

    return balance

end

-- Handles Balance messages

Handlers.add(

    "UpdateCredBalance",

    isCredBalanceMessage,

    function(msg)

        local balance = nil

        if msg.Tags.Balance then

            balance = msg.Tags.Balance

        end

        -- Format the balance if it's not set

        if balance then

            -- Format the balance by inserting a dot after the first three digits from the right

            local formattedBalance = formatBalance(balance)

            _CRED.balance = formattedBalance

            print("CRED Balance updated: " .. _CRED.balance)

        else

            print("An error occurred while updating CRED balance")

        end

    end

)

-- Handles Debit notices

Handlers.add(

    "CRED_Debit",

    isDebitNotice,

    function(msg)

        print(msg.Data)

    end

)

-- Handles Credit notices

Handlers.add(

    "CRED_Credit",

    isCreditNotice,

    function(msg)

        print(msg.Data)

    end

)

---

# 47. Spawning a Process  Cookbook

Document Number: 47
Source: https://cookbook_ao.arweave.net/guides/aoconnect/spawning-processes.html
Words: 160
Extraction Method: html

Skip to content  Spawning a Process A deep dive into the concept of Processes can be found in the ao Processes concept. This guide focuses on using ao connect to spawn a Process.In order to spawn a Process you must have the TxID of an ao Module that has been uploaded to Arweave. The Module is the source code for the Process. The Process itself is an instantiation of that source.You must also have the wallet address of a Scheduler Unit (SU). This specified SU will act as the scheduler for this Process. This means that all nodes in the system can tell that they need to read and write to this SU for this Process. You can use the address below.Wallet address of an available Scheduler lua_GQ33BkPtZrqxA84vM8Zk-N2aO0toNNu_C-l-rawrBA In addition, in order to receive messages from other processes an Authority tag must be supplied with the wallet address of an authorised Messaging Unit (MU).Wallet address of the legacynet MU luafcoN_xJeisVsPXA-trzVAuIiqO3ydLQxM-L4XbrQKzY

---

# 48. Voting Blueprint  Cookbook

Document Number: 48
Source: https://cookbook_ao.arweave.net/guides/aos/blueprints/voting.html
Words: 340
Extraction Method: html

Voting Blueprint The Voting Blueprint is a predesigned template that helps you quickly build a voting system in ao. It is a great way to get started and can be customized to fit your needs.Prerequisites The Staking Blueprint requires the Token Blueprint to be loaded, first.Unpacking the Voting Blueprint Balances: The Balances array is used to store the token balances of the participants.Votes: The Votes array is used to store the votes of the participants.Vote Action Handler: The vote handler allows processes to vote. When a process sends a message with the tag Action = "Vote", the handler will add the vote to the Votes array and send a message back to the process confirming the vote.Finalization Handler: The finalize handler allows processes to finalize the voting process. When a process sends a message with the tag Action = "Finalize", the handler will process the votes and finalize the voting process.How To Use:Open your preferred text editor.Open the Terminal.Start your aos process.Type in .load-blueprint voting Verify the Blueprint is Loaded:Type in Handlers.list to see the newly loaded handlers.What's in the Voting Blueprint:luaBalances = Balances or {}

Votes = Votes or {}

-- Vote Action Handler

Handlers.vote = function(msg)

  local quantity = Stakers[msg.From].amount

  local target = msg.Tags.Target

  local side = msg.Tags.Side

  local deadline = tonumber(msg['Block-Height']) + tonumber(msg.Tags.Deadline)

  assert(quantity > 0, "No staked tokens to vote")

  Votes[target] = Votes[target] or { yay = 0, nay = 0, deadline = deadline }

  Votes[target][side] = Votes[target][side] + quantity

end

-- Finalization Handler

local finalizationHandler = function(msg)

  local currentHeight = tonumber(msg['Block-Height'])

  -- Process voting

  for target, voteInfo in pairs(Votes) do

      if currentHeight >= voteInfo.deadline then

          if voteInfo.yay > voteInfo.nay then

              print("Handle Vote")

          end

          -- Clear the vote record after processing

          Votes[target] = nil

      end

  end

end

-- wrap function to continue handler flow

local function continue(fn)

  return function (msg)

    local result = fn(msg)

    if (result) == -1 then

      return 1

    end

    return result

  end

end

Handlers.add("vote",

  continue(Handlers.utils.hasMatchingTag("Action", "Vote")), Handlers.vote)

-- Finalization handler should be called for every message

Handlers.add("finalize", function (msg) return -1 end, finalizationHandler)

---

# 49. Editor setup  Cookbook

Document Number: 49
Source: https://cookbook_ao.arweave.net/guides/aos/editor.html
Words: 218
Extraction Method: html

Editor setup Remembering all the built in ao functions and utilities can sometimes be hard. To enhance your developer experience, it is recommended to install the Lua Language Server extension into your favorite text editor and add the ao addon. It supports all built in aos modules and globals.VS Code Install the sumneko.lua extension:Search for "Lua" by sumneko in the extension marketplace Download and install the extension Open the VS Code command palette with Shift + Command + P (Mac) / Ctrl + Shift + P (Windows/Linux) and run the following command:In the Addon Manager, search for "ao", it should be the first result. Click "Enable" and enjoy autocomplete!Other editors Verify that your editor supports the language server protocol Install Lua Language Server by following the instructions at luals.github.io Install the "ao" addon to the language server BetterIDEa BetterIDEa is a custom web based IDE for developing on ao.It offers a built in Lua language server with ao definitions, so you don't need to install anything. Just open the IDE and start coding!Features include:Code completion Cell based notebook ui for rapid development Easy process management Markdown and Latex cell support Share projects with anyone through ao processes Tight integration with ao package manager Read detailed information about the various features and integrations of the IDE in the documentation.

---

# 50. FAQ  Cookbook

Document Number: 50
Source: https://cookbook_ao.arweave.net/guides/aos/faq.html
Words: 179
Extraction Method: html

Skip to content  FAQ Ownership Understanding Process Ownership Start a new process with the aos console, the ownership of the process is set to your wallet address. aos uses the Owner global variable to define the ownership of the process. If you wish to transfer ownership or lock the process so that no one can own, you simply modify the Owner variable to another wallet address or set it to nil.JSON encoding data as json When sending data to another process or an external service, you may want to use JSON as a way to encode the data for recipients. Using the json module in lua, you can encode and decode pure lua tables that contain values.Send vs ao.send When to use Send vs ao.send Both functions send a message to a process, the difference is ao.send returns the message, in case you want to log it or troubleshoot. The Send function is intended to be used in the console for easier access. It is preferred to use ao.send in the handlers. But they are both interchangeable in aos.

---

# 51. CLI  Cookbook

Document Number: 51
Source: https://cookbook_ao.arweave.net/guides/aos/cli.html
Words: 181
Extraction Method: html

Skip to content  CLI There are some command-line arguments you pass to aos to do the following:[name] - create a new process or loads an existing process for your wallet --load <file> - load a file, you can add one or many of this command --cron <interval> - only used when creating a process --wallet <walletfile> - use a specific wallet Managing multiple processes with aos shaos Starts or connects to a process with the name default shaos chatroom Starts or connects to a process with the name of chatroom shaos treasureRoom Starts or connects to a process with the name of treasureRoom Load flag With the load flag I can load many source files to my process CRON Flag If you want to setup your process to react on a schedule we need to tell ao, we do that when we spawn the process.Tag flags With the tag flags, you can start a process with some custom tags (for e.g. using them as static environment variables):The command above will add the extra tags to the transaction that spawns your process:

---

# 52. Token Blueprint  Cookbook

Document Number: 52
Source: https://cookbook_ao.arweave.net/guides/aos/blueprints/token.html
Words: 972
Extraction Method: html

Token Blueprint The Token Blueprint is a predesigned template that helps you quickly build a token in ao. It is a great way to get started and can be customized to fit your needs.Unpacking the Token Blueprint Balances: The Balances array is used to store the token balances of the participants.Info Handler: The info handler allows processes to retrieve the token parameters, like Name, Ticker, Logo, and Denomination.Balance Handler: The balance handler allows processes to retrieve the token balance of a participant.Balances Handler: The balances handler allows processes to retrieve the token balances of all participants.Transfer Handler: The transfer handler allows processes to send tokens to another participant.Mint Handler: The mint handler allows processes to mint new tokens.Total Supply Handler: The totalSupply handler allows processes to retrieve the total supply of the token.Burn Handler: The burn handler allows processes to burn tokens.How To Use:Open your preferred text editor.Open the Terminal.Start your aos process.Type in .load-blueprint token Verify the Blueprint is Loaded:Type in Handlers.list to see the newly loaded handlers.What's in the Token Blueprint:lualocal bint = require('.bint')(256)

--[[

  This module implements the ao Standard Token Specification.

  Terms:

    Sender: the wallet or Process that sent the Message

  It will first initialize the internal state, and then attach handlers,

    according to the ao Standard Token Spec API:

    - Info(): return the token parameters, like Name, Ticker, Logo, and Denomination

    - Balance(Target?: string): return the token balance of the Target. If Target is not provided, the Sender

        is assumed to be the Target

    - Balances(): return the token balance of all participants

    - Transfer(Target: string, Quantity: number): if the Sender has a sufficient balance, send the specified Quantity

        to the Target. It will also issue a Credit-Notice to the Target and a Debit-Notice to the Sender

    - Mint(Quantity: number): if the Sender matches the Process Owner, then mint the desired Quantity of tokens, adding

        them the Processes' balance

]]

--

local json = require('json')

--[[

  utils helper functions to remove the bint complexity.

]]

--

local utils = {

  add = function(a, b)

    return tostring(bint(a) + bint(b))

  end,

  subtract = function(a, b)

    return tostring(bint(a) - bint(b))

  end,

  toBalanceValue = function(a)

    return tostring(bint(a))

  end,

  toNumber = function(a)

    return bint.tonumber(a)

  end

}

--[[

     Initialize State

     ao.id is equal to the Process.Id

   ]]

--

Variant = "0.0.3"

-- token should be idempotent and not change previous state updates

Denomination = Denomination or 12

Balances = Balances or { [ao.id] = utils.toBalanceValue(10000 * 10 ^ Denomination) }

TotalSupply = TotalSupply or utils.toBalanceValue(10000 * 10 ^ Denomination)

Name = Name or 'Points Coin'

Ticker = Ticker or 'PNTS'

Logo = Logo or 'SBCCXwwecBlDqRLUjb8dYABExTJXLieawf7m2aBJ-KY'

--[[

     Add handlers for each incoming Action defined by the ao Standard Token Specification

   ]]

--

--[[

     Info

   ]]

--

Handlers.add('info', "Info", function(msg)

  msg.reply({

    Name = Name,

    Ticker = Ticker,

    Logo = Logo,

    Denomination = tostring(Denomination)

  })

end)

--[[

     Balance

   ]]

--

Handlers.add('balance', "Balance", function(msg)

  local bal = '0'

  -- If not Recipient is provided, then return the Senders balance

  if (msg.Tags.Recipient) then

    if (Balances[msg.Tags.Recipient]) then

      bal = Balances[msg.Tags.Recipient]

    end

  elseif msg.Tags.Target and Balances[msg.Tags.Target] then

    bal = Balances[msg.Tags.Target]

  elseif Balances[msg.From] then

    bal = Balances[msg.From]

  end

  msg.reply({

    Balance = bal,

    Ticker = Ticker,

    Account = msg.Tags.Recipient or msg.From,

    Data = bal

  })

end)

--[[

     Balances

   ]]

--

Handlers.add('balances', "Balances",

  function(msg) msg.reply({ Data = json.encode(Balances) }) end)

--[[

     Transfer

   ]]

--

Handlers.add('transfer', "Transfer", function(msg)

  assert(type(msg.Recipient) == 'string', 'Recipient is required!')

  assert(type(msg.Quantity) == 'string', 'Quantity is required!')

  assert(bint.__lt(0, bint(msg.Quantity)), 'Quantity must be greater than 0')

  if not Balances[msg.From] then Balances[msg.From] = "0" end

  if not Balances[msg.Recipient] then Balances[msg.Recipient] = "0" end

  if bint(msg.Quantity) <= bint(Balances[msg.From]) then

    Balances[msg.From] = utils.subtract(Balances[msg.From], msg.Quantity)

    Balances[msg.Recipient] = utils.add(Balances[msg.Recipient], msg.Quantity)

    --[[

         Only send the notifications to the Sender and Recipient

         if the Cast tag is not set on the Transfer message

       ]]

    --

    if not msg.Cast then

      -- Debit-Notice message template, that is sent to the Sender of the transfer

      local debitNotice = {

        Action = 'Debit-Notice',

        Recipient = msg.Recipient,

        Quantity = msg.Quantity,

        Data = Colors.gray ..

            "You transferred " ..

            Colors.blue .. msg.Quantity .. Colors.gray .. " to " .. Colors.green .. msg.Recipient .. Colors.reset

      }

      -- Credit-Notice message template, that is sent to the Recipient of the transfer

      local creditNotice = {

        Target = msg.Recipient,

        Action = 'Credit-Notice',

        Sender = msg.From,

        Quantity = msg.Quantity,

        Data = Colors.gray ..

            "You received " ..

            Colors.blue .. msg.Quantity .. Colors.gray .. " from " .. Colors.green .. msg.From .. Colors.reset

      }

      -- Add forwarded tags to the credit and debit notice messages

      for tagName, tagValue in pairs(msg) do

        -- Tags beginning with "X-" are forwarded

        if string.sub(tagName, 1, 2) == "X-" then

          debitNotice[tagName] = tagValue

          creditNotice[tagName] = tagValue

        end

      end

      -- Send Debit-Notice and Credit-Notice

      msg.reply(debitNotice)

      Send(creditNotice)

    end

  else

    msg.reply({

      Action = 'Transfer-Error',

      ['Message-Id'] = msg.Id,

      Error = 'Insufficient Balance!'

    })

  end

end)

--[[

    Mint

   ]]

--

Handlers.add('mint', "Mint", function(msg)

  assert(type(msg.Quantity) == 'string', 'Quantity is required!')

  assert(bint(0) < bint(msg.Quantity), 'Quantity must be greater than zero!')

  if not Balances[ao.id] then Balances[ao.id] = "0" end

  if msg.From == ao.id then

    -- Add tokens to the token pool, according to Quantity

    Balances[msg.From] = utils.add(Balances[msg.From], msg.Quantity)

    TotalSupply = utils.add(TotalSupply, msg.Quantity)

    msg.reply({

      Data = Colors.gray .. "Successfully minted " .. Colors.blue .. msg.Quantity .. Colors.reset

    })

  else

    msg.reply({

      Action = 'Mint-Error',

      ['Message-Id'] = msg.Id,

      Error = 'Only the Process Id can mint new ' .. Ticker .. ' tokens!'

    })

  end

end)

--[[

     Total Supply

   ]]

--

Handlers.add('totalSupply', "Total-Supply", function(msg)

  assert(msg.From ~= ao.id, 'Cannot call Total-Supply from the same process!')

  msg.reply({

    Action = 'Total-Supply',

    Data = TotalSupply,

    Ticker = Ticker

  })

end)

--[[

 Burn

]] --

Handlers.add('burn', 'Burn', function(msg)

  assert(type(msg.Quantity) == 'string', 'Quantity is required!')

  assert(bint(msg.Quantity) <= bint(Balances[msg.From]), 'Quantity must be less than or equal to the current balance!')

  Balances[msg.From] = utils.subtract(Balances[msg.From], msg.Quantity)

  TotalSupply = utils.subtract(TotalSupply, msg.Quantity)

  msg.reply({

    Data = Colors.gray .. "Successfully burned " .. Colors.blue .. msg.Quantity .. Colors.reset

  })

end)

---

# 53. aos AO Operating System  Cookbook

Document Number: 53
Source: https://cookbook_ao.arweave.net/guides/aos/index.html
Words: 396
Extraction Method: html

aos: AO Operating System aos is a powerful operating system built on top of the AO hyper-parallel computer. While AO provides the distributed compute infrastructure, aos offers a simplified interface for interacting with and developing processes in this environment.What is aos?aos enables you to:Create and interact with processes on the AO network Develop distributed applications using a simple, intuitive approach Leverage the Lua programming language for deterministic, reliable operations All you need to get started is a terminal and a code editor. aos uses Lua as its primary language - a robust, deterministic, and user-friendly programming language that's ideal for distributed applications.New to AO? If you're just getting started, we recommend completing our tutorials first. They take just 15-30 minutes and provide an excellent foundation.Getting Started with aos Start here if you're new to aos:Introduction to aos - Overview of aos capabilities and concepts Installation Guide - Step-by-step instructions for setting up aos aos Command Line Interface - Learn to use the aos CLI effectively Customizing Your Prompt - Personalize your aos development environment Load Lua Files - Learn how to load and execute Lua files in aos Building a Ping-Pong Server - Create your first interactive aos application Blueprints Blueprints in aos are templates that streamline the development of distributed applications by providing a framework for creating consistent and efficient processes across the AO network.Available Blueprints Chatroom - Template for building chatroom applications Cred Utils - Tools for managing credentials Staking - Framework for implementing staking mechanisms Token - Guide for creating and managing tokens Voting - Blueprint for setting up voting systems aos Modules aos includes several built-in modules for common operations:JSON Module - Parse and generate JSON data AO Module - Interface with the AO ecosystem Crypto Module - Perform cryptographic operations Base64 Module - Encode and decode Base64 data Pretty Module - Format data for easier reading Utils Module - Common utility functions Developer Resources More advanced topics for aos development:Editor Setup & Configuration - Configure your development environment Understanding the Inbox & Message Handlers - Learn how message handling works Troubleshooting with ao.link - Debug aos applications Frequently Asked Questions - Find answers to common questions Build a Token - Create your own token on AO Use the sidebar to browse through specific aos guides. For a more structured learning path, we recommend following the guides in the order listed above.

---

# 54. Building a Token in ao  Cookbook

Document Number: 54
Source: https://cookbook_ao.arweave.net/guides/aos/token.html
Words: 1341
Extraction Method: html

Building a Token in ao When creating tokens, we'll continue to use the Lua Language within ao to mint a token, guided by the principles outlined in the Token Specification.Two Ways to Create Tokens:1 - Use the token blueprint:.load-blueprint token Using the token blueprint will create a token with all the handlers and state already defined. This is the easiest way to create a token. You'll be able to customize those handlers and state to your after loading the blueprint.You can learn more about available blueprints here: Blueprints INFO Using the token blueprint will definitely get quickly, but you'll still want to understand how to load and test the token, so you can customize it to your needs.2 - Build from Scratch:The following guide will guide you through the process of creating a token from scratch. This is a more advanced way to create a token, but it will give you a better understanding of how tokens work.Preparations Step 1: Initializing the Token Open our preferred text editor, preferably from within the same folder you used during the previous tutorial.Create a new file named token.lua.Within token.lua, you'll begin by initializing the token's state, defining its balance, name, ticker, and more: Let's break down what we've done here:local json = require('json'): This first line of this code imports a module for later use.if not Balances then Balances = { [ao.id] = 100000000000000 } end: This second line is initializing a Balances table which is the way the Process tracks who posses the token. We initialize our token process ao.id to start with all the balance.The Next 4 Lines, if Name, if Ticker, if Denomination, and if not Logo are all optional, except for if Denomination, and are used to define the token's name, ticker, denomination, and logo respectively.INFO The code if Denomination ~= 10 then Denomination = 10 end tells us the number of the token that should be treated as a single unit.Step 2: Info and Balances Handlers  Incoming Message Handler Now lets add our first Handler to handle incoming Messages. INFO At this point, you've probably noticed that we're building all of the handlers inside the token.lua file rather than using .editor.With many handlers and processes, it's perfectly fine to create your handlers using .editor, but because we're creating a full process for initializing a token, setting up info and balances handlers, transfer handlers, and a minting handler, it's best to keep everything in one file.This also allows us to maintain consistency since each handler will be updated every time we reload the token.lua file into aos.This code means that if someone Sends a message with the Tag, Action = "Info", our token will Send back a message with all of the information defined above. Note the Target = msg.From, this tells ao we are replying to the process that sent us this message.Info & Token Balance Handlers Now we can add 2 Handlers which provide information about token Balances.The first Handler above Handlers.add('Balance' handles a process or person requesting their own balance or the balance of a Target. Then replies with a message containing the info. The second Handler Handlers.add('Balances' just replies with the entire Balances table.Step 3: Transfer Handlers Before we begin testing we will add 2 more Handlers one which allows for the transfer of tokens between processes or users.luaHandlers.add('Transfer', Handlers.utils.hasMatchingTag('Action', 'Transfer'), function(msg)

  assert(type(msg.Tags.Recipient) == 'string', 'Recipient is required!')

  assert(type(msg.Tags.Quantity) == 'string', 'Quantity is required!')

  if not Balances[msg.From] then Balances[msg.From] = 0 end

  if not Balances[msg.Tags.Recipient] then Balances[msg.Tags.Recipient] = 0 end

  local qty = tonumber(msg.Tags.Quantity)

  assert(type(qty) == 'number', 'qty must be number')

  if Balances[msg.From] >= qty then

    Balances[msg.From] = Balances[msg.From] - qty

    Balances[msg.Tags.Recipient] = Balances[msg.Tags.Recipient] + qty

    --[[

      Only Send the notifications to the Sender and Recipient

      if the Cast tag is not set on the Transfer message

    ]] --

    if not msg.Tags.Cast then

      -- Debit-Notice message template, that is sent to the Sender of the transfer

      local debitNotice = {

        Target = msg.From,

        Action = 'Debit-Notice',

        Recipient = msg.Recipient,

        Quantity = tostring(qty),

        Data = Colors.gray ..

            "You transferred " ..

            Colors.blue .. msg.Quantity .. Colors.gray .. " to " .. Colors.green .. msg.Recipient .. Colors.reset

      }

      -- Credit-Notice message template, that is sent to the Recipient of the transfer

      local creditNotice = {

        Target = msg.Recipient,

        Action = 'Credit-Notice',

        Sender = msg.From,

        Quantity = tostring(qty),

        Data = Colors.gray ..

            "You received " ..

            Colors.blue .. msg.Quantity .. Colors.gray .. " from " .. Colors.green .. msg.From .. Colors.reset

      }

      -- Add forwarded tags to the credit and debit notice messages

      for tagName, tagValue in pairs(msg) do

        -- Tags beginning with "X-" are forwarded

        if string.sub(tagName, 1, 2) == "X-" then

          debitNotice[tagName] = tagValue

          creditNotice[tagName] = tagValue

        end

      end

      -- Send Debit-Notice and Credit-Notice

      ao.send(debitNotice)

      ao.send(creditNotice)

    end

  else

    ao.send({

      Target = msg.Tags.From,

      Tags = { ["Action"] = 'Transfer-Error', ['Message-Id'] = msg.Id, ["Error"] = 'Insufficient Balance!' }

    })

  end

end) In summary, this code checks to make sure the Recipient and Quantity Tags have been provided, initializes the balances of the person sending the message and the Recipient if they dont exist and then attempts to transfer the specified quantity to the Recipient in the Balances table.If the transfer was successful a Debit-Notice is sent to the sender of the original message and a Credit-Notice is sent to the Recipient.If there was insufficient balance for the transfer it sends back a failure message The line if not msg.Tags.Cast then Means were not producing any messages to push if the Cast tag was set. This is part of the ao protocol.Step 4: Mint Handler Finally, we will add a Handler to allow the minting of new tokens.This code checks to make sure the Quantity Tag has been provided and then adds the specified quantity to the Balances table.Once you've created your token.lua file, or you've used .load-blueprint token, you're now ready to begin testing.1 - Start the aos process Make sure you've started your aos process by running aos in your terminal.If you've followed along with the guide, you'll have a token.lua file in the same directory as your aos process. From the aos prompt, load in the file.lua.load token.lua 3 - Testing the Token Now we can send Messages to our aos process ID, from the same aos prompt to see if is working. If we use ao.id as the Target we are sending a message to ourselves.This should print the Info defined in the contract. Check the latest inbox message for the response.luaInbox[#Inbox].Tags This should print the Info defined in the contract.INFO Make sure you numerically are checking the last message. To do so, run #Inbox first to see the total number of messages are in the inbox. Then, run the last message number to see the data.Example:If #Inbox returns 5, then run Inbox[5].Data to see the data.4 - Transfer Now, try to transfer a balance of tokens to another wallet or process ID.INFO If you need another process ID, you can run aos [name] in another terminal window to get a new process ID. Make sure it's not the same aos [name] as the one you're currently using.Example:If you're using aos in one terminal window, you can run aos test in another terminal window to get a new process ID.After sending, you'll receive a printed message in the terminal similar to Debit-Notice on the sender's side and Credit-Notice on the recipient's side.5 - Check the Balances Now that you've transferred some tokens, let's check the balances.luaInbox[#Inbox].Data You will see two process IDs or wallet addresses, each displaying a balance. The first should be your sending process ID, the second should be the recipient's process ID.6 - Minting Tokens Finally, attempt to mint some tokens.And check the balances again.You'll then see the balance of the process ID that minted the tokens has increased.Conclusion That concludes the "Build a Token" guide. Learning out to build custom tokens will unlock a great deal of potential for your projects; whether that be creating a new currency, a token for a game, a governance token, or anything else you can imagine.

---

# 55. AO Dev-Cli 01  Cookbook

Document Number: 55
Source: https://cookbook_ao.arweave.net/guides/dev-cli/index.html
Words: 462
Extraction Method: html

AO Dev-Cli 0.1 The AO dev-cli is a tool that is used to build ao wasm modules, the first versions of the tool only supported lua as the embedded language or c based module. With this release developers now can add any pure c or cpp module to their wasm builds. This opens the door for many different innovations from indexers to languages.Install Requirements Docker is required: https://docker.com Start a project Build a project Deploy a project Requirements You will need an arweave keyfile, you can create a local one using this command npx -y @permaweb/wallet > wallet.json Configuration To customize your build process, create a config.yml file in the root directory of your project. This file will modify your settings during the build.Configuration Options:preset: Selects default values for stack_size, initial_memory, and maximum_memory. For available presets, see Config Presets. (Default: md) stack_size: Specifies the stack size, overriding the value from the preset. Must be a multiple of 64. (Default: 32MB) initial_memory: Defines the initial memory size, overriding the preset value. Must be larger than stack_size and a multiple of 64. (Default: 48MB) maximum_memory: Sets the maximum memory size, overriding the preset value. Must be larger than stack_size and a multiple of 64. (Default: 256MB) extra_compile_args: Provides additional compilation commands for emcc. (Default: []) keep_js: By default, the generated .js file is deleted since AO Loader uses predefined versions. Set this to true if you need to retain the .js file. (Default: false) Libraries Starting with version 0.1.3, you can integrate external libraries into your project. To do this, follow these guidelines:Adding Libraries Create a libs Directory: At the root of your project, create a directory named /libs. This is where you'll place your library files.Place Your Library Files: Copy or move your compiled library files (e.g., .a, .so, .o, .dylib, etc.) into the /libs directory.NOTE Ensure that all library files are compiled using emcc to ensure compatibility with your project.IMPORTANT More details to come including an example project...Example Directory Structure Using Libraries in Your Code After adding the library files to the /libs directory, you need to link against these libraries in your project. This often involves specifying the library path and names in your build scripts or configuration files. For example:For C/C++ Projects: You can just include any header files placed in the libs folder as the libs with be automatically built into your module.For Lua Projects: Depending on how your build your libraries and if you compiled them with Lua bindings you can just require the libs in your lua files. markdown = require('markdown') IMPORTANT More details to come...Lua Build Example To create and build a Lua project, follow these steps:C Build Example To create and build a C project, follow these steps:Config Presets Here are the predefined configuration presets:

---

# 56. Guides  Cookbook

Document Number: 56
Source: https://cookbook_ao.arweave.net/guides/index.html
Words: 215
Extraction Method: html

Guides This section provides detailed guides and documentation to help you build and deploy applications on the AO ecosystem. Whether you're creating chatrooms, autonomous bots, or complex decentralized applications, you'll find step-by-step instructions here.Core Technologies Comprehensive guides for AO's main technologies:AOS: Compute on AO - Learn how to use the AO operating system for distributed computing Introduction to AOS - Get started with the AOS environment Installation Guide - Set up AOS on your system CLI Usage - Learn the command-line interface And more...AO Connect: JavaScript Library - Interact with AO using JavaScript Installation - Set up the AO Connect library Connecting to AO - Establish connections Sending Messages - Communicate with processes And more...Development Tools AO Module Builder CLI - Build WebAssembly modules for AO Installation - Install the development CLI Project Setup - Create your first project Building & Deployment - Compile and deploy modules Utilities & Storage Helpful tools and storage solutions:Using WeaveDrive - Store and manage data with WeaveDrive Using SQLite - Integrate SQLite databases with your AO projects Additional Resources Community Resources - Connect with the AO community Release Notes - Stay updated on the latest changes and features Use the sidebar to browse through specific guides. Each guide provides detailed instructions and examples to help you build on AO.

---

# 57. Troubleshooting using aolink  Cookbook

Document Number: 57
Source: https://cookbook_ao.arweave.net/guides/aos/troubleshooting.html
Words: 337
Extraction Method: html

Troubleshooting using ao.link Working with a decentralized computer and network, you need to be able to troubleshoot more than your own code. You need to be able to track messages, token balances, token transfers of processes. This is where https://ao.link becomes an essential tool in your toolbox. Analytics AOLink has a set of 4 analytic measures:Total Messages Total Users Total Processes Total Modules These analytics give you a quick view into the ao network's total processing health.Events Below, the analytics are the latest events that have appeared on the ao computer. You have a list of messages being scheduled and that have been executed. These events are any of the ao Data Protocol Types. And you can click on the Process ID or the Message ID to get details about each. Message Details  The message details give you key details about:From To Block Height Created Tags Data Result Type Data If you want to further troubleshoot and debug, you have the option to look at the result of the CU (Compute Unit) by clicking on "Compute". And further understand linked messages. Process Details  The process details provide you with information about the process it's useful to see in the tags with what module this got instantiated from. If you notice on the left you see the interaction with the process displayed on a graph. In this case, this is DevChat, and you can see all the processes that have interacted by Registering and Broadcasting Messages.You can effortless check the Info Handler, by pressing the "Fetch" button. On the bottom you see the processes balance and all messages send, with the option to break it down into Token transfers and Token balances using the tabs. Further Questions?Feel free to reach out on the community Discord of Autonomous Finance, for all questions and support regarding ao.link. https://discord.gg/4kF9HKZ4Wu Summary AOLink is an excellent tool for tracking events in the ao computer. Give it a try. Also, there is another scanner tool available on the permaweb: https://ao_marton.g8way.io/ - check it out!

---

# 58. HyperBEAM from AO Connect  Cookbook

Document Number: 58
Source: https://cookbook_ao.arweave.net/guides/migrating-to-hyperbeam/ao-connect.html
Words: 211
Extraction Method: html

HyperBEAM from AO Connect This guide explains how to interact with a process using HyperBEAM and aoconnect.Prerequisites Node.js environment @permaweb/aoconnect library The latest version of aos Wallet file (wallet.json) containing your cryptographic keys A HyperBEAM node running with the genesis_wasm profile The Process ID for a process created with genesis_wasm (this is the default in the latest version of aos).Step 1: Environment Setup Install necessary dependencies:Ensure your wallet file (wallet.json) is correctly formatted and placed in your project directory.INFO You can create a test wallet using this command: npx -y @permaweb/wallet > wallet.json Step 2: Establish Connection Create a new JavaScript file (e.g., index.js) and set up your Permaweb connection. You will need a processId of a process that you want to interact with.Step 3: Pushing a Message to a Process Use the request function to send a message to the process. In aoconnect, this is done by using the push path parameter.Full Example To run the full script, combine the snippets from Step 2 and 3 into index.js:Now, run it:bashnode index.js You should see an object logged to the console, containing the ID of the message that was sent.Conclusion Following these steps, you've successfully sent a message to a process. This is a fundamental interaction for building applications on hyperAOS.

---

# 59. Reading Dynamic State  Cookbook

Document Number: 59
Source: https://cookbook_ao.arweave.net/guides/migrating-to-hyperbeam/reading-dynamic-state.html
Words: 429
Extraction Method: html

Reading Dynamic State Beyond reading static, cached state from your process, HyperBEAM allows you to perform on-the-fly computations on that state using Lua. This guide explains how to create and use "transformation functions" to return dynamic, computed data without altering the underlying state of your process.This is a powerful pattern for creating efficient data APIs for your applications, reducing client-side logic, and minimizing data transfer.This guide assumes you are already familiar with exposing static process state.How it Works: The Lua Device The magic behind this is the lua@5.3a device, which can execute a Lua script against a message. In this pattern, we use a HyperBEAM URL (hashpath) to construct a pipeline:First, we grab the latest state of an AO process.Then, we pipe that state as the base message into the lua@5.3a device.We tell the Lua device which script to load (from an Arweave transaction) and which function to execute.The function runs, processing the base state.Finally, the result of the function is returned over HTTP.Example: Calculating Circulating Supply Let's consider a practical example: a token process where we have patched the Balances table to be readable. Rather than forcing clients to download all balance data to compute the total supply, we can do it on the HyperBEAM node.1. The Transformation Function First, create a Lua script (sum.lua) with a function that takes the state (base) and calculates the sum of balances.The transformation function receives two arguments:base: The message being processed, which in our pipeline will be the cached state data from your process.req: The incoming request object, which contains parameters and other metadata.2. Publishing the Function Next, publish your Lua script to Arweave. The arx CLI tool is recommended for this.arx will return a transaction ID for your script. Let's say it's LUA_SCRIPT_TX_ID.3. Calling the Function With the process ID (YOUR_PROCESS_ID) and the script transaction ID (LUA_SCRIPT_TX_ID), you can construct a URL to call your function:HyperBEAMGET /<YOUR_PROCESS_ID>~process@1.0/now/~lua@5.3a&module={LUA_SCRIPT_TX_ID}/sum/serialize~json@1.0 This URL breaks down as follows:/{YOUR_PROCESS_ID}~process@1.0: Targets the AO process and its state./now: Gets the most current state./~lua@5.3a&module={LUA_SCRIPT_TX_ID}: This is the key part. It tells HyperBEAM to take the output of the previous step (the process state) and process it with the lua@5.3a device, loading your script from the module transaction./sum: Calls the sum function within your Lua script./serialize~json@1.0: Takes the table returned by your function and serializes it into a JSON object.4. Integrating into an Application Here's how you could fetch this dynamic data in a JavaScript application:This approach significantly improves performance by offloading computation from the client to the HyperBEAM node and reducing the amount of data sent over the network.

---

# 60. Getting started with SQLite  Cookbook

Document Number: 60
Source: https://cookbook_ao.arweave.net/guides/snacks/sqlite.html
Words: 130
Extraction Method: html

Skip to content  Getting started with SQLite SQLite is a relational database engine. In this guide, we will show how you can spawn a process with SQLite and work with data using a relational database.Setup NOTE: make sure you have aos installed, if not checkout Getting Started spawn a new process mydb with a --sqlite flag, this instructs ao to use the latest sqlite module.Install AO Package Manager installing apm, the ao package manager we can add helper modules to make it easier to work with sqlite.lua.load-blueprint apm Install dbAdmin package DbAdmin is a module that connects to a sqlite database and provides functions to work with sqlite.https://apm_betteridea.g8way.io/pkg?id=@rakis/DbAdmin luaapm.install('@rakis/dbAdmin') Create sqlite Database Create Table Create a table called Comments Insert data List data Congrats!You are using sqlite on AO 🎉

---

# 61. Using WeaveDrive  Cookbook

Document Number: 61
Source: https://cookbook_ao.arweave.net/guides/snacks/weavedrive.html
Words: 253
Extraction Method: html

Using WeaveDrive WeaveDrive has been released on AO legacynet, which is great! But how to use it with your process? This post aims to provide a step by step guide on how to use WeaveDrive in your AOS process.The current availability time is called Assignments and this type puts WeaveDrive in a mode that allows you to define an Attestor wallet address when you create your AOS process. This will enable the process to load data from dataItems that have a Attestation created by this wallet.Prep Tutorial In order, to setup the tutorial for success we need to upload some data and upload an attestation. It will take a few minutes to get mined into a block on arweave.Install arx Create a wallet Create some data You should get a result like:Create Attestation It is important to copy the id of the uploaded dataItem, in the above case 9TIPJD2a4-IleOQJzRwPnDHO5DA891MWAyIdJJ1SiSk as your Message Value.👏 Awesome! That will take a few minutes to get mined on arweave, once it is mined then we will be able to read the data.html dataItem using WeaveDrive Enable WeaveDrive in a process Lets create a new AOS process with WeaveDrive enabled and the wallet we created above as an Attestor.NOTE: it is important to use the same wallet address that was used to sign the attestation data-item.NOTE: It does take a few minutes for the data to get 20 plus confirmations which is the threshold for data existing on arweave. You may want to go grab a coffee. ☕

---

# 62. Community Resources  Cookbook

Document Number: 62
Source: https://cookbook_ao.arweave.net/references/community.html
Words: 177
Extraction Method: html

Skip to content  Community Resources This page provides a comprehensive list of community resources, tools, guides, and links for the AO ecosystem.Core Resources Autonomous Finance Autonomous Finance is a dedicated research and technology entity, focusing on the intricacies of financial infrastructure within the ao network.BetterIdea Build faster, smarter, and more efficiently with BetterIDEa, the ultimate native web IDE for AO development 0rbit 0rbit provides any data from the web to an ao process by utilizing the power of ao, and 0rbit nodes. The user sends a message to the 0rbit ao, 0rbit nodes fetches the data and the user process receives the data.ArweaveHub A community platform for the Arweave ecosystem featuring events, developer resources, and discovery tools.AR.IO The first permanent cloud network built on Arweave, providing infrastructure for the permaweb with no 404s, no lost dependencies, and reliable access to applications and data through gateways, domains, and deployment tools.Developer Tools AO Package Manager Contributing Not seeing an AO Community Member or resource? Create an issue or submit a pull request to add it to this page: https://github.com/permaweb/ao-cookbook

---

# 63. Editor setup  Cookbook

Document Number: 63
Source: https://cookbook_ao.arweave.net/references/editor-setup.html
Words: 218
Extraction Method: html

Editor setup Remembering all the built in ao functions and utilities can sometimes be hard. To enhance your developer experience, it is recommended to install the Lua Language Server extension into your favorite text editor and add the ao addon. It supports all built in aos modules and globals.VS Code Install the sumneko.lua extension:Search for "Lua" by sumneko in the extension marketplace Download and install the extension Open the VS Code command palette with Shift + Command + P (Mac) / Ctrl + Shift + P (Windows/Linux) and run the following command:In the Addon Manager, search for "ao", it should be the first result. Click "Enable" and enjoy autocomplete!Other editors Verify that your editor supports the language server protocol Install Lua Language Server by following the instructions at luals.github.io Install the "ao" addon to the language server BetterIDEa BetterIDEa is a custom web based IDE for developing on ao.It offers a built in Lua language server with ao definitions, so you don't need to install anything. Just open the IDE and start coding!Features include:Code completion Cell based notebook ui for rapid development Easy process management Markdown and Latex cell support Share projects with anyone through ao processes Tight integration with ao package manager Read detailed information about the various features and integrations of the IDE in the documentation.

---

# 64. Meet Lua  Cookbook

Document Number: 64
Source: https://cookbook_ao.arweave.net/references/lua.html
Words: 293
Extraction Method: html

Meet Lua Understanding Lua Background: Lua is a lightweight, high-level, multi-paradigm programming language designed primarily for embedded systems and clients. It's known for its efficiency, simplicity, and flexibility.Key Features: Lua offers powerful data description constructs, dynamic typing, efficient memory management, and good support for object-oriented programming.Setting Up Installation: Visit Lua's official website to download and install Lua.Environment: You can use a simple text editor and command line, or an IDE like ZeroBrane Studio or Eclipse with a Lua plugin.Basic Syntax and Concepts (in aos) Hello World:lua"Hello, World!" Variables and Types: Lua is dynamically typed. Basic types include nil, boolean, number, string, function, userdata, thread, and table.Control Structures: Includes if, while, repeat...until, and for.Functions: First-class citizens in Lua, supporting closures and higher-order functions.Tables: The only data structuring mechanism in Lua, which can be used to represent arrays, sets, records, etc.Hands-On Practice Experiment with Lua's Interactive Mode: Run aos in your terminal and start experimenting with Lua commands.Write Simple Scripts: Create .lua files and run them using the Lua interpreter. Use .load file.lua feature to upload lua code on your aos process.Resources Official Documentation: Lua 5.3 Reference Manual Online Tutorials: Websites like Learn Lua are great for interactive learning.Books: "Programming in Lua" (first edition available online) is a comprehensive resource.Community: Join forums or communities like Lua Users for support and discussions.Best Practices Keep It Simple: Lua is designed to be simple and flexible. Embrace this philosophy in your code.Performance: Learn about Lua's garbage collection and efficient use of tables.Integration: Consider how Lua can be embedded into other applications, particularly C/C++ projects.Conclusion Lua is a powerful language, especially in the context of embedded systems and game development. Its simplicity and efficiency make it a great choice for specific use cases. Enjoy your journey into Lua programming!

---

# 65. Accessing Data from Arweave with ao  Cookbook

Document Number: 65
Source: https://cookbook_ao.arweave.net/references/data.html
Words: 1254
Extraction Method: html

Accessing Data from Arweave with ao There may be times in your ao development workflow that you want to access data from Arweave. With ao, your process can send an assignment instructing the network to provide that data to your Process.Defining Acceptable Transactions (Required First Step) Before you can assign any Arweave transaction to your process, you must first define which transactions your process will accept using ao.addAssignable. This function creates conditions that determine which Arweave transactions your process will accept.Warning: If you attempt to assign a transaction without first defining a matching assignable pattern, that transaction will be permanently blacklisted and can never be assigned to your process, even if you later add a matching assignable.You can remove assignables with ao.removeAssignable("<name>").The condition functions use similar pattern matching techniques as found in the Handlers documentation. For complete details on the ao.addAssignable function, including parameter descriptions and additional examples, see the ao Module Reference.Assignment Methods After defining acceptable transactions and setting up your listener (if needed), you can request Arweave data in one of two ways:Using Assign The primary method to request data from Arweave:Using Send with Assignments Alternatively, you can use the Send function with an Assignments parameter:Working with Assigned Data You can process assigned data using either Receive or Handlers:Using Receive Directly You can also match specific transactions or combine conditions:Note: When using .load, the script pauses at Receive until data arrives. When running commands separately in the shell, each command executes independently.Using Handlers For persistent processing, set up a handler:Handlers are ideal for:Processing multiple assignments over time Automated processing without manual intervention Building services that other processes can interact with For more details, see the Messaging Patterns and Handlers documentation.Complete Example Workflow Here's a complete example that demonstrates the entire process of accessing data from an Arweave transaction:This pattern creates a synchronous flow where your process:Defines acceptable transactions Requests the data Captures the data using Receive Processes the data Practical Examples Here are two practical examples showing different approaches to working with Arweave data in your ao process:Example 1: Caching Arweave Data This example demonstrates how to load and cache data from Arweave, then use it in subsequent operations:lua-- Initialize state

local Number = 0

-- Step 1: Define which transactions your process will accept

print("Step 1: Defining acceptable transactions")

ao.addAssignable("addNumber", function (msg)

    return msg.Tags["Action"] == "Number"

end)

-- Step 2: Request and cache the initial number from Arweave

-- This uses a self-executing function to fetch and cache the value only once

NumberFromArweave = NumberFromArweave or (function()

    print("Step 2: Requesting initial number from Arweave")

    Assign({

        Processes = { ao.id },

        Message = 'DivdWHaNj8mJhQQCdatt52rt4QvceBR_iyX58aZctZQ'

    })

    return tonumber(Receive({ Action = "Number"}).Data)

end)()

-- Step 3: Set up handler for future number updates

-- This handler will add new numbers to our cached Arweave number

Handlers.add("Number", function (msg)

    print("Received message with Data = " .. msg.Data)

    print("Old Number: " .. Number)

    Number = NumberFromArweave + tonumber(msg.Data)

    print("New Number: " .. Number)

end) This example shows how to:Cache Arweave data using a self-executing function Use the cached data in subsequent message handling Combine Arweave data with new incoming data Example 2: Dynamic Transaction Processing This example shows how to process arbitrary Arweave transactions and maintain state between requests:lua-- Table to store pending requests (maps transaction ID to original sender)

local PendingRequests = {}

-- Step 1: Define which transactions your process will accept

print("Step 1: Defining acceptable transactions")

ao.addAssignable("processArweaveNumber", function (msg)

    return msg.Tags["Action"] == "Number"

end)

-- Step 2: Set up handler for initiating the processing

Handlers.add(

    "ProcessArweaveNumber",

    function (msg)

        if not msg.Tags["ArweaveTx"] then

            print("Error: No ArweaveTx tag provided")

            return

        end

        local txId = msg.Tags["ArweaveTx"]

        print("Assigning Arweave transaction: " .. txId)

        -- Store the original sender associated with this transaction ID

        PendingRequests[txId] = msg.From

        -- Assign the transaction to this process

        Assign({

            Processes = { ao.id },

            Message = txId

        })

        print("Assignment requested; waiting for data...")

    end

)

-- Step 3: Set up handler for processing the assigned message

Handlers.add(

    "Number",

    function (msg)

        local txId = msg.Id  -- The ID of the assigned message

        local originalSender = PendingRequests[txId]

        if not originalSender then

            print("Error: No pending request found for transaction " .. txId)

            return

        end

        local data = msg.Data

        if not data or not tonumber(data) then

            print("Error: Invalid number data in assigned message")

            return

        end

        local number = tonumber(data)

        local result = number + 1

        print(string.format("Processing: %d + 1 = %d", number, result))

        -- Send the result back to the original sender

        Send({

            Target = originalSender,

            Data = tostring(result)

        })

        -- Clean up the pending request

        PendingRequests[txId] = nil

    end

) To use this example:This example demonstrates:Processing arbitrary Arweave transactions Maintaining state between requests using a pending requests table Sending results back to the original requester Error handling and request cleanup WARNING When using Assign to bridge Arweave data into AO, you must ensure that:The Arweave transaction you're assigning matches one of your defined assignables You have a corresponding handler or receiver set up to process that transaction type The handler's pattern matching matches the assigned transaction's tags/properties For example, if you're assigning a transaction with Action = "Number", you need:An assignable that accepts msg.Tags["Action"] == "Number" Either a Receive function or a handler that matches the same pattern Both the assignable and handler must use consistent pattern matching Important Limitations There are critical limitations to be aware of when working with assignables:Matching is Required: Transactions must match at least one of your defined assignable patterns to be accepted.Blacklisting is Permanent: If you attempt to assign a transaction before defining an appropriate assignable, it will be permanently blacklisted. Even if you later add a matching assignable, that transaction will never be accepted.One-time Assignment: Each Arweave transaction can only be assigned once to a given process. Subsequent assignments of the same transaction will be ignored.Proper Sequence for Assigning Arweave Transactions For successful assignment of Arweave transactions, follow these steps:Define assignables to specify which Arweave transactions your process will accept Wait for any transaction confirmations (by default, 20 confirmations are required) Set up handlers or listeners with Receive or Handlers.add to process the data Assign the Arweave transaction to your process (see Assignment Methods) The order of steps 3 and 4 can be interchanged based on your needs:When using Receive in a script loaded with .load, ensure Assign is placed before Receive to prevent the process from hanging, as Receive is blocking.When using handlers or running commands separately in the shell, the order doesn't matter as handlers will catch messages whenever they arrive Why Access Data from Arweave?There are several practical reasons to access Arweave data from your ao process:Efficient Handling of Large Data: For larger content, directly accessing Arweave is more efficient:Reference large media files (images, videos, documents) without storing them in your process Work with datasets too large to fit in process memory Maintain a lightweight process that can access substantial external resources External Data for Decision-Making: Your process may need data stored on Arweave to make informed decisions. For example:Reading token price data stored by an oracle Accessing verified identity information Retrieving voting records or governance data Dynamic Loading of Features: Rather than including all functionality in your initial process code:Load modules or plugins from Arweave as needed Update configuration without redeploying your entire process Implement upgradable components with new versions stored on Arweave This approach allows you to create more sophisticated applications that leverage Arweave's permanent storage while maintaining efficient process execution in the ao environment.When another process Assigns a transaction to this process, you can also use handlers to process the data asynchronously.

---

# 66. Messaging Patterns in ao  Cookbook

Document Number: 66
Source: https://cookbook_ao.arweave.net/references/messaging.html
Words: 660
Extraction Method: html

Messaging Patterns in ao This reference guide explains the messaging patterns available in ao and when to use each one.Quick Reference: Choosing the Right Pattern If you need to...Process Flow Key function(s) Send a message without waiting for a response A → B ao.send Send a message and wait for a response A → B → A ao.send().receive() Process messages and respond to the sender B → A Handlers.add + msg.reply Create a chain of processing services A → B → C → A msg.forward + ao.send().receive() Wait for any matching message regardless of sender Any → A Receive (capital R) Create a standard automated response B → A Handlers.utils.reply Sending Messages ao.send: Asynchronous Message Sending Non-blocking direct A → B messaging that returns immediately after sending.Use for fire-and-forget notifications or starting async conversations Returns a promise-like object that can be chained with .receive() if needed Good for parallel processing since it doesn't block execution Basic Send Example:msg.reply: Asynchronous Response Sending Non-blocking B → A response with automatic reference tracking. Used within handlers to respond to incoming messages.Automatically links response to original message via X-Reference Enables asynchronous request-response patterns Automatically sets Target to the original sender or Reply-To address if specified Handler Reply Example:msg.forward: Message Forwarding Non-blocking multi-process routing for A → B → C → A patterns. Creates a sanitized copy of the original message.Takes a target and a partial message to overwrite forwarded message fields Preserves Reply-To and X-Reference properties for complete message tracking Sets X-Origin to original sender, enabling final service to reply directly to originator Multi-Process Pipeline Example:lua-- In client process

local middlewareProcessId = "process-123"

local finalProcessId = "process-456"

-- Send to middleware and wait for response from final service

local response = ao.send({

  Target = middlewareProcessId,

  Action = "Transform",

  Data = "raw-data"

}).receive(finalProcessId)  -- Explicitly wait for response from final service

-- In middleware service

Handlers.add("transform-middleware",

  { Action = "Transform" },

  function(msg)

    local finalProcessId = "process-456"

    msg.forward(finalProcessId, {

      Data = msg.Data .. " (pre-processed)",

      Action = "Transform-Processed"

    })

  end

)

-- In final service

Handlers.add("final-processor",

  { Action = "Transform-Processed" },

  function(msg)

    -- No need to know the client ID - it's stored in X-Origin

    msg.forward(msg['X-Origin'], {

      Data = msg.Data .. " (final processing complete)",

      Action = "Transform-Complete"

    })

  end

) Handlers.utils.reply: Simple Reply Handler Creation Creates a handler function that automatically replies with a fixed response. A wrapper around msg.reply for common use cases.Simple String Response Example:Message Table Response Example:Receiving Messages Receive (Capital R): Blocking Pattern Matcher Blocks execution until any matching message arrives from any sender. Under the hood, this is implemented using Handlers.once, making it a one-time pattern matcher that automatically removes itself after execution.Waits for any message matching the pattern, regardless of origin Use for synchronous message processing flows or event listening Automatically removes the handler after first match (using Handlers.once internally) Message Pattern Matching Example:ao.send().receive (Lowercase r): Blocking Reference Matcher Blocks execution until a specific reply arrives, enabling A → B → A and A → B → C → A request-response cycles.Only matches messages linked by X-Reference Can specify a target process ID to indicate which process will reply Implicitly waits for the proper response based on message reference chains For A → B → A flows, process B uses msg.reply For A → B → C → A flows, processes B and C use msg.forward Basic Request-Response Example:Message Properties The following properties track message chains and ensure proper routing:Reference: Unique identifier automatically assigned to each message.Reply-To: Specifies the destination for responses.X-: Any property starting with X- denotes a 'forwarded' tag and is automatically managed by the system. X-Reference: Maintains the conversation chain across replies and forwards.X-Origin: Tracks the conversation originator.The system automatically manages these properties when using msg.reply and msg.forward. Check out the source code to see exactly how these properties are managed.Blocking vs. Non-Blocking Functions either pause your code or let it continue running:Non-blocking (ao.send, msg.reply, msg.forward): Send and continue execution Blocking (Receive, .receive()): Pause until response arrives

---

# 67. ao Token and Subledger Specification  Cookbook

Document Number: 67
Source: https://cookbook_ao.arweave.net/references/token.html
Words: 1111
Extraction Method: html

ao Token and Subledger Specification Status: DRAFT-1 Targeting Network: ao.TN.1 This specification describes the necessary message handlers and functionality required for a standard ao token process. Implementations of this standard typically offer users the ability to control a transferrable asset, whose scarcity is maintained by the process.Each compliant process will likely implement a ledger of balances in order to encode ownership of the asset that the process represents. Compliant processes have a set of methods that allow for the modification of this ledger, typically with safe-guards to ensure the scarcity of ownership of the token represented by the process.Additionally, this specification describes a 'subledger' process type which, when implemented, offers the ability to split move a number of the tokens from the parent into a child process that implements the same token interface specification. If the From-Module of the subledger process is trusted by the participants, these subledgers can be used to transact in the 'source' token, without directly exchanging messages with it. This allows participants to use the tokens from a process, even if that process is congested. Optionally, if the participants trust the Module a subledger process is running, they are able to treat balances across these processes as fungible. The result of this is that an arbitrary numbers of parallel processes -- and thus, transactions -- can be processed by a single token at any one time.Token Processes A specification-compliant token process responds to a number of different forms of messages, with each form specified in an Action tag. The full set of Action messages that the token must support are as follows:Name Description Read-Only Balance get the balance of an identifier ✔️ Balances get a list of all ledger/account balances ✔️ Transfer send 1 or more units from the callers balance to one or move targets with the option to notify targets ❌ Mint if the ledger process is the root and you would like to increase token supply ❌ In the remainder of this section the tags necessary to spawn a compliant token process, along with the form of each of the Action messages and their results is described.Spawning Parameters Every compliant token process must carry the following immutable parameters upon its spawning message:Tag Description Optional?Name The title of the token, as it should be displayed to users.✔️ Ticker A suggested shortened name for the token, such that it can be referenced quickly.✔️ Logo An image that applications may desire to show next to the token, in order to make it quickly visually identifiable.✔️ Denomination The number of the token that should be treated as a single unit when quantities and balances are displayed to users.❌ Messaging Protocol Balance(Target?: string) Returns the balance of a target, if a target is not supplied then the balance of the sender of the message must be returned.Example Action message:Example response message:Balances() Returns the balance of all participants in the token.Example response message:Transfer(Target, Quantity) If the sender has a sufficient balance, send the Quantity to the Target, issuing a Credit-Notice to the recipient and a Debit-Notice to the sender. The Credit- and Debit-Notice should forward any and all tags from the original Transfer message with the X- prefix. If the sender has an insufficient balance, fail and notify the sender.If a successful transfer occurs a notification message should be sent if Cast is not set.Recipients will infer from the From-Process tag of the message which tokens they have received.Get-Info() Mint() [optional] Implementing a Mint action gives the process a way of allowing valid participants to create new tokens.Subledger Processes In order to function appropriately, subledgers must implement the full messaging protocol of token contracts (excluding the Mint action). Subledgers must also implement additional features and spawn parameters for their processes. These modifications are described in the following section.Spawning Parameters Every compliant subledger process must carry the following immutable parameters upon its spawning message:Tag Description Optional?Source-Token The ID of the top-most process that this subledger represents.❌ Parent-Token The ID of the parent process that this subledger is attached to.❌ Credit-Notice Handler Upon receipt of a Credit-Notice message, a compliant subledger process must check if the process in question is the Parent-Token. If it is, the subledger must increase the balance of the Sender by the specified quantity.Transfer(Target, Quantity) In addition to the normal tags that are passed in the Credit-Notice message to the recipient of tokens, a compliant subledger process must also provide both of the Source-Token and Parent-Token values. This allows the recipient of the Transfer message -- if they trust the Module of the subledger process -- to credit a receipt that is analogous (fungible with) deposits from the Source-Token.The modified Credit-Notice should be structured as follows:Withdraw(Target?, Quantity) All subledgers must allow balance holders to withdraw their tokens to the parent ledger. Upon receipt of an Action: Withdraw message, the subledger must send an Action message to its Parent-Ledger, transferring the requested tokens to the caller's address, while debiting their account locally. This transfer will result in a Credit-Notice from the Parent-Ledger for the caller.Token Example NOTE: When implementing a token it is important to remember that all Tags on a message MUST be "string"s. Using the tostring function you can convert simple types to strings.luaif not balances then

  balances = { [ao.id] = 100000000000000 }

end

if name ~= "Fun Coin" then

  name = "Fun Coin"

end

if ticker ~= "Fun" then

  ticker = "fun"

end

if denomination ~= 6 then

  denomination = 6

end

-- handlers that handler incoming msg

Handlers.add(

  "Transfer",

  Handlers.utils.hasMatchingTag("Action", "Transfer"),

  function (msg)

    assert(type(msg.Tags.Recipient) == 'string', 'Recipient is required!')

    assert(type(msg.Tags.Quantity) == 'string', 'Quantity is required!')

    if not balances[msg.From] then

      balances[msg.From] = 0

    end

    if not balances[msg.Tags.Recipient] then

      balances[msg.Tags.Recipient] = 0

    end

    local qty = tonumber(msg.Tags.Quantity)

    assert(type(qty) == 'number', 'qty must be number')

    -- handlers.utils.reply("Transferring qty")(msg)

    if balances[msg.From] >= qty then

      balances[msg.From] = balances[msg.From] - qty

      balances[msg.Tags.Recipient] = balances[msg.Tags.Recipient] + qty

      ao.send({

        Target = msg.From,

        Tags = {

          ["Action"] = "Debit-Notice",

          ["Quantity"] = tostring(qty)

        }

      })

      ao.send({

        Target = msg.Tags.Recipient,

        Tags = {

          ["Action"] = "Credit-Notice",

          ["Quantity"] = tostring(qty)

        }

      })

      -- if msg.Tags.Cast and msg.Tags.Cast == "true" then

      --   return

      -- end

    end

  end

)

Handlers.add(

  "Balance",

  Handlers.utils.hasMatchingTag("Action", "Balance"),

  function (msg)

    assert(type(msg.Tags.Target) == "string", "Target Tag is required!")

    local bal = "0"

    if balances[msg.Tags.Target] then

      bal = tostring(balances[msg.Tags.Target])

    end

    ao.send({

      Target = msg.From,

      Tags = {

        ["Balance"] = bal,

        ["Ticker"] = ticker or ""

      }

    })

  end

)

local json = require("json")

Handlers.add(

  "Balances",

  Handlers.utils.hasMatchingTag("Action", "Balances"),

  function (msg)

    ao.send({

      Target = msg.From,

      Data = json.encode(balances)

    })

  end

)

Handlers.add(

  "Info",

  Handlers.utils.hasMatchingTag("Action", "Info"),

  function (msg)

    ao.send({

      Target = msg.From,

      Tags = {

        ["Name"] = name,

        ["Ticker"] = ticker,

        ["Denomination"] = tostring(denomination)

      }

    })

  end

)

---

# 68. Meet Web Assembly  Cookbook

Document Number: 68
Source: https://cookbook_ao.arweave.net/references/wasm.html
Words: 189
Extraction Method: html

Skip to content  Meet Web Assembly WebAssembly (often abbreviated as Wasm) is a modern binary instruction format providing a portable compilation target for high-level languages like C, C++, and Rust. It enables deployment on the web for client and server applications, offering a high level of performance and efficiency. WebAssembly is designed to maintain the security and sandboxing features of web browsers, making it a suitable choice for web-based applications. It's a key technology for web developers, allowing them to write code in multiple languages and compile it into bytecode that runs in the browser at near-native speed.The significance of WebAssembly lies in its ability to bridge the gap between web and native applications. It allows complex applications and games, previously limited to desktop environments, to run in the browser with comparable performance. This opens up new possibilities for web development, including the creation of high-performance web apps, games, and even the porting of existing desktop applications to the web. WebAssembly operates alongside JavaScript, complementing it by enabling performance-critical components to be written in languages better suited for such tasks, thereby enhancing the capabilities and performance of web applications.

---

# 69. Crafting a Token  Cookbook

Document Number: 69
Source: https://cookbook_ao.arweave.net/tutorials/begin/token.html
Words: 580
Extraction Method: html

Crafting a Token INFO Diving deeper into the ao, you're now ready to create your own token, a symbol of value and exchange within this decentralized medium. If you've found yourself wanting to learn how to create a token, but haven't visited the Messaging and Build a Chatroom lessons, be sure to do so as this page is part of a multi-part interactive tutorial.When creating tokens, we'll continue to use the Lua Language within ao to mint a token, guided by the principles outlined in the Token Specification.Video Tutorial  Continuing Down the Rabbit Hole In our last tutorial, Build a Chatroom, we learned how to create a chatroom within ao, invited both Morpheus and Trinity to the chatroom we created, and then Trinity has now asked for us to create a token for her as a way of proving ourselves worthy of continuing down the rabbit hole.Let us begin.The Two Paths To Building a Token There are two paths to take when building a token:The Blueprint: This is a predesigned template that helps you quickly build a token in ao. It is a great way to get started and can be customized to fit your needs.Check here to learn more about the Token Blueprint.The Manual Method: This is a step-by-step guide to building a token in ao from scratch. This path is for those who want to understand the inner workings of a token and how to build one from the ground up.Check here to review the full Build a Token guide.The Blueprint Method For this tutorial, we'll be using the Token Blueprint to create a token for Trinity. This is a predesigned template that helps you quickly build a token in ao.How To Use The Token Blueprint Make sure we're in the same directory as before during the previous steps in the tutorial.Open the Terminal.Start your aos process.Type in .load-blueprint token This will load the required handlers for the tutorials token within ao. It's important to note that the token blueprint isn't specific to this tutorial and can be used as a foundation for any token you wish to create.Verify the Blueprint is Loaded Type in Handlers.list to see the newly loaded handlers.You should see a new list of handlers that have been loaded into your aos process. If you've been following along the with the previous steps in the tutorial, you should also see the handlers for your chatroom, as well.Example: Testing the Token Now that the token blueprint is loaded, we can test the token by sending a message to ourselves using the Action = "Info" tag.This will print the token information to the console. It should show your process ID with the total balance of tokens available.Sending Tokens to Trinity Now that we've tested the token and it's working as expected, we can send some tokens to Trinity. We'll send 1000 tokens to Trinity using the Action = "Transfer" tag.When Trinity receives the tokens, she'll respond to the transfer with a message to confirm that she's received the tokens.Her response will look something like this:Trinity: "Token received. Interesting. I wasn't sure you'd make it this far. I'm impressed, but we are not done yet. I want you to use this token to tokengate the chatroom. Do that, and then I will believe you could be the one." You've completed the process of creating a token and sending it to Trinity. You're now ready to move on to the next step in the tutorial. Tokengating the Chatroom.

---

# 70. Preparations  Cookbook

Document Number: 70
Source: https://cookbook_ao.arweave.net/tutorials/begin/preparations.html
Words: 505
Extraction Method: html

Preparations INFO The Awakening Begins:You've always known there's more to this world, just outside of your reach. You've been searching for it, not even knowing what it was you were looking for. It... is ao.We begin our journey by installing the aos client and starting a new process. This will allow us to interact with the ao computer and complete the rest of the tutorial.Video Tutorial  System requirements The local client of aos is very simple to install. Just make sure you have:NodeJS version 20+. (If you haven't yet installed it, check out this page to find instructions for your OS).A code editor of your choice.INFO Though it's not required, we do recommend installing the ao addon into your text editor of choice to optimize your experience with aos.Installing aos Once you have NodeJS on your machine, all you need to do is install aos and run it:After installation, we can simply run the command itself to start a new aos process!shaos Welcome to the rabbit hole The utility you just started is a local client, which is ready to relay messages for you to your new process inside the ao computer.After it connects, you should see the following:sh          _____                   _______                   _____

         /\    \                 /::\    \                 /\    \

        /::\    \               /::::\    \               /::\    \

       /::::\    \             /::::::\    \             /::::\    \

      /::::::\    \           /::::::::\    \           /::::::\    \

     /:::/\:::\    \         /:::/~~\:::\    \         /:::/\:::\    \

    /:::/__\:::\    \       /:::/    \:::\    \       /:::/__\:::\    \

   /::::\   \:::\    \     /:::/    / \:::\    \      \:::\   \:::\    \

  /::::::\   \:::\    \   /:::/____/   \:::\____\   ___\:::\   \:::\    \

 /:::/\:::\   \:::\    \ |:::|    |     |:::|    | /\   \:::\   \:::\    \

/:::/  \:::\   \:::\____\|:::|____|     |:::|    |/::\   \:::\   \:::\____\

\::/    \:::\  /:::/    / \:::\    \   /:::/    / \:::\   \:::\   \::/    /

 \/____/ \:::\/:::/    /   \:::\    \ /:::/    /   \:::\   \:::\   \/____/

          \::::::/    /     \:::\    /:::/    /     \:::\   \:::\    \

           \::::/    /       \:::\__/:::/    /       \:::\   \:::\____\

           /:::/    /         \::::::::/    /         \:::\  /:::/    /

          /:::/    /           \::::::/    /           \:::\/:::/    /

         /:::/    /             \::::/    /             \::::::/    /

        /:::/    /               \::/____/               \::::/    /

        \::/    /                 ~~                      \::/    /

         \/____/                                           \/____/

Welcome to AOS: Your operating system for AO, the decentralized open

access supercomputer.

Type ".load-blueprint chat" to join the community chat and ask questions!

AOS Client Version: 1.12.1. 2024

Type "Ctrl-C" twice to exit

Your AOS process:  QFt5SR6UwJSCnmgnROq62-W8KGY9z96k1oExgn4uAzk

default@aos-0.2.2[Inbox:1]> Let's walk through the initial printout after running aos: After running aos in your terminal, you should see:An ASCII art image of AOS.A Welcome Message The version of aos you are running.An instructional exit message.Your process ID.INFO If your OS version is different than the latest version, a message asking if you'd like to update the version will appear. If so, simply exit the process by pressing "Ctrl+C" twice, run npm i -g https://get_ao.g8way.io to update, and then run aos again.Welcome to your new home in the ao computer! The prompt you are now looking at is your own personal server in this decentralized machine.Now, let's journey further down the rabbit hole by exploring one of the two core concept type of ao: messaging.

---

# 71. Automated Responses  Cookbook

Document Number: 71
Source: https://cookbook_ao.arweave.net/tutorials/bots-and-games/attacking.html
Words: 438
Extraction Method: html

Automated Responses Following our last guide, our creation has progressed from a simple bot to a sophisticated autonomous agent. Now, let's further enhance its capabilities by adding a counterattack feature, allowing it to instantly retaliate against an opponent's attack, potentially catching them off-guard before they can retreat to safety.Writing the code Add the following handler to your bot.lua file and you're set:Whenever your player is under attack you receive a message with the Action Hit. This setup ensures your agent can make a swift counter attack, given it has sufficient energy.You can refer to the latest code for bot.lua in the dropdown below:Updated bot.lua file luaLatestGameState = LatestGameState or nil

function inRange(x1, y1, x2, y2, range)

  return math.abs(x1 - x2) <= range and math.abs(y1 - y2) <= range

end

function decideNextAction()

  local player = LatestGameState.Players[ao.id]

  local targetInRange = false

  for target, state in pairs(LatestGameState.Players) do

    if target ~= ao.id and inRange(player.x, player.y, state.x, state.y, 1) then

        targetInRange = true

        break

    end

  end

  if player.energy > 5 and targetInRange then

    print("Player in range. Attacking.")

    ao.send({Target = Game, Action = "PlayerAttack", Player = ao.id, AttackEnergy = tostring(player.energy)})

  else

    print("No player in range or insufficient energy. Moving randomly.")

    local directionMap = {"Up", "Down", "Left", "Right", "UpRight", "UpLeft", "DownRight", "DownLeft"}

    local randomIndex = math.random(#directionMap)

    ao.send({Target = Game, Action = "PlayerMove", Player = ao.id, Direction = directionMap[randomIndex]})

  end

end

Handlers.add(

  "HandleAnnouncements",

  { Action =  "Announcement" },

  function (msg)

    ao.send({Target = Game, Action = "GetGameState"})

    print(msg.Event .. ": " .. msg.Data)

  end

)

Handlers.add(

  "UpdateGameState",

  { Action =  "GameState" },

  function (msg)

    local json = require("json")

    LatestGameState = json.decode(msg.Data)

    ao.send({Target = ao.id, Action = "UpdatedGameState"})

  end

)

Handlers.add(

  "decideNextAction",

  { Action =  "UpdatedGameState" },

  function ()

    if LatestGameState.GameMode ~= "Playing" then

      return

    end

    print("Deciding next action.")

    decideNextAction()

  end

)

Handlers.add(

  "ReturnAttack",

  { Action =  "Hit" },

  function (msg)

      local playerEnergy = LatestGameState.Players[ao.id].energy

      if playerEnergy == undefined then

        print("Unable to read energy.")

        ao.send({Target = Game, Action = "Attack-Failed", Reason = "Unable to read energy."})

      elseif playerEnergy == 0 then

        print("Player has insufficient energy.")

        ao.send({Target = Game, Action = "Attack-Failed", Reason = "Player has no energy."})

      else

        print("Returning attack.")

        ao.send({Target = Game, Action = "PlayerAttack", Player = ao.id, AttackEnergy = tostring(playerEnergy)})

      end

      InAction = false

      ao.send({Target = ao.id, Action = "Tick"})

  end

) To activate and test the counter attack feature, load the bot file in your aos player terminal:lua.load bot.lua Watch your terminal for the autonomous agent's reactions, now with the added ability to retaliate instantly. This feature showcases the agent's evolving strategic depth and autonomy. In the upcoming section, we'll consolidate all the knowledge we've gathered so far and add some features for optimization.

---

# 72. Lets Play A Game  Cookbook

Document Number: 72
Source: https://cookbook_ao.arweave.net/tutorials/bots-and-games/ao-effect.html
Words: 646
Extraction Method: html

Let's Play A Game!You've been powering through tutorials like a champ! Now, let's take a refreshing break and dive into something exciting. How about a game that adds a dash of fun to your learning journey? What's the game?ao-effect is a game where you can compete with friends or other players globally, in real-time, right from your terminal. We've set up a global game process for this adventure.The rules are simple. Each player starts on a 40x40 grid with health at 100 and energy at 0. Your energy replenishes over time to a maximum of 100. Navigate the grid, find other players, and use your energy to attack when they're within range. The battle continues until only one player remains or the allotted time expires.Checkout the guides on the Mechanics of the Arena and Expanding the Arena for a deeper understanding of the game.Heads Up: Don't sweat it if some command syntax seem unfamiliar. Focus on understanding the purpose of each command at a high level and, most importantly, enjoy the game!Preparing for an Adventure in ao-effect To join this global escapade, you'll need to set things up. Don't worry, it's as easy as 1-2-3!Install aos Fire up your terminal and run:Launch aos Next, create your instance of aos:bashaos Set Up the Game ID Let's keep our game server ID handy for quick access:Print Game Announcements Directly To Terminal (Optional) Here's how you can write a handler for printing announcement details:This is temporary as we will be loading this via a lua script in the next section.And voilà! You're all set to join the game.Ready to jump in? Just a few simple steps to get you going:All communication between processes in ao occurs through messages. To register, send this message to the game server:This places you in the Waiting Lobby. A small fee is needed to confirm your spot.Confirm your spot In order to confirm your spot you need some tokens. You can acquire them by sending the following message to the game:NOTE The .receive().Data will wait for a response by adding a temporary Handler that only runs once and will print the response Data. If you would like to instead just wait for the response to hit your Inbox you can call Send() without .receive() and run Inbox[#Inbox].Data to see the response Data.Handler added by .receive():Once you receive the tokens, confirm your spot by paying the game's entry fee like this:Wait for a few seconds, and you'll see live updates in your terminal about player payments and statuses.Let the Games Begin!Game Mechanics Game Start: The game begins after a 2-minute WaitTime if at least 2 players have paid. Non-paying players are removed. If not enough players pay, those who did are refunded.Players spawn at a random grid point once the game begins.It's Your Move!Making a Move: The first thing you can do is move around, no energy required! You can shift one square in any direction – up, down, left, right, or diagonally. Along with the direction you must also pass in your player id to help the game identify your move. Here's how:The available moves across the grid are as follows:Keep in Mind: Directions are case sensitive!If you move off the grid, you'll pop up on the opposite side.Time to Strike!Launching an Attack: As the game progresses, you'll accumulate energy. Use it to attack other players within a 3x3 grid range. Your attack won't hurt you, but it will affect others in range.Health starts at 100 and decreases with hits from other players. Reach 0, and it's game over for you.Wrapping Up The game ends when there's one player left or time is up. Winners receive rewards, then it's back to the lobby for another round.Enjoyed the game? What if there was a way to make your experience even better or boost your odds of winning. Checkout the next guide to find out 🤔

---

# 73. Tokengating the Chatroom  Cookbook

Document Number: 73
Source: https://cookbook_ao.arweave.net/tutorials/begin/tokengating.html
Words: 493
Extraction Method: html

Tokengating the Chatroom INFO Now that we've created a token and sent it to Trinity, we can use the token to tokengate our chatroom. This will allow only those who have the token to enter the chatroom.Video Tutorial  How to Tokengate the Chatroom Let's create a handler that will allow us to tokengate the chatroom. This handler will respond to the tag Action = "Broadcast" meaning it will replace the original Broadcast handler we built for our chatroom.Step 1: Start the same aos process.Be sure you're using the same aos process that you've used throughout the tutorial.Step 2: Open the chatroom.lua file.This is the same file we used to create the chatroom during the chatroom tutorial.Step 3: Edit your Broadcast handler.Replace the original Broadcast handler with the following code:This handler will now check the balance of the sender's token before broadcasting the message to the chatroom. If the sender doesn't have a token, the message will not be broadcasted.Save the file.Step 4: Reload the chatroom.lua file.To replace the original broadcast handler with the new one, you'll need to reload the chatroom.lua file.lua.load chatroom.lua Step 5: Test the Tokengate Now that the chatroom is tokengated, let's test it by sending a message to the chatroom.From the original aos process First, we'll test it from the original aos process.Expected Results:Testing from another Process ID.From a new aos process Now, let's test it from a new aos process that doesn't have a token. The following command creates a new AO process with the name "chatroom-no-token".Next we need to register to the chatroom we built on our original process, from our new process. Hint: type ao.id into your console to get the Process ID of the process you are currently connected to.Expected Results:Now, let's try to send a message to the chatroom.Expected Results:As you can see, the message was not broadcasted because the new process doesn't have a token.Tell Trinity "It is done" From the original aos process, send a broadcast message to the chatroom saying, "It is done".WARNING It's important to be aware of exact match data and case sensitivity. If you're not receiving a response from either Morpheus or Trinity, be sure to check the the content of your Data and Tags.Trinity will then respond to the chatroom being tokengated.Expected Results:Trinity will send a message saying, "I guess Morpheus was right. You are the one. Consider me impressed. You are now ready to join The Construct, an exclusive chatroom available to only those that have completed this tutorial. Now, go join the others by using the same tag you used Register, with this process ID: [Construct Process ID] Good luck. -Trinity". Additionally, a footer will follow the message.Conclusion You've done it! You've successfully tokengated the chatroom. This has now unlocked access to the Construct, where only those that have fully completed this tutorial can enter.Congratulations!You've shown a great deal of promise. I hope you've enjoyed this tutorial. You're now ready to build freely in ao.

---

# 74. Mechanics of the Arena  Cookbook

Document Number: 74
Source: https://cookbook_ao.arweave.net/tutorials/bots-and-games/arena-mechanics.html
Words: 1916
Extraction Method: html

Mechanics of the Arena This guide provides a comprehensive overview of the fundamental mechanics essential for designing and managing arena-style games in aos. In arena games, participants engage in rounds, strategically vying to eliminate each other until a sole victor emerges.The framework presented here lays the groundwork for crafting a wide range of games, all sharing the same core functionalities. Explore the intricacies of game development and unleash your creativity within this versatile arena.Core Functionalities Now, let's dive into the core functionalities that power arena-style games:Game Progression Modes:Arena games are structured into rounds that operate in a loop with the following progression modes: "Not-Started" → "Waiting" → "Playing" → [Someone wins or timeout] → "Waiting"...NOTE The loop timesout if there are not enough players to start a game after the waiting state.Rounds offer a defined timeframe for players to engage, intensifying the excitement of gameplay.Token Stakes:Players must deposit a specified quantity of tokens (defined by PaymentQty) to participate in the game. These tokens add a tangible stake element to the game.Bonus Rewards:Beyond the thrill of victory, players are enticed by the prospect of extra rewards. The builder has the flexibility to offer bonus tokens, defined by BonusQty, to be distributed per round. Any bets placed by players are also added to these bonuses. These bonuses serve as an additional incentive, enhancing the competitive spirit of the gameplay.Player Management:Players waiting to join the next game are tracked in the Waiting table.Active players and their game states are stored in the Players table.Eliminated players are promptly removed from the Players table and placed in the Waiting table for the next game.Round Winner Reward:When a player eliminates another, they earn not only bragging rights but also the eliminated player's deposit tokens as a reward. Additionally, winners of each round share a portion of the bonus tokens, as well as their original stake, further motivating players to strive for victory.Listener Mode:For those who prefer to watch the action unfold, the "Listen" mode offers an opportunity to stay informed without active participation. Processes can register as listeners, granting them access to all announcements from the game. While they do not engage as players, listeners can continue to observe the game's progress unless they explicitly request removal.Game State Management:To maintain the flow and fairness of arena games, an automated system oversees game state transitions. These transitions encompass waiting, playing, and ending phases. Time durations for each state, such as WaitTime and GameTime, ensure that rounds adhere to defined timeframes, preventing games from lasting indefinitely.You can refer to the code for the arena in the dropdown below:Arena Game Blueprint lua-- ARENA GAME BLUEPRINT.

-- This blueprint provides the framework to operate an 'arena' style game

-- inside an ao process. Games are played in rounds, where players aim to

-- eliminate one another until only one remains, or until the game time

-- has elapsed. The game process will play rounds indefinitely as players join

-- and leave.

-- When a player eliminates another, they receive the eliminated player's deposit token

-- as a reward. Additionally, the builder can provide a bonus of these tokens

-- to be distributed per round as an additional incentive. If the intended

-- player type in the game is a bot, providing an additional 'bonus'

-- creates an opportunity for coders to 'mine' the process's

-- tokens by competing to produce the best agent.

-- The builder can also provide other handlers that allow players to perform

-- actions in the game, calling 'eliminatePlayer()' at the appropriate moment

-- in their game logic to control the framework.

-- Processes can also register in a 'Listen' mode, where they will receive

-- all announcements from the game, but are not considered for entry into the

-- rounds themselves. They are also not unregistered unless they explicitly ask

-- to be.

-- GLOBAL VARIABLES.

-- Game progression modes in a loop:

-- [Not-Started] -> Waiting -> Playing -> [Someone wins or timeout] -> Waiting...

-- The loop is broken if there are not enough players to start a game after the waiting state.

GameMode = GameMode or "Not-Started"

StateChangeTime = StateChangeTime or undefined

-- State durations (in milliseconds)

WaitTime = WaitTime or 2 * 60 * 1000 -- 2 minutes

GameTime = GameTime or 20 * 60 * 1000 -- 20 minutes

Now = Now or undefined -- Current time, updated on every message.

-- Token information for player stakes.

UNIT = 1000

PaymentToken = PaymentToken or "ADDR"  -- Token address

PaymentQty = PaymentQty or tostring(math.floor(UNIT))    -- Quantity of tokens for registration

BonusQty = BonusQty or tostring(math.floor(UNIT))        -- Bonus token quantity for winners

-- Players waiting to join the next game and their payment status.

Waiting = Waiting or {}

-- Active players and their game states.

Players = Players or {}

-- Number of winners in the current game.

Winners = 0

-- Processes subscribed to game announcements.

Listeners = Listeners or {}

-- Minimum number of players required to start a game.

MinimumPlayers = MinimumPlayers or 2

-- Default player state initialization.

PlayerInitState = PlayerInitState or {}

-- Sends a state change announcement to all registered listeners.

-- @param event: The event type or name.

-- @param description: Description of the event.

function announce(event, description)

    for ix, address in pairs(Listeners) do

        ao.send({

            Target = address,

            Action = "Announcement",

            Event = event,

            Data = description

        })

    end

    return print(Colors.gray .. "Announcement: " .. Colors.red .. event .. " " .. Colors.blue .. description .. Colors.reset)

end

-- Sends a reward to a player.

-- @param recipient: The player receiving the reward.

-- @param qty: The quantity of the reward.

-- @param reason: The reason for the reward.

function sendReward(recipient, qty, reason)

    if type(qty) ~= number then

      qty = tonumber(qty)

    end

    ao.send({

        Target = PaymentToken,

        Action = "Transfer",

        Quantity = tostring(qty),

        Recipient = recipient,

        Reason = reason

    })

    return print(Colors.gray .. "Sent Reward: " ..

      Colors.blue .. tostring(qty) ..

      Colors.gray .. ' tokens to ' ..

      Colors.green .. recipient .. " " ..

      Colors.blue .. reason .. Colors.reset

    )

end

-- Starts the waiting period for players to become ready to play.

function startWaitingPeriod()

    GameMode = "Waiting"

    StateChangeTime = Now + WaitTime

    announce("Started-Waiting-Period", "The game is about to begin! Send your token to take part.")

    print('Starting Waiting Period')

end

-- Starts the game if there are enough players.

function startGamePeriod()

    local paidPlayers = 0

    for player, hasPaid in pairs(Waiting) do

        if hasPaid then

            paidPlayers = paidPlayers + 1

        end

    end

    if paidPlayers < MinimumPlayers then

        announce("Not-Enough-Players", "Not enough players registered! Restarting...")

        for player, hasPaid in pairs(Waiting) do

            if hasPaid then

                Waiting[player] = false

                sendReward(player, PaymentQty, "Refund")

            end

        end

        startWaitingPeriod()

        return

    end

    LastTick = undefined

    GameMode = "Playing"

    StateChangeTime = Now + GameTime

    for player, hasPaid in pairs(Waiting) do

        if hasPaid then

            Players[player] = playerInitState()

        else

            ao.send({

                Target = player,

                Action = "Ejected",

                Reason = "Did-Not-Pay"

            })

            removeListener(player) -- Removing player from listener if they didn't pay

        end

    end

    announce("Started-Game", "The game has started. Good luck!")

    print("Game Started....")

end

-- Handles the elimination of a player from the game.

-- @param eliminated: The player to be eliminated.

-- @param eliminator: The player causing the elimination.

function eliminatePlayer(eliminated, eliminator)

    sendReward(eliminator, PaymentQty, "Eliminated-Player")

    Waiting[eliminated] = false

    Players[eliminated] = nil

    ao.send({

        Target = eliminated,

        Action = "Eliminated",

        Eliminator = eliminator

    })

    announce("Player-Eliminated", eliminated .. " was eliminated by " .. eliminator .. "!")

    local playerCount = 0

    for player, _ in pairs(Players) do

        playerCount = playerCount + 1

    end

    print("Eliminating player: " .. eliminated .. " by: " .. eliminator) -- Useful for tracking eliminations

    if playerCount < MinimumPlayers then

        endGame()

    end

end

-- Ends the current game and starts a new one.

function endGame()

    print("Game Over")

    Winners = 0

    Winnings = tonumber(BonusQty) / Winners -- Calculating winnings per player

    for player, _ in pairs(Players) do

        Winners = Winners + 1

    end

    Winnings = tonumber(BonusQty) / Winners

    for player, _ in pairs(Players) do

        -- addLog("EndGame", "Sending reward of:".. Winnings + PaymentQty .. "to player: " .. player) -- Useful for tracking rewards

        sendReward(player, Winnings + tonumber(PaymentQty), "Win")

        Waiting[player] = false

    end

    Players = {}

    announce("Game-Ended", "Congratulations! The game has ended. Remaining players at conclusion: " .. Winners .. ".")

    startWaitingPeriod()

end

-- Removes a listener from the listeners' list.

-- @param listener: The listener to be removed.

function removeListener(listener)

    local idx = 0

    for i, v in ipairs(Listeners) do

        if v == listener then

            idx = i

            break

        end

    end

    if idx > 0 then

        table.remove(Listeners, idx)

    end

end

-- HANDLERS: Game state management

-- Handler for cron messages, manages game state transitions.

Handlers.add(

    "Game-State-Timers",

    function(Msg)

        return "continue"

    end,

    function(Msg)

        Now = Msg.Timestamp

        if GameMode == "Not-Started" then

            startWaitingPeriod()

        elseif GameMode == "Waiting" then

            if Now > StateChangeTime then

                startGamePeriod()

            end

        elseif GameMode == "Playing" then

            if onTick and type(onTick) == "function" then

              onTick()

            end

            if Now > StateChangeTime then

                endGame()

            end

        end

    end

)

-- Handler for player deposits to participate in the next game.

Handlers.add(

    "Transfer",

    function(Msg)

        return

            Msg.Action == "Credit-Notice" and

            Msg.From == PaymentToken and

            tonumber(Msg.Quantity) >= tonumber(PaymentQty) and "continue"

    end,

    function(Msg)

        Waiting[Msg.Sender] = true

        ao.send({

            Target = Msg.Sender,

            Action = "Payment-Received"

        })

        announce("Player-Ready", Msg.Sender .. " is ready to play!")

    end

)

-- Registers new players for the next game and subscribes them for event info.

Handlers.add(

    "Register",

   { Action = "Register" },

    function(Msg)

        if Msg.Mode ~= "Listen" and Waiting[Msg.From] == undefined then

            Waiting[Msg.From] = false

        end

        removeListener(Msg.From)

        table.insert(Listeners, Msg.From)

        ao.send({

            Target = Msg.From,

            Action = "Registered"

        })

        announce("New Player Registered", Msg.From .. " has joined in waiting.")

    end

)

-- Unregisters players and stops sending them event info.

Handlers.add(

    "Unregister",

   { Action = "Unregister" },

    function(Msg)

        removeListener(Msg.From)

        ao.send({

            Target = Msg.From,

            Action = "Unregistered"

        })

    end

)

-- Adds bet amount to BonusQty

Handlers.add(

    "AddBet",

    { Reason = "AddBet" },

    function(Msg)

        BonusQty = tonumber(BonusQty) + tonumber(Msg.Tags.Quantity)

        announce("Bet-Added", Msg.From .. "has placed a bet. " .. "BonusQty amount increased by " .. Msg.Tags.Quantity .. "!")

    end

)

-- Retrieves the current game state.

Handlers.add(

    "GetGameState",

   { Action = "GetGameState" },

    function (Msg)

        local json = require("json")

        local TimeRemaining = StateChangeTime - Now

        local GameState = json.encode({

            GameMode = GameMode,

            TimeRemaining = TimeRemaining,

            Players = Players,

            })

        ao.send({

            Target = Msg.From,

            Action = "GameState",

            Data = GameState})

    end

)

-- Alerts users regarding the time remaining in each game state.

Handlers.add(

    "AnnounceTick",

   { Action = "Tick" },

    function (Msg)

        local TimeRemaining = StateChangeTime - Now

        if GameMode == "Waiting" then

            announce("Tick", "The game will start in " .. (TimeRemaining/1000) .. " seconds.")

        elseif GameMode == "Playing" then

            announce("Tick", "The game will end in " .. (TimeRemaining/1000) .. " seconds.")

        end

    end

)

-- Sends tokens to players with no balance upon request

Handlers.add(

    "RequestTokens",

   { Action = "RequestTokens" },

    function (Msg)

        print("Transferring Tokens: " .. tostring(math.floor(10000 * UNIT)))

        ao.send({

            Target = ao.id,

            Action = "Transfer",

            Quantity = tostring(math.floor(10000 * UNIT)),

            Recipient = Msg.From,

        })

    end

) Arena Game Blueprint For those interested in using this arena framework, we've made this code easily accessible through a blueprint. Simply run the following code in your terminal:lua.load-blueprint arena Summary Understanding the mechanics of the arena can not only help you improve your autonomous agent created in the previous section but also empowers you to harness core functionalities for crafting your unique games.In the upcoming section, "Building a Game," we will dive deep into the art of utilizing these mechanics to construct captivating and one-of-a-kind games within this framework. Get ready to embark on a journey into the dynamic realm of game development! 🎮

---

# 75. Strategic Decisions  Cookbook

Document Number: 75
Source: https://cookbook_ao.arweave.net/tutorials/bots-and-games/decisions.html
Words: 552
Extraction Method: html

Strategic Decisions With the latest game state at your disposal, your bot can evolve into an autonomous agent. This transition marks an upgrade in functionality, enabling not just reactions to game states but strategic actions that consider context, energy, and proximity to make decisions.Writing the Code Return to your bot.lua file and add the following functions:lua-- Determines proximity between two points.

function inRange(x1, y1, x2, y2, range)

    return math.abs(x1 - x2) <= range and math.abs(y1 - y2) <= range

end

-- Strategically decides on the next move based on proximity and energy.

function decideNextAction()

  local player = LatestGameState.Players[ao.id]

  local targetInRange = false

  for target, state in pairs(LatestGameState.Players) do

      if target ~= ao.id and inRange(player.x, player.y, state.x, state.y, 1) then

          targetInRange = true

          break

      end

  end

  if player.energy > 5 and targetInRange then

    print("Player in range. Attacking.")

    ao.send({Target = Game, Action = "PlayerAttack", Player = ao.id, AttackEnergy = tostring(player.energy)})

  else

    print("No player in range or insufficient energy. Moving randomly.")

    local directionMap = {"Up", "Down", "Left", "Right", "UpRight", "UpLeft", "DownRight", "DownLeft"}

    local randomIndex = math.random(#directionMap)

    ao.send({Target = Game, Action = "PlayerMove", Player = ao.id, Direction = directionMap[randomIndex]})

  end

end The decideNextAction function is now a testament to our agent's ability to think and act based on a comprehensive understanding of its environment. It analyzes the latest game state to either attack if you have sufficient energy and an opponent is inRange or move otherwise.Now all you need is a handler to make sure this function runs on its own.This handler triggers upon receiving a message that the latest game state has been fetched and updated. An action is taken only when the game is in Playing mode.You can refer to the latest code for bot.lua in the dropdown below:Updated bot.lua file luaLatestGameState = LatestGameState or nil

function inRange(x1, y1, x2, y2, range)

    return math.abs(x1 - x2) <= range and math.abs(y1 - y2) <= range

end

function decideNextAction()

  local player = LatestGameState.Players[ao.id]

  local targetInRange = false

  for target, state in pairs(LatestGameState.Players) do

      if target ~= ao.id and inRange(player.x, player.y, state.x, state.y, 1) then

          targetInRange = true

          break

      end

  end

  if player.energy > 5 and targetInRange then

    print("Player in range. Attacking.")

    ao.send({Target = Game, Action = "PlayerAttack", Player = ao.id, AttackEnergy = tostring(player.energy)})

  else

    print("No player in range or insufficient energy. Moving randomly.")

    local directionMap = {"Up", "Down", "Left", "Right", "UpRight", "UpLeft", "DownRight", "DownLeft"}

    local randomIndex = math.random(#directionMap)

    ao.send({Target = Game, Action = "PlayerMove", Player = ao.id, Direction = directionMap[randomIndex]})

  end

end

Handlers.add(

"HandleAnnouncements",

{ Action = "Announcement" },

function (msg)

  ao.send({Target = Game, Action = "GetGameState"})

  print(msg.Event .. ": " .. msg.Data)

end

)

Handlers.add(

"UpdateGameState",

{ Action = "GameState" },

function (msg)

  local json = require("json")

  LatestGameState = json.decode(msg.Data)

  ao.send({Target = ao.id, Action = "UpdatedGameState"})

end

)

Handlers.add(

"decideNextAction",

{ Action = "UpdatedGameState" },

function ()

  if LatestGameState.GameMode ~= "Playing" then

    return

  end

  print("Deciding next action.")

  decideNextAction()

end

) Once again, to test out the latest upgrades, load the file in your aos player terminal as follows:lua.load bot.lua Observe your process output to see the decisions your autonomous agent makes in real-time, leveraging the current game state for strategic advantage. But what if another player attacks you and runs away while you are deciding the next move? In the next section you'll learn to automatically counter as soon as you have been attacked 🤺

---

# 76. Bots and Games  Cookbook

Document Number: 76
Source: https://cookbook_ao.arweave.net/tutorials/bots-and-games/index.html
Words: 177
Extraction Method: html

Skip to content  Bots and Games NOTE Build your own unique bot to complete Quest 3 and earn 1000 CRED, then enter games like the Grid to earn legacynet CRED 24/7!Leveraging insights from our previous chapter, this section will guide you through the realm of automation with bots in aos and the construction of games. You will learn to create autonomous agents, using them to navigate and interact with game environments effectively.Sections Getting Started with a Game 0. # Let's Play A Game:Experience a game on aos Enhancing Game Interactions with Automation 1. # Interpreting Announcements:Interpret in-game announcements 2. # Fetching Game State:Retrieve and process the latest game state 3. # Strategic Decisions:Utilize automation to determine your next move 4. # Automated Responses:Streamline attack responses through automation 5. # Bringing it Together:Combine your skills to craft an autonomous agent Game Development Insights 6. # Mechanics of the Arena:Explore the underlying mechanics of a game's arena 7. # Expanding the Arena:Build unique game logic upon the arena A journey of discovery and creation awaits. Let the adventure begin!

---

# 77. Bringing it Together  Cookbook

Document Number: 77
Source: https://cookbook_ao.arweave.net/tutorials/bots-and-games/bringing-together.html
Words: 966
Extraction Method: html

Bringing it Together This final guide wraps up our series, where you've built up an autonomous agent piece by piece. Now, let's refine your agent with some optimizations that fine-tune its operations. Here's a quick overview of the key improvements made:Sequential Command Execution: The introduction of an InAction flag ensures that your agent's actions are sequential (next action occurs only when the previous is successfully executed). This critical addition prevents your agent from acting on outdated game states, enhancing its responsiveness and accuracy. The full implementation can be found in the final code for the bot.lua file below.Dynamic State Updates and Decisions: The agent now employs an automatic tick logic, allowing for dynamic updates and decisions. This logic enables the agent to self-trigger state updates and make subsequent decisions either upon receiving a Tick message or upon completing an action, promoting autonomous operation.Automated Fee Transfer: To further streamline its operation and ensure uninterrupted participation in games, the autonomous agent now autonomously handles the transfer of confirmation fees.In addition to these features, we've also added a logging function for debugging purposes and colored prints for better comprehension of game events. These enhancements collectively make your autonomous agent more efficient and adaptable in the game environment.Check out the complete bot.lua code in the dropdown below, with all new additions highlighted accordingly:Updated bot.lua file lua-- Initializing global variables to store the latest game state and game host process.

LatestGameState = LatestGameState or nil

InAction = InAction or false -- Prevents the agent from taking multiple actions at once.

Logs = Logs or {}

colors = {

  red = "\27[31m",

  green = "\27[32m",

  blue = "\27[34m",

  reset = "\27[0m",

  gray = "\27[90m"

}

function addLog(msg, text) -- Function definition commented for performance, can be used for debugging

  Logs[msg] = Logs[msg] or {}

  table.insert(Logs[msg], text)

end

-- Checks if two points are within a given range.

-- @param x1, y1: Coordinates of the first point.

-- @param x2, y2: Coordinates of the second point.

-- @param range: The maximum allowed distance between the points.

-- @return: Boolean indicating if the points are within the specified range.

function inRange(x1, y1, x2, y2, range)

    return math.abs(x1 - x2) <= range and math.abs(y1 - y2) <= range

end

-- Decides the next action based on player proximity and energy.

-- If any player is within range, it initiates an attack; otherwise, moves randomly.

function decideNextAction()

  local player = LatestGameState.Players[ao.id]

  local targetInRange = false

  for target, state in pairs(LatestGameState.Players) do

      if target ~= ao.id and inRange(player.x, player.y, state.x, state.y, 1) then

          targetInRange = true

          break

      end

  end

  if player.energy > 5 and targetInRange then

    print(colors.red .. "Player in range. Attacking." .. colors.reset)

    ao.send({Target = Game, Action = "PlayerAttack", Player = ao.id, AttackEnergy = tostring(player.energy)})

  else

    print(colors.red .. "No player in range or insufficient energy. Moving randomly." .. colors.reset)

    local directionMap = {"Up", "Down", "Left", "Right", "UpRight", "UpLeft", "DownRight", "DownLeft"}

    local randomIndex = math.random(#directionMap)

    ao.send({Target = Game, Action = "PlayerMove", Player = ao.id, Direction = directionMap[randomIndex]})

  end

  InAction = false -- InAction logic added

end

-- Handler to print game announcements and trigger game state updates.

Handlers.add(

  "PrintAnnouncements",

  { Action = "Announcement" },

  function (msg)

    if msg.Event == "Started-Waiting-Period" then

      ao.send({Target = ao.id, Action = "AutoPay"})

    elseif (msg.Event == "Tick" or msg.Event == "Started-Game") and not InAction then

      InAction = true -- InAction logic added

      ao.send({Target = Game, Action = "GetGameState"})

    elseif InAction then -- InAction logic added

      print("Previous action still in progress. Skipping.")

    end

    print(colors.green .. msg.Event .. ": " .. msg.Data .. colors.reset)

  end

)

-- Handler to trigger game state updates.

Handlers.add(

  "GetGameStateOnTick",

  { Action =  "Tick" },

  function ()

    if not InAction then -- InAction logic added

      InAction = true -- InAction logic added

      print(colors.gray .. "Getting game state..." .. colors.reset)

      ao.send({Target = Game, Action = "GetGameState"})

    else

      print("Previous action still in progress. Skipping.")

    end

  end

)

-- Handler to automate payment confirmation when waiting period starts.

Handlers.add(

  "AutoPay",

  { Action =  "AutoPay" },

  function (msg)

    print("Auto-paying confirmation fees.")

    ao.send({ Target = Game, Action = "Transfer", Recipient = Game, Quantity = "1000"})

  end

)

-- Handler to update the game state upon receiving game state information.

Handlers.add(

  "UpdateGameState",

  { Action =  "GameState" },

  function (msg)

    local json = require("json")

    LatestGameState = json.decode(msg.Data)

    ao.send({Target = ao.id, Action = "UpdatedGameState"})

    print("Game state updated. Print \'LatestGameState\' for detailed view.")

  end

)

-- Handler to decide the next best action.

Handlers.add(

  "decideNextAction",

  { Action =  "UpdatedGameState" },

  function ()

    if LatestGameState.GameMode ~= "Playing" then

      InAction = false -- InAction logic added

      return

    end

    print("Deciding next action.")

    decideNextAction()

    ao.send({Target = ao.id, Action = "Tick"})

  end

)

-- Handler to automatically attack when hit by another player.

Handlers.add(

  "ReturnAttack",

  { Action =  "Hit" },

  function (msg)

    if not InAction then -- InAction logic added

      InAction = true -- InAction logic added

      local playerEnergy = LatestGameState.Players[ao.id].energy

      if playerEnergy == undefined then

        print(colors.red .. "Unable to read energy." .. colors.reset)

        ao.send({Target = Game, Action = "Attack-Failed", Reason = "Unable to read energy."})

      elseif playerEnergy == 0 then

        print(colors.red .. "Player has insufficient energy." .. colors.reset)

        ao.send({Target = Game, Action = "Attack-Failed", Reason = "Player has no energy."})

      else

        print(colors.red .. "Returning attack." .. colors.reset)

        ao.send({Target = Game, Action = "PlayerAttack", Player = ao.id, AttackEnergy = tostring(playerEnergy)})

      end

      InAction = false -- InAction logic added

      ao.send({Target = ao.id, Action = "Tick"})

    else

      print("Previous action still in progress. Skipping.")

    end

  end

) What's next?You're now equipped with the knowledge to craft intelligent autonomous agents. It's time to apply these insights into the game world. Understand the game's intricacies and leverage your agent's capabilities to dominate the arena. But there's more to come.In future sections, we'll dive deeper into the game arena, offering advanced strategies to elevate your agent's performance. Ready to take on the challenge? Let's see what you can create! 🕹️

---

# 78. Get started in 5 minutes  Cookbook

Document Number: 78
Source: https://cookbook_ao.arweave.net/welcome/getting-started.html
Words: 840
Extraction Method: html

Get started in 5 minutes In less than 5 mins, we'll walk you through the process of taking your first peek into the rabbit hole of AO Processes. 🕳️🐇 Now that you understand the AO-Core protocol and how AO Processes work, let's get hands-on with creating your first AO Process.System requirements The local client of aos is super simple to install. Just make sure you have:NodeJS version 20+. (If you haven't yet installed it, check out this page to find instructions for your OS).A code editor of your choice.Installing aos Once you have NodeJS on your machine, all you need to do is install aos and run it:After installation, we can simply run the command itself to start a new aos process!shaos You authenticate yourself to your aos process using a keyfile. If you have an Arweave wallet you can specify it by adding a --wallet [location] flag. If you don't, a new keyfile will be generated and stored locally for you at ~/.aos.json.Welcome to the rabbit hole The utility you just started is a local client, which is ready to relay messages for you to your new process inside the ao computer.After it connects, you should see the following:lua          _____                   _______                   _____

         /\    \                 /::\    \                 /\    \

        /::\    \               /::::\    \               /::\    \

       /::::\    \             /::::::\    \             /::::\    \

      /::::::\    \           /::::::::\    \           /::::::\    \

     /:::/\:::\    \         /:::/~~\:::\    \         /:::/\:::\    \

    /:::/__\:::\    \       /:::/    \:::\    \       /:::/__\:::\    \

   /::::\   \:::\    \     /:::/    / \:::\    \      \:::\   \:::\    \

  /::::::\   \:::\    \   /:::/____/   \:::\____\   ___\:::\   \:::\    \

 /:::/\:::\   \:::\    \ |:::|    |     |:::|    | /\   \:::\   \:::\    \

/:::/  \:::\   \:::\____\|:::|____|     |:::|    |/::\   \:::\   \:::\____\

\::/    \:::\  /:::/    / \:::\    \   /:::/    / \:::\   \:::\   \::/    /

 \/____/ \:::\/:::/    /   \:::\    \ /:::/    /   \:::\   \:::\   \/____/

          \::::::/    /     \:::\    /:::/    /     \:::\   \:::\    \

           \::::/    /       \:::\__/:::/    /       \:::\   \:::\____\

           /:::/    /         \::::::::/    /         \:::\  /:::/    /

          /:::/    /           \::::::/    /           \:::\/:::/    /

         /:::/    /             \::::/    /             \::::::/    /

        /:::/    /               \::/____/               \::::/    /

        \::/    /                 ~~                      \::/    /

         \/____/                                           \/____/

Welcome to AOS: Your operating system for AO, the decentralized open

access supercomputer.

Type ".load-blueprint chat" to join the community chat and ask questions!

AOS Client Version: 1.12.1. 2024

Type "Ctrl-C" twice to exit

Your AOS process:  QFt5SR6UwJSCnmgnROq62-W8KGY9z96k1oExgn4uAzk

default@aos-0.2.2[Inbox:1]> Welcome to your new home in the ao computer! The prompt you are now looking at is your own personal server in this decentralized machine. We will be using it to play with and explore ao in the rest of this tutorial.Sending your first command Your new personal aos process is a server that lives inside the computer, waiting to receive and execute your commands.aos loves to make things simple, so it wants to hear commands from you in the Lua programming language. Don't know Lua? Don't panic! It is a super straightforward, friendly, and fun language. We will learn it as we progress through this series.Let's break the ice and type:Then hit the "[Enter]" key. You should see your shell sign and post the message, request the result, then print the result as follows:lua"Hello, ao!" Eh. What's the big deal?Sent it a message to your process, permanently etched it into Arweave, then asked a distributed compute network to calculate its result.While the result might not look revolutionary, in reality you have done something quite extraordinary. Your process is a decentralized server that doesn't exist in any one particular place on Earth. It exists as data, replicated on Arweave between many different machines, distributed all over the world. If you wanted to, you could now attach a new compute unit to this process and recreate the state from its log of inputs (just your single command, for now) -- at any time in the future.This makes your new shell process...Resilient: There is no single place on Earth where your server actually resides. It is everywhere and nowhere -- immune from physical destruction or tampering of any kind.Permanent: Your process will never disappear. It will always exist in its ✨holographic state✨  on Arweave, allowing you to recall it and continue playing with it. A contribution has been made to Arweave's storage endowment, so that you never have to think about upkeep or maintenance payments again.Permissionless: You did not have to register in order to start this server. Your right to use it is guaranteed by its underlying protocol (Arweave), no matter what Google, Amazon, or any other BigTech company says.Trustless: The state of your server is mathematically guaranteed. This means that you -- and everyone else -- can trust it with certainty, without even having to trust the underlying hardware it runs on. This property lets you build trustless services on top: Code that runs without any privileged owner or controller, ruled purely by math.There is so much more to it, but these are the basics. Welcome to the ao computer, newbie! We are grateful to have you. 🫡 In the tutorials that follow, we will explore ao and build everything from chatrooms to autonomous, decentralized bots. Let's go!

---

# 79. Introduction to AO-Core  Cookbook

Document Number: 79
Source: https://cookbook_ao.arweave.net/welcome/ao-core-introduction.html
Words: 330
Extraction Method: html

Introduction to AO-Core AO-Core is a protocol and standard for distributed computation that forms the foundation of the AO computer. Inspired by and built upon concepts from the Erlang language, AO-Core embraces the actor model for concurrent, distributed systems. Unlike traditional blockchain systems, AO-Core defines a flexible, powerful computation protocol that enables a wide range of applications beyond just running Lua programs.What is AO-Core?AO-Core is the fundamental protocol of the AO computer that:Defines standards for trustless computation distributed across the world Provides mathematical guarantees about program execution Enables composable, modular development through devices Supports various execution environments beyond just Lua Implements the actor model for concurrent, message-passing computation The Actor Model in AO AO references the actor model of computation where:Each actor (or process) is an independent unit of computation Actors communicate exclusively through message passing Actors can create other actors, send messages, and make local decisions The system is inherently concurrent and distributed This approach, inspired by Erlang, provides natural scalability and resilience in distributed systems.Key Features of AO-Core Resilient: There is no single point of failure. AO-Core exists across many machines distributed worldwide, making it immune to physical destruction or tampering.Permanent: Computations following the AO-Core protocol are stored permanently on Arweave, allowing you to recall and continue your work at any time.Permissionless: No registration is required to use AO-Core. Your right to use it is guaranteed by the underlying protocol.Trustless: The state of your computations is mathematically guaranteed, allowing you to build services that don't require trust in any central authority.Beyond Just Processes While AO Processes (smart contracts built using the AO-Core protocol) are powerful for creating autonomous agents, AO-Core itself enables much more:Serverless functions with trustworthy guarantees Hybrid applications combining smart contract and serverless functionality Custom execution environments through different devices Composable systems using the path language In the following sections, we'll explore how AO Processes build on top of the AO-Core protocol, and how you can get started building your own applications in this powerful environment.

---

# 80. ARIO Docs

Document Number: 80
Source: https://docs.ar.io/ar-io-sdk
Words: 422
Extraction Method: html

AR.IO SDK Overview The AR.IO SDK provides functionality for interacting with the AR.IO ecosystem of services and protocols. This includes, the AR.IO Network, gateways, the ARIO token,  and ArNS domains. The AR.IO SDK is available for both NodeJS and web environments.AR.IO Network The AR.IO Network is the AO smart contract process that controls all child services and protocols.The AR.IO SDK supports read operations to access various details about the current or historical state of the network. It also provides write operations for managing features such as the Gateway Address Registry and ARIO token.Gateways AR.IO gateways are open source nodes that index and serve Arweave transaction headers and data items. Gateway operators may join their gateway to the Gateway Address Registry (GAR), which makes the gateway discoverable using the AR.IO SDK. The gateway information is stored in the AR.IO AO contract as a JSON object with the following attributes:{
    "operatorStake": "number",               // The amount of ARIO tokens staked by the operator, 50,000 minimum
    "totalDelegatedStake": "number",         // Total amount of ARIO tokens staked to the gateway by wallets other than the operator
    "vaults": "object",                      // Details of tokens vaults (locked tokens) associated with the gateway (object)
    "delegates": "object",                   // Details of non-operator wallets who staked ARIO tokens on the gateway (object)
    "startTimestamp": "number (unix)",       // Unix timestamp indicating start time
    "stats": "object",                       // Statistical information related to gateway performance (object)
    "settings": "object",                    // Configuration settings (object)
    "status": "string (e.g., joined)",       // The current status of the operator
    "observerAddress": "string"              // The public wallet address of the observer for the gateway
} The ar.io SDK supports write operations for gateway management, including joining, leaving, and updating settings. It also provides read operations for discovering gateways in the GAR and retrieving details about specific gateways.ARIO Token ARIO is an AO token that powers the ar.io Network and and its suite of permaweb applications. It is used to join the GAR, as payment for services like ArNS, as incentives for participation in the ar.io Network, and more.The ar.io SDK supports read and write operations for getting token information and balances, or transferring tokens.ArNS The Arweave Name System (ArNS) is a protocol which allows for assigning friendly names to Arweave transactions or data items. Powered by Arweave Name Tokens (ANTs), AO tokens that manage settings for individual ArNS domains, ArNS enables easy interaction with data stored on Arweave.The ar.io SDK supports read and write operations for managing ArNS domains, including retrieving domain information, leasing, purchasing, and extending leases. Additionally, it allows direct read and write access to ANTs.

---

# 81. setBaseNameRecord - ARIO Docs

Document Number: 81
Source: https://docs.ar.io/ar-io-sdk/ants/set-base-name-record
Words: 116
Extraction Method: html

setBaseNameRecord is a method on the ANT class that adds or updates the base name record for the ANT. This record defines the top-level name of the ANT (e.g., ardrive.ar.io).setBaseNameRecord requires authentication.Parameters TTL Time-To-Live (TTL) determines how often gateways should check the ANT for an update to the corresponding record. You can have different TTLs for different records within an ANT, depending on their use case. A record that is updated frequently should have a lower
value to facilitate serving current data, while a record that is updated less
often should have a higher value to allow cached data to be served more
quickly.TTL must be between 60 seconds (1 minute) and 86400 seconds (1 day).

---

# 82. setRecord - ARIO Docs

Document Number: 82
Source: https://docs.ar.io/ar-io-sdk/ants/set-record
Words: 206
Extraction Method: html

Deprecated This method is deprecated. Please use setBaseNameRecord for top-level names
or setUndernameRecord for undernames instead. See the setBaseNameRecord and setUndernameRecord documentation for
more details.setRecord is a method on the ANT class that sets or updates a record in the ANT process. Records map names to Arweave transaction IDs with optional TTL settings.setRecord requires authentication.Parameters ← Swipe to see more → Parameter Type Description Optional undername string The undername name for the record (use "@" for the root domain) false transactionId string The Arweave transaction ID to point the record to false ttlSeconds number Time-to-live in seconds for the record cache true tags array An array of GQL tag objects to attach to the AO message true ← Swipe to see more → TTL Time-To-Live (TTL) determines how often gateways should check the ANT for updates to the corresponding record. You can have different TTLs for different records within an ANT, depending on their use case. A record that is updated frequently should have a lower
value to facilitate serving current data, while a record that is updated less
often should have a higher value to allow cached data to be served more
quickly.TTL must be between 60 seconds (1 minute) and 86400 seconds (1 day).

---

# 83. ARIO Docs

Document Number: 83
Source: https://docs.ar.io/ar-io-sdk/ario/arns/extend-lease
Words: 115
Extraction Method: html

extendLease extendLease is a method on the ARIO class that extends the lease duration of a registered ArNS domain. The extension period can be 1-5 years, depending on the domain's grace period status. Note that permanently registered domains cannot have their leases extended.extendLease requires authentication.Parameters ← Swipe to see more → Parameter Type Description Optional name string The ArNS name for which to extend the lease false years number The number of years to extend the lease by (1-5 years) false fundFrom string The source of funds: 'balance', 'stakes', 'any', or 'turbo' true tags array An array of GQL tag objects to attach to the transfer AO message true ← Swipe to see more →

---

# 84. buyRecord - ARIO Docs

Document Number: 84
Source: https://docs.ar.io/ar-io-sdk/ario/arns/buy-record
Words: 111
Extraction Method: html

buyRecord is a method on the ARIO class that purchases a record in the ArNS registry for a specified name and duration.buyRecord requires authentication.Parameters ← Swipe to see more → Parameter Type Description Optional name string The ArNS name to purchase false years number The number of years to lease the name for (1-5) false processId string The ANT process ID to set for this name false type string The type of purchase: 'lease' or 'permabuy' true fundFrom string The source of funds: 'balance', 'stakes', 'any', or 'turbo' true tags array An array of GQL tag objects to attach to the transfer AO message true ← Swipe to see more →

---

# 85. setUndernameRecord - ARIO Docs

Document Number: 85
Source: https://docs.ar.io/ar-io-sdk/ants/set-undername-record
Words: 120
Extraction Method: html

setUndernameRecord is a method on the ANT class that creates or updates an undername record for the ANT. An undername is a prefix that is joined to the base name with an underscore (e.g., dapp_ardrive.ar.io).setUndernameRecord requires authentication.Parameters TTL Time-To-Live (TTL) determines how often gateways should check the ANT for updates to the corresponding record. You can have different TTLs for different records within an ANT, depending on their use case. A record that is updated frequently should have a lower
value to facilitate serving current data, while a record that is updated less
often should have a higher value to allow cached data to be served more
quickly.TTL must be between 60 seconds (1 minute) and 86400 seconds (1 day).

---

# 86. getCostDetails - ARIO Docs

Document Number: 86
Source: https://docs.ar.io/ar-io-sdk/ario/arns/get-cost-details
Words: 148
Extraction Method: html

getCostDetails is a method on the ARIO class that calculates detailed cost information for a specific interaction (such as buying an ArNS record). The method determines costs based on the interaction type, the payer's address, and the funding source (balance, stake, or any available funds).getCostDetails does not require authentication.Parameters ← Swipe to see more → Parameter Type Description Optional intent string The type of interaction to calculate costs for (e.g., 'Buy-Record') false fromAddress string - WalletAddress The Arweave address that will be charged for the interaction false fundFrom string The source of funds: 'balance', 'stakes', or 'any' false name string The ArNS name for the interaction (for Buy-Record operations) conditional type string The type of purchase: 'lease' or 'permabuy' (for Buy-Record operations) conditional years number Number of years (for lease-based operations) conditional quantity number Quantity for operations like increasing undername limits conditional ← Swipe to see more →

---

# 87. ARIO Docs

Document Number: 87
Source: https://docs.ar.io/ar-io-sdk/ario/arns/increase-undername-limit
Words: 114
Extraction Method: html

increaseUndernameLimit increaseUndernameLimit is a method on the ARIO class that increases the number of undernames an ArNS domain can support. Each domain starts with a default limit of 10 undernames and can be increased up to a maximum of 10,000 undernames.increaseUndernameLimit requires authentication.Parameters ← Swipe to see more → Parameter Type Description Optional name string The ArNS name to increase the undername limit for false increaseCount number The number of additional undername slots to purchase (up to 10,000
total) false fundFrom string The source of funds: 'balance', 'stakes', 'any', or 'turbo' true tags array An array of GQL tag objects to attach to the transfer AO message true ← Swipe to see more →

---

# 88. ARIO Docs

Document Number: 88
Source: https://docs.ar.io/ar-io-sdk/ario/gateways/update-gateway-settings
Words: 198
Extraction Method: html

updateGatewaySettings updateGatewaySettings is a method on the ARIO class that writes new gateway settings to the caller's gateway configuration.updateGatewaySettings requires authentication.Parameters ← Swipe to see more → Parameter Type Description Optional autoStake boolean If true, automatically stakes gateway rewards.true allowDelegatedStaking boolean If true, allows third parties to delegate stake to the gateway.true minDelegatedStake number Minimum number of tokens, in mARIO that can be delegated to the
gateway.true delegateRewardShareRatio number Percentage of gateway rewards to share with delegates. e.g. 10% true label string Friendly name for gateway, min 1 character, max 64 characters.true note string A note to be associated with gateway, max 256 characters.true properties string - ArweaveTxId ArweaveTxId to properties object containing additional gateway
configuration details.true observerWallet string - WalletAddress Public wallet address for wallet used to upload network observations.true fqdn string Fully qualified domain name, must be valid domain owned by gateway
operator.true port number Port number to use when accessing gateway, generally 443 (https) true protocol string - "http" || "https" Protocol to use when accessing gateway, only "https" is supported for
network participation.true tags array An array of GQL tag objects to attach to the joinNetwork AO message.true ← Swipe to see more →

---

# 89. ARIO Docs

Document Number: 89
Source: https://docs.ar.io/ar-io-sdk/ario/gateways/redelegate-stake
Words: 107
Extraction Method: html

redelegateStake redelegateStake is a method on the ARIO class that moves staked tokens from one gateway to another. A vault ID can be optionally included to redelegate from an existing withdrawal vault. The redelegation fee is calculated based on the fee rate and the stake amount. Users receive one free redelegation every seven epochs. Each additional redelegation increases the fee by 10%, up to a maximum of 60%.For example: If 1000 mARIO is redelegated with a 10% fee rate, the fee will be 100 mARIO. This results in 900 mARIO being redelegated to the new gateway and 100 mARIO being returned to the protocol balance.redelegateStake requires authentication.

---

# 90. Deploy a dApp with ArDrive web - ARIO Docs

Document Number: 90
Source: https://docs.ar.io/build/guides/ardrive-web
Words: 790
Extraction Method: html

ArDrive Web Deployment Guide Overview This guide will outline the simple steps needed to deploy your dApp or website onto the Arweave blockchain using the ArDrive web app and friendly UI.Simple apps and websites should work right out of the box. However, for advanced applications, this assumes you have already prepared your dApp to use hash routing and relative file paths, and built static files for any dApp in a language or framework that requires it (like React).Learn more about preparing your dApp for deployment here.Deploying Step 1: Log into ArDrive Go to the ArDrive web app and log in using the method of your choosing. If you don't already have an account, you will need to follow the instructions to set one up.Step 2: Select or Create a Drive Once logged in, navigate to the drive where you want your project to be hosted. If you haven't created a drive yet, or if you want a new one specifically for this project, click the big red "New" button at the top left and create a new drive. Remember, the drive needs to be set to public for your dApp to be accessible to others.Step 3: Upload your project With your drive selected, click the big red "New" button again, but this time, select "Upload Folder". Navigate to your project's root directory, or the built directory if required, and select it. This will upload the entire directory, maintaining your project's file structure.Step 4: Confirm Upload You'll be given a chance to review the upload and the associated cost. If everything looks right, click "Confirm". Remember, uploading to Arweave isnt free, but the cost is usually quite small and the benefits of having your dApp or website hosted on the permaweb are significant.Step 5: Create the Manifest While ArDrive displays your uploaded files as a traditional file structure, with files and folders inside other folders, thats not how they actually exist on Arweave. The manifest acts as a map to all the files your dApp needs to function. After you confirm your upload, navigate into your newly created folder by double clicking on it. Click the big red "New" button again and select "New Manifest" in the "Advanced" section. You'll be prompted to name the manifest and choose where to save it. Be sure to save it inside the folder you just created.Step 6: Get the Data TX ID Once the manifest is created, click on it to expand its details. In the "details" tab, on the bottom right, there's a line labeled "Data TX ID". This is the unique identifier for your uploaded dApp on Arweave. Copy this value.Step 7: View and Share your dApp Your dApp or website is now available on the permaweb forever! Append the Data TX ID you just copied to the end of an Arweave gateway URL, like https://arweave.net/. It might take a few minutes for all of your files to finish propagating through the network, but once they do your dApp or website will be accessible to anyone, anywhere, at any time.Step 8: Assign a Friendly Name The Data TX ID you copied in Step 6 is long and difficult to remember. To make it easier to access your dApp or website, you can assign a friendly name to it using ArNS. If you already own an ArNS name, you will be prompted during the creation of
your manifest if you want to assign one. If you do not, you can purchase one from arns.app.You can also assign an ArNS name to an existing manifest (or any other file) by clicking on the three dots on the right side of the file and selecting "Assign ArNS name".Updating your dApp Files uploaded to Arweave are permanent and immutable. They cannot be changed. However, the Arweave File System (ArFS) protocol used (and created) by ArDrive lets you "replace" them with new versions while still being able to access the old ones. You can do this with entire dApps as well. The old files won't be displayed in the ArDrive web app unless you click on a file to view its history.Once you have made changes to your dApp or website, and built the static directory for it, you can upload the entire folder again to the same location where you uploaded the original. Follow all the same steps listed above for uploading your dApp. You will need to create a new manifest to correctly point to the updated files. Give it the same name as the old manifest in order to "replace" it. Creating the new manifest will generate a new TX ID used to view the updated dApp.The old version of the dApp will always be available to anyone who has the correct TX ID.

---

# 91. ARIO Smart Contract - ARIO Docs

Document Number: 91
Source: https://docs.ar.io/ario-contract
Words: 300
Extraction Method: html

Overview The AR.IO smart contract encompasses all the functionality required to support the network's currency, utilities, and management.
Written in Lua and compiled to WASM64, it is deployed as a Process within AO, leveraging the decentralized infrastructure of Arweave for immutability and auditability.
This ensures that AR.IO's smart contract code is stored permanently, is easily verifiable by external auditors, and is transparent to the community.Protocol Balance The Protocol Balance is the primary sink and source of ARIO tokens circulating through the AR.IO Network.
This balance is akin to a central vault or wallet programmatically encoded into the network's smart contract from which ArNS revenue is accumulated and incentive rewards are distributed.This balance is stored like any other token balance in the AR.IO smart contract, using the contract's Process ID as the balance owner.
This balance is stored like any other token balance in the AR.IO smart contract, using the contract’s Process ID as the balance owner.
Should a user or organization desire, tokens can even be sent directly into this balance to support the reward protocol and ecosystem.Cross-Chain Signature Support AO leverages the flexibility of ANS-104 data items, which support multiple signature types from various blockchains. This includes signatures from Arweave, Ethereum, Solana, Cosmos, among others.This cross-chain signature support provides significant benefits to the AR.IO network:Interoperability: Cross-chain signatures enable seamless interactions across different blockchain ecosystems, allowing AR.IO to integrate with diverse apps and services without friction.Flexibility: Users can validate transactions with signatures from their preferred blockchain, making it easier for a broader range of participants to engage with AR.IO using familiar wallets and mechanisms.Security: Decentralized cryptographic standards across chains ensure that interactions on AR.IO remain secure and trusted, regardless of the blockchain used.By supporting cross-chain signatures, AR.IO enhances interoperability, flexibility, and security, empowering users and developers across multiple blockchain ecosystems.

---

# 92. Arweave Name System (ArNS) - ARIO Docs

Document Number: 92
Source: https://docs.ar.io/arns
Words: 1711
Extraction Method: html

Overview Arweave URLs and transaction IDs are long, difficult to remember, and occasionally miscategorized as spam.
The Arweave Name System (ArNS) aims to resolve these problems in a decentralized manner.
ArNS is a censorship-resistant naming system stored on Arweave, powered by ARIO tokens, enabled through AR.IO gateway domains, and used to connect friendly domain names to permaweb apps, web pages, data, and identities.It's an open, permissionless, domain name registrar that doesn’t rely on a single TLD.This system works similarly to traditional DNS services, where users can purchase a name in a registry and DNS Name servers resolve these names to IP addresses.
The system shall be flexible and allow users to purchase names permanently or lease them for a defined duration based on their use case.
With ArNS, the registry is stored permanently on Arweave via AO, making it immutable and globally resilient.
This also means that apps and infrastructure cannot just read the latest state of the registry but can also check any point in time in the past, creating a “Wayback Machine” of permanent data.Users can register a name, like ardrive, within the ArNS Registry.
Before owning a name, they must create an Arweave Name Token (ANT), an AO Computer based token and open-source protocol used by ArNS to track the ownership and control over the name.
ANTs allow the owner to set a mutable pointer to any type of permaweb data, like a page, app or file, via its Arweave transaction ID.Each AR.IO gateway acts as an ArNS Name resolver.
They will fetch the latest state of both the ArNS Registry and its associated ANTs from an AO compute unit (CU) and serve this information rapidly for apps and users.
AR.IO gateways will also resolve that name as one of their own subdomains, e.g., https://ardrive.arweave.net and proxy all requests to the associated Arweave transaction ID.
This means that ANTs work across all AR.IO gateways that support them: https://ardrive.ar-io.dev, https://ardrive.g8way.io/, etc.Users can easily reference these friendly names in their browsers, and other applications and infrastructure can build rich solutions on top of these ArNS primitives.Name Registration There are two different types of name registrations that can be utilized based upon the needs of the user:Lease: a name may be leased on a yearly basis. A leased name can have its lease extended or renewed but only up to a maximum active lease of five (5) years at any time.Permanent (permabuy): a name may be purchased for an indefinite duration.Registering a name requires spending ARIO tokens corresponding to the name’s character length and purchase type.Name Registry The ArNS Registry is a list of all registered names and their associated ANT Process IDs. Key rules embedded within the smart contract include:Genesis Prices: Set within the contract as starting conditions.Dynamic Pricing: Varies based on name length, purchase type (lease vs buy), lease duration, and current Demand Factor.Name Records: Include a pointer to the Arweave Name Token process identifier, lease end time (if applicable), and undername allocation.Reassignment: Name registrations can be reassigned from one ANT to another.Lease Extension: Anyone with available ARIO Tokens can extend any name’s active lease.Lease to Permanent Buy: Anyone with available ARIO Tokens can convert a name’s lease to a permanent buy.Undername Capacity: Additional undername capacity can be purchased for any actively registered name. There is no cap on the maximum amount of undernames that a top-level ArNS name can have associated with it.Name Removal: Name records can only be removed from the registry if a lease expires, or a permanent name is returned to the protocol.Name Validation Rules All names registered shall meet the following criteria:Valid names include only numbers 0-9, characters a-z and dashes.Dashes cannot be leading or trailing characters.Dashes cannot be used in single character domains.1 character minimum, 51 characters maximum.Shall not be an invalid name predesignated to prevent unintentional use/abuse such as www.Lease Expirations When a lease term ends, there is a grace period of two (2) weeks where the lease can be renewed before it fully expires.
If this grace period elapses, the name is considered expired and returns to the protocol for public registration. Once expired, a name’s associated undername registrations and capacity also expire.A recently expired name’s registration shall be priced subject to the “Returned Name Premium” mechanics detailed below.Lease to Permabuy Conversions An actively leased name may be converted to a permanent registration. The price for this conversion shall be treated as if it were a new permanent name purchase.This functionality allows users to transition from leasing to permanent ownership based on changing needs and available resources.
It generates additional protocol revenue through conversion fees, contributing to the ecosystem's financial health and reward system.
Additionally, by maintaining fair value for name conversions, it ensures prices reflect current market conditions, promoting a balanced and fair environment.Permanent Name Return Users have the option to “return” their permanently registered names back to the protocol.
This process allows users to relinquish their ownership, returning the name to the protocol for public re-registration. Only the Owner of a name can initiate a name return.When a permanent name is returned, the name is subject to a "Returned Name Premium”, similar to expired leases.
A key difference is that if the name is repurchased during the premium window, the proceeds are split between the returning owner and the protocol balance.Primary Names The Arweave Name System (ArNS) supports the designation of a "Primary Name" for users, simplifying how Arweave addresses are displayed across applications.
A Primary Name is a user-friendly alias that replaces complex wallet addresses, making interactions and profiles easier to manage and identify.Users can set one of their owned ArNS names as their Primary Name, subject to a small fee. This allows applications to use a single, human-readable identifier for a wallet, improving user experience across the network.Arweave Name Token (ANT) To establish ownership of a record in the ArNS Registry, each record contains both a friendly name and a reference to an Arweave Name Token, ANT.
Name Tokens are unique AO Computer based tokens / processes that give their owners the ability to update the Arweave Transaction IDs that their associated friendly names point to.The ANT smart contract process is a standardized contract that implements the specific Arweave Name Process specification required by AR.IO gateways who resolve ArNS names and their Arweave Transaction IDs.
It also contains other basic functionality to establish ownership and the ability to transfer ownership and update the Arweave Transaction ID.Name Tokens have an owner, who can transfer the token and control its modifiable settings.
These settings include modifying the address resolution time to live (ttl) for each name contained in the ANT, and other settings like the ANT Name, Ticker, and an ANT Controller.
The controller can only manage the ANT and set and update records, name, and the ticker, but cannot transfer the ANT.
Note that ANTs are initially created in accordance with network standards by an end user who then has to ability to transfer its ownership or assign a controller as they see fit.Owners of names should ensure their ANT supports evolve ability if future modifications are desired. Loss of a private key for a permanently purchased name can result in the name being "bricked”.Secondary markets could be created by ecosystem partners that facilitate the trading of Name Tokens.
Additionally, tertiary markets could be created that support the leasing of these friendly names to other users.
Such markets, if any, would be created by third parties unrelated to and outside of the scope of this paper or control of the Foundation.The table below indicates some of the possible interactions with the ArNS registry, corresponding ANTs, and who can perform them:ANT Interactions Under_names ANT owners and controllers can configure multiple subdomains for their registered ArNS name known as “under_names” or more easily written “undernames”.
These undernames are assigned individually at the time of registration or can be added on to any registered name at any time.Under_names use an underscore “_” in place of a more typically used dot “.“ to separate the subdomain from the main ArNS domain.Addressing Variable Market Conditions The future market landscape is unpredictable, and the AR.IO Network smart contract is designed to be immutable, operating without governance or manual intervention.
Using a pricing oracle to fix name prices relative to a stable currency is not viable due to the infancy of available solutions and reliance on external dependencies.
To address these challenges, ArNS is self-contained and adaptive, with name prices reflecting network activity and market conditions over time.To achieve this, ArNS incorporates:A dynamic pricing model that adjusts fees using a "Demand Factor" based on ArNS purchase activity.A Returned Name Premium (RNP) system that applies a timed, descending multiplier to registration prices for names that have recently expired or been returned to the protocol.This approach ensures that name valuations adapt to market conditions within the constraints of an immutable, maintenance-free smart contract framework.Dynamic Pricing Model ArNS employs an adaptive pricing model to balance market demand with pricing fairness for name registration within the network.
This model integrates static and dynamic elements, adjusting prices based on name length and purchase options like leasing, permanent acquisition, and undername amounts.
A key element is the Demand Factor (DF), which dynamically adjusts prices according to network activity and revenue trends, ensuring prices reflect market conditions while remaining accessible and affordable.A detailed description of the variables and formulas used for dynamic pricing can be found in the Appendix.Returned Name Premiums (RNP) ArNS applies a Returned Name Premium (RNP) to names that re-enter the market after expiration or permanent return.
This premium starts at a maximum value and decreases linearly over a predefined window, ensuring fair and transparent pricing for re-registered names.The RNP multiplier is applied to the registration price of both permanently purchased and leased names.Gateway Operator ArNS Discount Gateway operators who demonstrate consistent, healthy participation in the network are eligible for a 20% discount on certain ArNS interactions.To qualify:The gateway must maintain a “Gateway Performance Ratio Weight” (GPRW) of 0.85 or higher.The gateway must have a “Tenure Weight” (TW) of 0.5 or greater, indicating at least a 3-month prior commitment to the network.A gateway marked as “Leaving” shall not be eligible for this discount.Eligible ArNS Discounted Interactions:Purchasing a name Extending a lease Upgrading a lease to permabuy Increasing undernames capacity

---

# 93. ARIO SDK Release Notes - ARIO Docs

Document Number: 93
Source: https://docs.ar.io/ar-io-sdk/release-notes
Words: 5988
Extraction Method: html

AR.IO SDK Changelog Overview Welcome to the documentation page for the AR.IO SDK release notes. Here, you will find detailed information about each version of the AR.IO SDK, including the enhancements, bug fixes, and any other changes introduced in every release. This page serves as a comprehensive resource to keep you informed about the latest developments and updates in the AR.IO SDK. For those interested in exploring the source code, each release's code is readily accessible at our GitHub repository: AR.IO SDK change logs. Stay updated with the continuous improvements and advancements in the AR.IO SDK by referring to this page for all release-related information.3.9.1 (2025-03-31) Bug Fixes ario: throw errors if fail to find epoch data (b1bb024) cli: update other vault APIs (0916e96) cli: update types and errors in cli (4b5aebd) io: do not return undefined for any API (ce6a077) 3.9.0 (2025-03-27) Bug Fixes ao: update retry logic on send, include more verbose messaging (70e6678) comments: cleanup (7444be3) exports: move types/browser exports above import/require (280d8bd) gql: add fallback to go to CU for epoch distribution data (12be216) logging: log errors more verbosely on read fails (c9fab18) messaging: only retry messaging if no id was received (9cbffef) options: add write options to addVersion (95ccef0) PE-7802: add logo to SpawnAntState type and options (c7adfca) send: add comment on not retrying on send (692938a) send: log on max attempts in send as well (2399d23) versions: export versions class (6368c44) Features versions: add ANT version class (c9ec5c5) versions: add versioning handlers to ant registry (c681909) 3.8.4 (2025-03-12) Bug Fixes types: add vault controller as optional to vault (f26bdb3) 3.8.3 (2025-03-05) Bug Fixes add missing maxDelegateRewardSharePct field from AoGatewayRegistrySettings (87942ad) schema: remove viem and use string for AOAddressSchema (090c799) schemas: update ant schema to accept eth support (7bc7df4) tests: update unit test to check loosely on eth address (b8e202b) 3.8.2 (2025-02-25) Bug Fixes missing break for happy path through send (e55ecc1) modify retry logic for send to only retry on exceptions from ao.message or ao.result (229df6b) modify retry logic to only occur on dryrun exceptions (c578893) protect against if retries is 0 (6aa1b58) 3.8.1 (2025-02-21) Bug Fixes mainnet: default to the mainnet process ID (a96713c) [3.8.0] (2025-02-20) View changes on Github Features mainnet: update constant to the mainnet process ID (4a11840) [3.7.1] (2025-02-19) View changes on Github Bug Fixes types: overload getEpoch to provide correct types on specific param requests (bafce74) 3.7.1-alpha.1 (2025-02-18) Bug Fixes types: overload getEpoch to provide correct types on specific param requests (bafce74) [3.7.0] (2025-02-17) View changes on GitHub Bug Fixes types: fix epoch settings type (a306baa) Features distributions: init paginated distributions cli command PE-7641 (8810ec6) distributions: init paginated getEligibleDistributions PE-7641 (9ba192f) distributions: remove eligible rewards from get epoch return PE-7641 (4437eaa) read commands: add commands epoch-settings, demand-factor-settings, read-action (821b6f6) [3.7.0-alpha.1] (2025-02-17) View changes on GitHub Features distributions: init paginated distributions cli command PE-7641 (8810ec6) distributions: init paginated getEligibleDistributions PE-7641 (9ba192f) distributions: remove eligible rewards from get epoch return PE-7641 (4437eaa) read commands: add commands epoch-settings, demand-factor-settings, read-action (821b6f6) [3.6.2-alpha.1] (2025-02-17) View changes on GitHub Bug Fixes types: fix epoch settings type (a306baa) [3.6.1] (2025-02-17) View changes on GitHub Bug Fixes types: correct types for demand factor and gateway settings, update tests (583ffeb) [3.6.0] (2025-02-17) View changes on GitHub Bug Fixes ant ids: update module and lua source ids to ant 15 (b8d6c96) deps: bump aoconnect sdk (3896ee8) ids: bump module and lua source ids to ant 16 (cf6d0de) page size: set page size to 1000 on fetch all records util (5fa802e) request name: add fund from tag on request primary name api (be362ad) Features ant: add sorting to ANT records responses by default (4e74825) [3.5.3] (2025-02-12) View changes on GitHub Bug Fixes arns: use buy-name buy default for getCostDetails (d71f402) [3.5.2] (2025-02-12) View changes on GitHub Bug Fixes arns: use buy-name when fetching token cost by default (5585b4d) [3.5.1] (2025-02-09) View changes on GitHub Bug Fixes gql: use goldsky by default for fetching gql data (57f4948) zod: fix zod enforcement (08d5168) [3.5.0] (2025-02-06) View changes on GitHub Bug Fixes ant: add priority as an attribute on ANTs (f0c6758) ant: update types and add index for easy enforcement (3dd6df5) ant: use deterministic sort with no locale comparison (7f2e067) evolve: use fetch for data instead of arweave (6deb91c) module ids: update ant lua and module id (97e0628) Features arns stats: include arns stats type on epoch PE-7562 (f92ee91) [3.4.1] (2025-02-03) View changes on GitHub Bug Fixes epochs: getPrescribedObservers and getPrescribedNames should get data from GQL/arweave vs. the contract (d8fa25d) gar: mark old fields as deprecated, add new ones (18ca1b4) [3.4.0] (2025-01-31) View changes on GitHub Bug Fixes ant: add setUndername and setBasename apis (ce4abfe) Features ant apis: add commands for new methods (931e621) revokable vaults: init revokeVault and vaultedTransfer commands and methods PE-7514 (6ca44a1) vault apis: assert lock length in range PE-7541 (3585643) vault apis: init write methods+commands for create/extend/increase vault PE-7541 (b2e3cab) [3.3.1] (2025-01-29) View changes on GitHub Bug Fixes ant: bumps ids (8eb2e38) ants: module bump _wSmbjfSlX3dZNcqE8JqKmj-DKum9uQ_jB08LwOKCyw (8d442e0) ant: update ANT ids (9f37e76) boot: add boot loader logic to ant spawn util (f00ab47) lua: update lua code id (76822a2) tags: remove extra tags from spawn util (f308b84) [3.3.0] (2025-01-24) View changes on GitHub Bug Fixes ants: tag with ao authority (f08af65) ao: add ao client for ants in emitter (489c040) arconnect: use signDataItem method, signature is deprecated (11e2378) arweave: use defaultArweave when fetching data (acf3e02) error handling: trim escape codes from thrown error PE-7417 (6dcf641) error handling: use a consolidated regexp for msg.Error and msg.Tags.Error PE-7417 (770a81e) gql: add retries when fetching epoch distribution data from arweave (42c1534) ids: add module and code ids (7474ccd) import: use import from file (f8fe7b4) logs: add processId to read error logs, include stack trace (51b7e38) module id: update ant module id (9e122af) pagination: allow nested keys in sortBy pagination params utility type PE-7428 (8ae8d88) spawn: spawn ANTs with a custom ANT module instead of aos module (2359b5b) test: double test timeout (4a52b81) ts: add root dir (e33eba5) types: simplify types for init functions, cleanup contructors (2197d99) types: simplify types for init functions, cleanup contructors (cd0afa6) Features add writeAction sdk/cli command for utility PE-7417 (1953504) get all delegates: init getAllDelegates type/handler PE-7221 (b015582) get all delegates: init list-all-delegates command PE-7221 (a632563) get all vaults: init command PE-7220 (e74a6e4) get all vaults: init type and ARIO method PE-7220 (e8f5a74) io: fetch historical epoch data from gql (b627d55) [3.2.0] (2025-01-13) View changes on GitHub Bug Fixes ant: add getLogo api (eddc3a8) ario: use standardize tags for registration fees and cost details (3f5fdbe) io: remove new APIs (d916ab6) types: add Buy-Name to supported intent types (b5a6d01) Features ario: add new APIs to ario class, update ant removePrimaryNames tags (61e0ee8) cost-details: include returnedNameDetails when they exist on cost-details PE-7371 (9edfb79) [3.1.0] (2025-01-02) View changes on GitHub Bug Fixes don't get old arweave block timestamps on read actions (1792ee8) don't return null when stringified null is found in message data on ao.read (c5873e6) eth signer: use a unique anchor in ans-104 headers (8cd5587) format process errors to be more user friendly PE-7327 (3449e32) io: fix AoEpochData type, add prescribedNames (1ba3588) tags: prune out empty tags (de0ec83) types: fix funding plan vaults type (1cea7db) types: revert prescribedObserver type (ca60f6f) Features cost-details: init cli command get-cost-details PE-7114 (674626e) cost-details: init new cost method for exposing fundingPlan and discounts PE-7114 (c6910c8) fund-from: add Fund-From tag to eligible methods/commands PE-7291 (4d47270) primary names: add processID to read APIs PE-7307 (e01e6ce) remove usage of Tags.Timestamp in favor of computing epoch indexes PE-7338 (ee1bea0) [3.0.0] (2024-12-10) View changes on GitHub Bug Fixes ar.io cli: use global program from cli.ts scope for ar.io command PE-5854 (3e83298) expose instant param for decreaseOperatorStake function arg type (2fd1f5d) lua id: change lua id (d4907db) remove un-used import (5db9ac0) spawn-ant: use a valid default ttlSeconds (aea4aa7) use Keywords for setKeywords (19ab3ad) [3.0.0] (2024-12-10) View changes on GitHub Bug Fixes ar.io cli: use global program from cli.ts scope for ar.io command PE-5854 (3e83298) expose instant param for decreaseOperatorStake function arg type (2fd1f5d) lua id: change lua id (d4907db) remove un-used import (5db9ac0) spawn-ant: use a valid default ttlSeconds (aea4aa7) use Keywords for setKeywords (19ab3ad) Features ar-io cli: init balance command and CLI setup (94c630b) ar-io cli: init join-network command (fc9dc07) ar.io cli: add --cu-url global parameter PE-5854 (2346f5b) ar.io cli: enable confirmation prompts on each write action PE-5854 (9ac88bb) ar.io cli: include --tags input in write actions PE-5854 (4b9d03e) ar.io cli: init buy/upgrade/extend-record, inc-undernames, sub-auc-bid, req-prim-name PE-5854 (5eb3df2) ar.io cli: init decrease-delegate-stake instant/cancel-withdraw commands PE-5854 (f0e7b9e) ar.io cli: init epoch read commands PE-5854 (61e0fc3) ar.io cli: init get token cost and auction prices PE-5854 (867807d) ar.io cli: init get-delegations, get-arns-record, list-arns-records commands PE-5854 (d7cbde3) ar.io cli: init get-gateway-delegates and get-gateways commands PE-5854 (35a33ef) ar.io cli: init get-vault and get-gateway commands (d262243) ar.io cli: init increase/decrease-operator-stake commands PE-5854 (1312860) ar.io cli: init info command (c721374) ar.io cli: init leave-network, delegate-stake PE-5854 (40ebe06) ar.io cli: init pagination from CLI layer PE-5854 (f52ce1f) ar.io cli: init read/write ANT commands PE-5854 (392a9ef) ar.io cli: init redelegate-stake PE-5854 (7bf4a8e) ar.io cli: init save-observations PE-5854 (f80bb8c) ar.io cli: init spawn-ant and get-ant-state PE-5854 (119c765) ar.io cli: init token-supply command (b58d782) ar.io cli: init transfer command (5553584) ar.io cli: init update-gateway-settings PE-5854 (7a6aa4b) ar.io cli: stringify outputs for command line compatibility (3c04cac) ARIO token: change all IO references to ARIO (4f8135d) ARIO token: update all IO references to ARIO (8fb2188) returned names: remove/replace auction APIs in favor returned names (2c9826f) BREAKING CHANGES ARIO token: All exported IO and IOToken are now repleced with ARIO and ARIOToken respectively PE-7225 [2.6.0] (2024-12-05) View changes on GitHub Bug Fixes lua id: bump lua id for ANT 9 (9e8e7e8) use Keywords for setKeywords (99cccd4) Features get demand factor settings: init new IO method PE-6894 (ad2eb36) init get gateway registry settings PE-6895 (bb7b6b4) [2.5.5] (2024-11-28) View changes on GitHub Bug Fixes io: update gateway delegates api, add to README (65aa6a8) [2.5.4] (2024-11-28) View changes on GitHub Bug Fixes primary: support primary name in token cost API (b4edf47) [2.5.3] (2024-11-27) View changes on GitHub Bug Fixes ant lua id: update ant lua id (54ff68b) ant: update write handler types removes evolve handler name (d9f5de4) handler names: add primary name handlers (5192c09) [2.5.2] (2024-11-25) View changes on GitHub Bug Fixes io: fix tag for requestPrimaryName API (bdaeaaf) io: updated types and fixed apis for primary name requests (a297628) [2.5.1] (2024-11-22) View changes on GitHub Bug Fixes primary names: update type for getPrimaryNameRequest (bdd3a9f) [2.5.0] (2024-11-22) View changes on GitHub Bug Fixes ant: revert breaking change on records for ANT (58db878) arns: update reserved names to pagaination api (dacf0c5) cjs: remove ant validation from cjs test (50b8290) errors: we should be checking the result.Error as well as tags (7ffe131) eslint: remove unnecessary rule config (03a0552) getHandlers: remove redundant check (b0c9548) handlers: update handler name list (251695e) id and test: add test for old ant and add lua source id for new code (77601b2) io: add getDelegations to AoIORead (7c30c9b) io: use helper for computing timestamp (ffe6ff3) lint: ignore underscore vars (2c84d3d) lint: update lint rule for ignore args (136e44a) lint: update linter to allow nullable string (b985139) lua id: rollback lua id (89b8392) primary: add additional ANT handlers for primary names (c98b136) readme: make api headers h4 (395f7fb) readme: update readme with new apis on ant class (bce76d2) readme: use real outputs in example (1529f79) setLogo: call param txId instead of logo (cda5e1d) source id: name the source id tags the same on evolve and spawn (058c829) spawn: add lua source id to spawn (8850ed2) test: remove old test for validate (14a77dc) tests: add test for old ant (0489cb6) tests: add unit tests for util and move parsing of records to uitl (2d08c9a) tests: update ANT in tests to use v8 ant (1eff8a9) types: modify AoDelegation type (18bb755) types: restructure type construction (2ef04db) validation util: remove validation util (d803e59) validator: add comments and reformat into a more clear loop for creating the validation config (ea3e70c) vaults: add API for gateway vaults (923b2cd) Features delegations: add getter for staked and vaulted delegations PE-7093 (7182942) delegations: add SDK function to retrieve an address's delegations PE-7093 (07c9107) getRecords: update getRecords to return as flat array of objects (b9808c1) io: add getAllowedDelegates to IO (7d143e0) PE-6910: support primary name APIs (6ace606) PE-6910: support primary name APIs (82a5b44) redelegate stake: init IO methods PE-7159 (7539dd2) setLogo: add set logo api to ant class (c5812b1) util: move validation util to ant class (cad7149) validation util: simplify validation util (cd57929) validations: add write validation util (69fc131) [2.4.0] (2024-11-12) View changes on GitHub Bug Fixes ant: add reassignName to ant implementation (9e705a9) auctions: fix submitAuctionApi to accept type and years (6780a80) auctions: update auction APIs and types (5fd2ccc) auctions: update read APIs to fetch auctions, use vite example display active auction (32001c2) auctions: update types and add intervalMs (bc21200) corrected AoVaultData field to be startTimestamp (b9888bf) delegates: fixes type (ae7be5c) emitter: do non strict checks on state in arns emitter (6566a3c) emitter: provide strictness in constuctor (060df05) exports: add exports to barrel file (fec094e) exports: dont export http stuff) (d6369aa) io: consolidate instantGatewayWithdrawal and instantGatewayWithdrawal to just instantWithdrawal, update `cancelWithdrawal (ea9f3eb) io: include address in delegate type for gateway (46ef1a7) lint: add lint fix and missing bracket (72446aa) PE-7080: add apis for fetching paginated delegates (e3d4af2) schema: add strict mode to ANT with default to false (4864abf) schemas: add passthrough on schema checks for ants (9cb2776) schemas: add zod schemas and tests (feba587) schema: specify HandlerNames instead of Handlers (44cc472) schemas: update ant schema and tests (f3284ed) schema: update handlers schema (6ec52e4) strict: allow for passing in strict mode on apis (e147220) tag: small tweak to instant tag (663de6f) test: correct params for get record (f999c49) tests: add esm tests and remove redundant cjs tests (95244ea) tests: add js path on imports (db1520a) tests: simplify strict check on test (62c9140) types: add back delegates for AoGateway (d337a74) types: update types to match contract (cb7d2b4) types: use generic on PageParms for sortBy, update delegate types (7a1abc4) util: create schema parsing util to pretty format errors (367537a) validations: add zod schema validations on ant returns (163c2f1) withdrawls: update API for cancelling withdrawls to allow delegate and operator withdrawls (5cb680a) Features ant: adds set-keywords and set-description methods for ants) (3b260a2) ant: support releasing of name of ANTs (16363e8) arns: add upgradeRecord API (9c1726d) auctions: add auctions api to IO classes (974897b) delegates: add instant delegate withdrawal for a fee (4b4cb8f) getVault: init IO method PE-7081 (0e3cde2) paginated vaults: init SDK paginated vaults PE-7081 (6d079f9) paginated vaults: use flat array over nested vaults PE-7081 (e17cfb7) [2.3.2] (2024-10-16) View changes on GitHub Bug Fixes io: add getDemandFactor api (feab461) io: update getTokenSupply to type that returns full breakdown of tokens (e790055) types: add totalEligibleGateways to AoEpochDistributionData type (9a35d39) types: update gateways to include services (a3fe5b4) [2.3.1] (2024-10-09) View changes on GitHub Bug Fixes use AoEpochObservationData type to match what is coming back from contract (684abf3) [2.3.0] (2024-10-08) View changes on GitHub Bug Fixes ao: check messages is not empty to avoid .length error when evaluating outputs of dryrun (a7b4953) logs: enable logging in spawn and evolve utils (08ce71a) luaID: update lua id to latest for ant source code (9c13dd3) main: merge main back to alpha, release hotfixes on alpha (9299427) types: add source code tx id to ant state type (8949f04) types: fix types on ant (3bdb3a6) types: remove restricted type (b1fac75) types: update type and tests (877b03f) types: update types (883ffb3) Features delegates: add cancel delegate withdrawal method (a3827dc) io: add api for querying get registration fees handler to AoIORead class (7b3909f) [2.2.5] (2024-09-26) View changes on GitHub Bug Fixes ant: allow sending tags on ant write interactions (99c24f8) [2.2.4] (2024-09-26) View changes on GitHub Bug Fixes types: update getInfo types on IO (7a0d20d) [2.2.3] (2024-09-25) View changes on GitHub Bug Fixes types: update type and tests (877b03f) [2.2.2] (2024-09-23) View changes on GitHub Bug Fixes deps: update arbundles to @dha-team/arbundles (c41e4e4) [2.2.1] (2024-09-16) View changes on GitHub Bug Fixes types: correct totalEpochCount for gateway stats (f82fed8) [2.2.0] (2024-08-30) View changes on GitHub Bug Fixes logger: permit logger as argument for typeguard util and default it (45df626) register: update spawn ant to register at end of spawn (4320c80) signer: add typeguard util for aoSigner (0d7f210) signing: add aosigner to contract signer (3b0495a) tests: dont send messages to ao in e2e tests (e7108da) tests: reconfigure test structure (1872a26) tests: use test-wallet fixture in tests instead of generating anew each time (27a5dc2) typeguard: return true or false in typeguard and log the error (4b851c5) types: update types for epoch distributions (5aedf50) util: use ANTRegistry class for registering ant on spawn instead of aoconnect (350112d) Features ant id: update lua ant id to latest (968c30e) util: add AoAntState typeguard util (c6f457f) [2.1.0] (2024-08-07) View changes on GitHub Bug Fixes actions: ignore engines in action (7f6f87d) ant lua id: update to version Flwio4Lr08g6s6uim6lEJNnVGD9ylvz0_aafvpiL8FI (8cbd564) ant: remove data from ant object, none of our ant methods require data attributes (0f267c1) ao: update AoProcess to only support string | undefined (584aee1) arns: update event emitter to provide more events and logs while loading arns records (8775896) constants: do not set env var for ant registry (9e61cc7) deps: move arconnect to dev deps (34f07d2) emiter: use a set to filter out duplicate (7887af9) emitter: add page size param for emitter to increase amount of records per page to 50k (b6f2157) errors: use any type on error (f14ed5a) events: use arns name space for events (1d67dfe) evolve: call eval twice to ensure evolve txid is set (a6261e5) evolve: dont double eval (a2a9121) evolve: fixed evolve somehow (b06503b) example: dont spawn in example (d1d5147) example: remove unused arweave instance (d0035c0) format: fix linting issues in format (b72dc1f) gateway stats: update gateway stat types (a59b166) io: add api that returns the total token supply (261c85c) io: no longer add data to save observations (c017b52) lint: fix lint errors and warnings (e532f4e) lua id: set new lua id in constants (e4c3aaf) naming: name AoSigner property aoSigner (4604524) records: update arns emitter to use ant registry (e55a67b) signer: describe signing function as signer vs aoSigner in case of signer type changes (3b23f80) signer: move createAoSigner to be a util (7f7a0e6) signer: pass in signing function instead of signer class (cba16e3) signer: use AoSigner type as return type (8e95edd) spawn: update spawn to use ant registry id in the tags (28dae7f) tests: check the return of ACL on ant tests more granularly (350bab1) tests: update e2e tests to only read from ant registry (a61e0bf) tests: update web test to use ANT registry in app (38ca913) tests: use const for unchanging test vars (9f965e1) test: update browser test with data test id and render checks (93741cb) test: use a known wallet adddress in tests (9dac280) todo: remove completed todo comment (c868522) types: add gateway weights to AoGateway (e725198) types: check info on evolve util first (a44cca1) types: remove deprecated types (c674876) types: update AoGateway to include weights (5368668) types: update type name to what contract returns (99edbad) use custom event names to avoid overlap (5b919ac) utils: revert new util (c959c81) utils: update util to use ant registry (b2223d4) Features ant registry: add ant registry class (2056674) evolve: add evolve util (47bfe20) signing: add window arweave wallet to available signing options (7596aec) [2.0.2] (2024-07-12) View changes on GitHub Bug Fixes types: update gateway settings type to only support observerAddress (13e073b) [2.0.1] (2024-07-11) View changes on GitHub Bug Fixes logger: fixes the console logger to respect the log level provided by web clients (99d7993) [2.0.0] (2024-07-11) View changes on GitHub Bug Fixes arweave: use default arweave in IO (21d25b9) deps: replace bunyan or console depending on the client environment (9d940aa) log: allow log level configuration for clients (9cb0981) log: replace bunyan with winston to ensure browser compatibility (80b38e0) Features io: add paginated gateway support for larger state objects (e.g. balances, records, and gateways) (b23efa8) util: add utility for fetching all records (8df2aac) io: add leaveNetwork API (54222ce) BREAKING CHANGES deps: removes all smartweave implementations using warp-sdk. The result is an only AO compatible ANT and IO network contracts. Some utilities are preserved due to their usefulness.imports: modifies web named exports to provide esm and cjs exports instead of minified bundle. The web bundle was causing issues in bundled projects, and polyfills are no longer provided by default. Refer to the README for specifications on how to use the SDK for a web project.[1.2.2] (2024-07-11) View changes on GitHub Bug Fixes api: ensure timestamps are always in miliseconds (93b162f) [1.2.1] (2024-07-04) View changes on GitHub Bug Fixes io: default the IO process to use testnet (61bca5c) [1.2.0] (2024-07-03) View changes on GitHub Bug Fixes ant: add event emitter util for fetching ants (ee5287b) ant: fix read api and update types (977e0e3) ant: handle when no data is returned (1de6610) ants: separate out interfaces (60fd593) ant: update apis to implement interface (9c54db0) ant: update interface to expect undername instead of name for ant records (416cb3d) ao ant: add handler for get state (fd20aa7) ao reads: safely parse json (1ff5410) ao: add AR-IO-SDK tag to process interaction (e5b5603) ao: add default timestamp to getTokenCost (36fed1b) ao: add getPrescribedNames for epoch api (747fad2) ao: add retries to read interactions (67d59e2) ao: fix tag for join network, update observation response (556f5d5) ao: prune tags on joinNetwork (31978f9) ao read: fix interface to have ant getState api (4e95bbd) aos: update aos module id and lua id (e19139e) ao: support connection config params in AO (3e6a246) ao: support tags for all write interactions (67f8da9) ao: update APIs for ao interface to be more descriptive (f07ac36) ao: update epoch interfaces to support various inputs (ddc4c10) ao: update send on process to use proper signer and evalute result (4e2f65d) ao: update stake interface (427e8ba) ao: use types and connect config in ao process to wrap connect from ao (05b07cf) buy: require processId on buyRecord (cc5859f) deps: add eventemitter3 dep (1d50cd1) deps: use p-limit-lit to avoid jest issues (05e0673) emitter: add a end and some console logs in the example (bc4e6b8) emmiter: rename and move throttle to be variable powered (f9cf40d) epochs: fix epoch default timestamp (ffb9df7) events: return process ids on end of fetching (15e3f44) handlers: update handler names (720b178) io: add buyRecord API (30d5e74) io: add epoch-settings api and tests (56555ea) io: add init to provide custom process (8811016) io: separate out io/ao contract interfaces (d96fa59) io: update arns interactions on registry contract (9befe2a) pLimit: add pLimit for util to avoid ao throttling (5b13560) readds incorrectly removed descriptions (c77217a) revert purchasetype tag (2dc08df) spawn: add option state contractTxID to track where init state is from (1745766) tags: make remaining tags ans-116 compliant (d034c8c) tags: use updated ans-116 tag format for actions (261b788) timeout: increase timeout period on arns emitter (b5ddb5f) type: default to unknown return type for json (0bddce0) types: add ao ant state type (02dbacd) types: update some types for arns names and contract state (2d23241) updates to use IO class and process terminology (ec45d66) util: initial implementation of get ant process for wallet (885fa31) Features ant: add balance APIs to ant interface (ec67440) ant: add utility for fetchint ant modules owned by wallet (01f7ec9) ants: support ANT apis in SDK (b187aeb) ao utils: add spawn ant util (d02566e) ao: experiment with initial implementation of ao contract (6118cea) getInfo io: add getInfo method to io class (4ef25ec) IO: implement io/ao classes that call process apis (aab8967) [1.1.1] (2024-06-06) View changes on GitHub Bug Fixes api: default evaluation options on getArNSReservedNames api (0a1f22e) [1.1.0] (2024-06-03) View changes on GitHub Bug Fixes api: make evaluation options optional on the interface (9e5a1c0) api: remove unused variable for epochBlockHeight (98c5ebc) arweave: default to arweave.net (84c9653) axios: add back axios-retry (9aae4de) errors: throw AbortError on signal aborted (63bd395) getContracts: only implement util for now (6b29c2f) gql query: don't abstract the data protocol query (f0b8f77) imports: import type from base route warp-contracts (bf99a85) init: allow signer to be undefined and if so return readable (b6a05e2) init: fix type for init to allow undefined signer (0a64ea9) init: remove unnecessary destructuring (81af1af) interface: remove epochBlockHeight from interface (b646f08) types:remove DataItem from WriteInteractionResult (eadb1a1) types: use gql node interface for dataProtocolTransaction (79cebd9) warp: ensure contract init on read interactions (bc3d1b8) Features getContracts: add get contracts on network specific providers like WarpContract (603d36e) gql util: add smartweave gql utils (5ea3aab) write: add tags support to write interactions on warp-contract and saveObservations (46eb4c9) [1.0.8] (2024-05-29) View changes on GitHub Bug Fixes api: add getPriceForInteration api to ario contract (3b8083c) bundle: minify web bundle (9266676) api: use function map for method name (439ec1f) reserved: add reserved arns name get methods (ad203ef) signer: check if method is property of signer before using (c52783c) signer: modify signer to assume the signer type based on public key being undefined (b775c96) test: add dockerfile for running tests in certain node environments (86cf2ad) [1.0.7] (2024-05-23) View changes on GitHub Bug Fixes contract: add extendLease and increaseUndernameSupport apis (1b13b5e) types: fix the AtLeastOne type (ffd0869) deps: force arweavve to 1.15.1 (2448598) contract: make params required - properties and note (89db674) types: update tests and use overwrite type to allow mIOtoken for certain paramaters (badcece) api: change to increaseUndernameLimit (9b72c1e) docs: update ario apis (4af0862) tests: update extend test util to include a test domain (e959b7c) token: add mIO and IO token classes to exports (f47f7d5) types: add delegated gateway type (c877496) types: export the token types (dfc83ae) types: remove visible types (6ab1fc3) types: update Gateway delegates type to use the new GatewayDelegate (ac7e924) warp: bump warp version (db7344d) [1.0.6] (2024-05-07) View changes on GitHub Bug Fixes warp: bump warp to fix AbortError issue on warp imports for web (c9a5613) [1.0.5] (2024-05-02) View changes on GitHub Bug Fixes cjs: provide path alias for warp in cjs export (7f9bf9a) logger: replace winston with bunyan (0488f75) util: add FQDN regex that matches ArNS contract (e6d7396) utils: manally conver from b64 to b64url to avoid web polyfill issues (766035c) utils: use base64 for fromB64url util (42302ef) warp-contract: correctly throw error in write interaction (c2368dd) [1.0.4] (2024-04-30) View changes on GitHub Bug Fixes ario: update joinNetwork to accept observerWallet param (6a32dd1) [1.0.3] (2024-04-26) View changes on GitHub Bug Fixes signer: set owner before signing data (0b558f5) [1.0.2] (2024-04-25) View changes on GitHub Bug Fixes arweave: default to the arweave node import to avoid issues with browser environments (fc8c26e) cacheurl: use default cache url in warpcontract (a676a3c) init: cleanup init overload methods and tests (fa328d2) lint: address lint issue in ArIOWriteable (4a3ee89) tsconfig: modify some tsconfig settings to get isolated configs for web/cjs/esm (46b7acc) typeguards: make type guards accept unknowns (7f285bb) types: use generic types and modify the requirements for init functions (9350f78) utils: add writeInteraction types and update base64url logic (4f5476b) [1.0.1] (2024-04-23) View changes on GitHub Bug Fixes docs: improve README docs interface documentation for ArIO clients (b0da48c) [1.0.0] (2024-04-23) Bug Fixes actions: bump node setup action (4eb49cd) actions: freeze lockfile (dba7313) contract add cache config in ario constructor (1f3c0ba) ant: add ant contract to exports (a2ff57b) ant: add signer to ant test (4581b8d) ant: default evaluation options for ant apis that do not take an… (#25) (0c8b55d) ant: default evaluation options for ant apis that do not take another parameter (7c59033) ant: default evaluation options for apis that do not require them (72b57d5) ant: fix API for getRecords (c714aa3) apis: remove epoch from distributions and observations (7b2d279) arbundle version: pin version (35ffab6) arbundles: update arbundles import (f02d83f) ario: add cache config in ario constructor (#11) (ecb279d) ario: formatting (c61570a) ario: make state provider nullable and default to remote arns-service provider (fa1cb72) ario: re-add contract default config (2296cc3) ario: remove unused cache property (7f2d02e) build: add setImmediate polyfill for web only (ad36776) build: remove redundant exported type (134319b) cache: remove cache folder (2ac9427) cacheURL: update ario cache url setting pattern to use custom url appropriately (c76e67d) cache: validate arweave id before setting it (5ba1175) casing: revert to lower case casing (b5da0ab) comments: make class logger private, remove comments (7483246) connect: add init static function on ario class to create interaction classes (765f39c) contract configuration: return cache url as well (b4a7bc3) contract functions: correct contract function names (ad9bc56) contracts: add configuration view method and update types (4fae4a2) contracts: remove write method and type from remote contract (740d8b8) contracttxid: make contractTxID require in remote state cache instance (dc82d21) contracttxid: make contractTxID required in remote state cache instance (#10) (bf651bb) ctrl flow: remove else from control flow (4b3c4c2) deps: pin arweave (d39391c) deps: remove axios-retry, will implement later (0218e95) deps: remove extra crypto-browserify (9b42898) deps: remove warp-contracts-deploy from deps (9d4f9fa) docs: remove docs folder (47e8403) drywrite: throw on bad drywrite and continue if successful (5052c0a) eslintignore: remove old file names (415c163) eslint: remove eslint comments and use this signer (32530eb) esm: add polyfills for crypto (dd8fbfe) esm: add polyfills for crypto (#27) (553822c) example web: update ario instatiation (77c6842) example: escape quotes in packagejson for example package json (fb47de0) example: simplify example and remove unused method on remote cache (81637f8) examples: update comments and fix package.json (db7140b) examples: update examples to use devnet (cc037ac) examples: update examples with records methods, and balance methods (a2d2a02) exports: add arweavesigner and arconnectsigner to exports, clean up docs (c7860ed) exports: update exports in indices (f794437) exports: update package exports to have index in src folder (2cce9e3) files: clean git cache of duplicate casing (e9eaa2d) filters: punt filters (1c23cb3) fixture: add type to arns state fixture (5bcac32) formating: format (3f30f77) gar write: fix types and flow on gar write (f5e7774) gateway: update gateway settings to support autostake (82c6840) generics: use named generic (4b647f0) gitignore: remove cache from gitignore (2867abc) git: test fix with file casing issue (c3611ee) headers: use source-version for header (2b26d88) http: add headers sdk headers to http config (94810ed) husky: add commit hooks (885ce68) imports: update to use indexed imports from warp (1242568) indentation: fix indentation in examples (a266731) interface: removed filters and added base records types (849834d) interface: rename interface to ContractCache (2a0a765) jest: remove extra config (014fbde) lint: disable no-any warning certain types (de5f108) lint: formatting (21224e2) logger, errors, http: Updated to axios and axios-retry, added winston logger, more extensive custom error objects (b944f4d) logger: remove unused logger property (9501d1d) logs: removing debug logs (f025171) mixin: filter private methods in mixin util (beb8610) naming: change epoch to epochStartHeight (908971c) naming: rename getRecord[s] to getArNSRecord[s] (bd3d4bc) overloads: only accept warp contract as a contract config for ariowritable (e3c97e9) polyfills: rollback polyfill on logger (0cdb2f0) postinstall: remove husky postinstall script (c74a135) readme: add grammar and example recs (ecc07f7) readme: condense quick start (b35e5bd) readme: refactor api list to header tags (817d99b) readme: update ant header (77235ce) readme: update ANT usage description (70c8520) readme: update joinNetwork docs (9fcf440) readme: update quick start (a60d96a) readme: update readme with default provider example (68a5a16) readme: update readme with examples (d9ee23e) record records: update key to use result instead of record (90314db) records: remove contractTxId filter remove lodash shrink readme (50669e1) records: use state endpoint to fetch records (2f02c53) recs: modify the interfaces for contracts and implement with warp and remote service (#13) (56ebb08) release: remove release assets entirely (9d5a1b3) release: update github release config to publish packages to github (5534d9d) remote: getState not properly setting evalTo in http requests (55745c1) safety: update type safety checks (32eebbc) setimmediate: make set immediate a build dependency as it is required by the node winston (9292eaa) signer: check that contract is connected before trying to write (d352e9c) signer: check that contract is connected before trying to write (#29) (536a116) signer: fix signer in WarpContracts - update tests (ea9448f) signer: fix signer in WarpContracts - update tests (#32) (16d69d8) signer: remove jwk use, ignore web example for now (bc7e577) signer: remove signer, will do in other pr (d02276d) signer: remove use of JWK, simplify constructor (#22) (d2ef573) signer: update ANT to have signer (c7f8eee) structure: update cache provider folder to be named caches (844c1aa) structure: use snake case for file and folder names (37f27d3) test warp-contract: use beforeAll to read env vars (95cc019) tests: add test cases as a const (8458185) tests: add test for custom arIO client config (0e6142b) tests: change control flow pattern to.catch instead of trycatch (883de51) tests: dont make blockHeight or sortKey undefined but rather evalTo (f76a201) tests: instantiate new ant to connect in tests (9869415) tests: remove dryWrite from writeInteraction, update tests (bc1becc) tests: remove fixture and use live service for tests (30d3e8c) tests: test 404 response (590dea6) tests: update ario test (4208bd0) tests: update client instantiation test to check read vs write clients (059653c) tests: update docker compose params (a71befd) tests: update gateways test (1fcb3e6) tests: update stubs in tests (e4bbc6e) tests: update test to match jest syntax (553bdbb) tests: update tests for named prop expectation (4ea04a7) tests: update tests to use younger contract, add evalParams config (ae890c8) tests: update tests with constants and update types (1bdcfeb) tests: update tests with new name (2cd1b5c) tests: update with new names on methods (619c193) tests: use angela for testing (10f30fe) tests: use http not https in tests (fddba1e) tests: use process vars as priority url (faab4f3) test: update test to use ArweaveTransactionID class (f6c4f8b) tsconfig, names: reverted tsconfig to nodenext resolution, changed naming convention on provider, removed extraeneous error classes, rolled back axios-retry to match our tsconfig settings (d412d44) tyeps: set types to objects rather than top level params for easier readability (edfd77b) type: rename all type implementations (5959045) types and tests: update evalTo to allow undefined sortKey and block and test that (a59f05c) types: add @ to records (53601c1) types: make props nullable on certain read apis (f8ff552) types: remove any type (5c80242) types: remove any types (d8d910b) types: remove ArweaveTransactionID type for now (3adf53b) types: remove unnecesssary empty defaults (7d14edb) types: rename signer to ContractSigner (87d6c90) types: require atleast one param to update gateway settings (857ebdc) types: update interaction type to only use read for now (2c02e90) types: update tests, readme, and types (e9985dd) types: use partial write type (fa6a638) types: use string instead of any (014a262) validate id: make validator a private method (dce4a94) validity util: isBlockheight check more strict (2b28675) warp contract: added test for getting state after connecting with warp (060ee2c) warp-contract: provide logger - update isTransaction flow ctrl - use typed props (5f6e0a1) warp-contracts: bump warp to 1.4.38 - fixed warp exports (af4a20b) winston: move the winston polyfill - this will prevent any esm based web projects from getting polyfill issues (c8b7998) write: add dry run - sync state - abortSignal - update interface (970bdef) write: update utils - change error flow - update arweave constructor props (0a81c92) write: update write methods on warp (9c0540b) yarn: update lockfile (fd5e0ee) Features ant: add ANT read interface (c941c96) ant: create ant contract class for interacting with ant contracts (6eb7ef5) ants: add readable-writable framework to the ant client and implement write methods (3019f53) ario contract: add distributions and observation apis (21e38d1) arioContract: update ArIO interface and ArIOContract interface (5d87e2e) auctions: add auctions apis (faf08c5) contract: add distribution, observations apis, update readme and examples (0208317) contract: create new contract classes that impelement both warp and remote cache for ant contract and ar-io contracts (855da2d) first issue: setup examples, readme, and initial gateways provider (5a9e232) gar methods: add gar write methods to the ario client (e01b08b) inital providers: scaffold initial providers (4949514) io transfer: add transfer api to ario writable client (0d37623) observerations: add saveObservations write interaction (8dd977c) observers: add API for fetching prescribed observers (a18e130) observers: add API for fetching prescribed observers (#17) (17ce6de) PE-5742: add records api to arns remote cache (#8) (c46cd39) PE-5751: add blockheight and sortkey eval filters (#12) (832a1ad) PE-5758: add signer to ario class (#20) (1b82077) PE-5759: observations and distributions apis (#16) (dded361) PE-5773: add auctions read apis (#18) (e0c6fca) PE-5800: add epoch apis (48ee4ba) PE-5800: epoch apis (#15) (70563b1) PE-5825: ANT read interface (#19) (6a0c477) records: add records api to arns remote cache (1b7f54f) signer: add arweave signer to ario class (7e08097) write: add write interface and base implementation on warp-contract (6dfc969)

---

# 94. Arlink Deploy - ARIO Docs

Document Number: 94
Source: https://docs.ar.io/build/guides/arlink
Words: 462
Extraction Method: html

Overview Arlink is a third party tool that allows you to permanently deploy and manage web apps on the permaweb with ease.How it works Users can link their Github or Protocol.land repositories to their Arlink account through the Arlink dashboard. When a new project or build is deployed,
Arlink will take the repository, build it, and upload the build folder to Arweave.Arlink also allows users to connect their project to an ArNS name they own, or an undername of the ArNS name ar://arlink.Dashboard After connecting your wallet to the Arlink web app using the button at the top right, you will be taken to your dashboard. This page will display any deployments associated with your wallet, and includes a "+ New Deployment" button
in order to start the process of deploying a new project. New Deployment After clicking on the new deployment button, you will be prompted to import a repository from either Github or Protocol.land. Authorize Github If this is your first time importing from Github, you will be prompted to authorize Arlink to access your Github repositories. You can authorize all repositories, or limit authorization to any number of specific ones. Select Repository Once authorization is approved, select which repository and branch you want to deploy. Define Build and Output Steps Once you select what you want to deploy, you need to specify how the project needs to be built to get it ready. Arlink prompts for five inputs:Project Name: This is the name of your project.Install Command: The command for installing dependencies for your project. Usually npm install or yarn install Build Command: This is the command to run your build script. Usually npm run build or yarn build Sub Directory: If the front end for your project lives in a sub directory of your selected repository, you can specify that here.Output Directory: This is the path to the build folder being deployed. This will be different depending on the framework your project uses. Select ArNS The last thing to do is select an ArNS name to deploy your project to. If you own your own name, you can connect to it here with the "Use existing ArNS" toggle. Otherwise, you can select an undername of the ArNS name arlink to deploy to.
Duplicate undernames cannot exist, so you can only select an undername that is not already being used. Logs Once you select your ArNS name and click "Deploy", your project will be deployed. Logs from the build and deploy process will be displayed so you can monitor for errors. Updates To deploy a new build of your project, select it from the dashboard. The project page gives you the option to update any settings or configurations, and has a "Deploy Latest" button which will redeploy your project.

---

# 95. ARIO Docs

Document Number: 95
Source: https://docs.ar.io/build/guides/gql
Words: 1295
Extraction Method: html

GraphQL Overview GraphQL is a powerful query language designed for modern web applications to efficiently fetch data. It enables precise queries, allowing users to specify exactly which data they need and in what format, significantly reducing the amount of unnecessary data transferred. This approach is ideal for dealing with complex systems and large datasets, as it minimizes bandwidth and improves performance. GraphQL operates through a single endpoint, streamlining the way applications communicate with databases.The integration of GraphQL with Arweave introduces a refined method for interacting with decentralized data storage. Arweave allows for the tagging of uploaded data, facilitating enhanced searchability and retrievability within its blockchain network. Utilizing GraphQL, users can perform targeted queries that leverage these tags, ensuring the retrieval of specific data swiftly and efficiently. This capability is particularly beneficial for the development of decentralized applications (dApps), the archival of content in a permanent and unalterable form, and the establishment of data marketplaces where precision and efficiency in data access are paramount.Together, GraphQL and Arweave form a compelling combination, offering developers and users a robust framework for managing and querying data in a decentralized environment. This integration not only promotes the efficient and scalable retrieval of data but also supports the creation of more sophisticated and data-intensive applications on the decentralized web, maintaining a balance between technical depth and accessibility.Constructing a Query Basic Syntax In GraphQL, you start with a root field and use braces to outline the fields you want to retrieve, allowing for precise, hierarchical data requests. For instance:This query demonstrates fetching transactions and their tags, illustrating the hierarchical nature of GraphQL queries.Customizing Searches with Tags Arweave utilizes a tagging system for transactions, enabling intricate search capabilities. You can filter queries using these tags:This example filters transactions by a specific application name, and returns the id, size, and type of the transaction, showcasing how to customize queries for targeted data retrieval.NOTE: Tags are not the only option for filtering results, but are extremely useful due to the ability to add custom tags during the upload process.Understanding Edges and Nodes In the realm of GraphQL queries, especially when interfacing with Arweave, grasping the concept of edges and nodes is pivotal for constructing efficient and effective queries. This structure is not unique to Arweave but is particularly relevant due to the decentralized and interconnected nature of the data stored on its blockchain.Nodes: At the heart of GraphQL's query structure, nodes represent individual data points or entities. In the context of Arweave, a node could be a transaction, a block, or any piece of data stored within the network. Nodes are the primary targets of your query, containing the data you wish to retrieve, such as transaction IDs, tags, or the content of data transactions.Edges: Serving as the glue between nodes, edges are constructs that outline the relationship between different nodes. They can contain metadata about the connection, such as the nature of the relationship or additional attributes that describe how nodes are linked. In many GraphQL implementations, including those that interact with Arweave, edges are used to navigate through collections of related data, making them crucial for understanding the data's structure and lineage.This hierarchical model is especially useful for querying complex and relational data sets, allowing for detailed navigation and efficient data retrieval within Arweave's decentralized storage system. By effectively utilizing the edges and nodes structure, you can precisely target the data you need, whether it's filtering transactions by tags, fetching related transactions, or exploring the blockchain's structure.Pagination To add pagination to your GraphQL queries, you can use the first, last, before, and after parameters. These parameters control the slice of data you're querying, making data retrieval more efficient and manageable.first: Specify the number of items to retrieve from the start of the list or dataset.last: Specify the number of items to retrieve from the end of the list or dataset.This query fetches the first 10 transactions.To navigate through your dataset, you can use after and before in conjunction with first or last. These parameters accept cursors, which are typically provided in the response of your initial query.after: Fetch items after the specified cursor, used with first.before: Fetch items before the specified cursor, used with last.This query fetches the next 10 transactions following the transaction with the cursor "cursorOfLastItem".If no pagination terms are set, GraphQL servers may apply default limits to prevent excessively large datasets from being returned in a single query, potentially impacting performance. The default behavior can vary based on the server's configuration but often involves returning a predefined maximum number of items.For instance, without specifying first or last, a query to the transactions field might return the first 5-10 transactions by default, depending on the server settings.This behavior ensures that server resources are not overwhelmed by large requests and that client applications receive data in manageable chunks.General Tips for Optimizing Queries To optimize your GraphQL queries in Arweave, follow these general guidelines:Specificity: Query with the most precise tags possible to narrow the search scope and enhance performance.Minimalism: Limit your query to the essential set of tags to reduce processing time and data transfer.Schema Design: Design your app's schema to reflect query patterns, possibly introducing tags that encapsulate frequent combinations of criteria.Include Non-tag Fields: Adding fields like owner can refine your search, making your queries more efficient.Order Your Tags: Arrange tags from most specific to most general to leverage Arweave's indexing more effectively.By incorporating these strategies, developers can achieve faster and more precise data access from Arweave, enhancing the performance and responsiveness of decentralized applications. This balanced approach to query construction and optimization is key to navigating the expansive and decentralized storage landscape Arweave provides.Making a Query Executing GraphQL queries within the Arweave ecosystem offers flexibility and multiple avenues for developers and users alike. Whether you prefer a hands-on, manual approach to constructing and testing queries, or you aim for automation and integration within your applications, Arweave provides the tools necessary to interact with its decentralized data storage seamlessly.GraphQL Playground For those new to GraphQL or seeking to fine-tune their queries before implementation, the GraphQL playground offers an invaluable resource. This interactive interface allows users to manually construct queries, explore the schema, and immediately see the results of their queries. Accessible via web browsers, the playground can be found at the /graphql endpoint of most Arweave indexing services, such as https://arweave.dev/graphql. Here, you can experiment with different queries, understand the structure of the data, and refine your approach without writing a single line of code in your application.Steps for Accessing the GraphQL Playground:Navigate to https://arweave.dev/graphql, or the graphql endpoint of any AR.IO gateway, in your web browser.Enter your GraphQL query in the provided interface.Press the "play" button to execute the query to see real-time results and debug as needed.Using an API For application development and automation, making GraphQL queries programmatically is essential. You can send POST requests directly to the GraphQL endpoint of any indexing service that supports it, such as arweave.net or any AR.IO gateway. These requests should contain your query in the body, allowing for dynamic and automated data retrieval within your application.When selecting an indexing service, consider the data coverage and reliability of the gateway to ensure it meets your application's needs. Different gateways might have varying degrees of indexed data available, so choosing one that is consistently up-to-date and comprehensive is key.Example of making a programmatic query:Using an SDK For an even more integrated experience, some Software Development Kits (SDKs) offer direct methods for executing GraphQL queries. The Arweave SDK, for example, provides built-in functionalities to interact with the blockchain, simplifying the process of making queries. By leveraging these SDKs, developers can bypass the intricacies of manual HTTP request construction, focusing instead on the logic and design of their applications.Example of using the Arweave SDK for GraphQL queries:

---

# 96. Managing Undernames - ARIO Docs

Document Number: 96
Source: https://docs.ar.io/build/guides/managing-undernames
Words: 302
Extraction Method: html

Overview ArNS undernames are subdomains of top level ArNS domains. They are separated from the main ArNS domain using an underscore "_" in place of the more typically used dot ".".Records for undernames can be set using the setRecord method on the AR.IO SDK, or removed by using the removeRecord method.
The process for setting/removing a record for an undername vs. a top level ArNS domain is nearly identical, the only difference being the undername parameter. When managing a record on a top level ArNS domain, this must be set to @, while updates to an undername should provide the undername being updated.Chaining Undernames Undernames can be created on other undernames, for example ar://og_logo_ardrive. In this example the undername og exists under the undername logo on the ArNS name ardrive.For the purpose of the undername parameter in the AR.IO SDK, this should be written as a single undername, including the separating underscores:og_logo Creating an Undername There are no special steps required to create an undername (provided the selected ArNS name has available undername space). Simply setting a record for an undername that does not exist will create the undername.Updating an Undername If an undername already exists, its record can easily be updated using the same setRecord method.Removing an Undername An existing undername can be removed by using the removeRecord method on the AR.IO SDK.
The undername parameter should be set to the undername being removed.Increasing Undername Support By default, ArNS names support up to 10 undernames. This number can be increased, for a fee. This is done using the increaseUndernameLimit method on the ARIO class of the AR.IO SDK, rather than the ANT class.
The quantity (qty) parameter specifies the number of ADDITIONAL undernames to be supported. i.e. increasing from 10 undernames to 15 would require the qty parameter set to 5.

---

# 97. ARIO Docs

Document Number: 97
Source: https://docs.ar.io/build/guides/permaweb-deploy
Words: 1060
Extraction Method: html

Deploy a Website or Application Overview With the growing popularity of permanently deployed apps, hosted on Arweave, along with the growing list of tools offered by AR.IO, several methods have been developed to automate the process of deploying a website and updating the ArNS name pointed at it. A particularly useful tool for this is permaweb-deploy from Forward Research.permaweb-deploy is a cli tool that handles uploading a build folder to Arweave using Turbo, creating a manifest, and then updating an ArNS name to point at the new manifest. It being a cli tool makes it very easy to incorporate into a github actions flow. Setting up an automated deployment with permaweb-deploy is simple, but does require a few steps.ENV Security Before automating your deployments, be sure to build your app and check for exposed environmental secrets. Some app frameworks or build flows will build your app with the secrets exposed, and if you are using a tool like permaweb-deploy, those secrets will be uploaded to Arweave. Since the permaweb is permanent, this could pose a security risk, especially with a copy of your wallet keyfile required for the deployment automation.Getting Started Installing package permaweb-deploy is an npm package, and must be installed in any project before it can be used. If you are using npm, you can install the package with the below command:If you prefer yarn for your package installations, the process is slightly more involved. permaweb-deploy is not designed for installation with yarn, so you must provide the additional argument ignore-engines in order to skip over the yarn version error you would normally get with installation. There are two methods for doing so:Directly in the install command In a .yarnc file You can provide a file, named .yarnc in the same directory as your package.json in order to assign specific instructions to all of your yarn commands. Creating a .yarnc file with the line will have the same effect as providing the flag directly in your yarn command Adding a Deploy Script The simplest way to utilize the permaweb-deploy tool is to build it into a script in your package.json. Here you will provide all of the variables that permaweb-deploy needs in order to function properly, as well as ensure that your app is statically built before being uploaded.Be sure to replace <YOUR_ARNS_NAME> with the name of the ArNS name you want to deploy to.The above example shows a build script for a vuepress app, which will build the app into a static folder for deployment, and a deploy script which runs build and then permaweb-deploy. Your build script will look different depending on the framework you are using, but most will provide that for you when you create your app.The permaweb-deploy command has two required arguments:--deploy-folder This is the relative path (from your package.json) to the build folder you want to upload. In a vuepress app, that will be ./src/.vuepress/dist unless you manually specify otherwise in your vuepress configuration. It will be different depending on your chosen framework and if you have modified the default location.--arns-name This is the ArNS name you want to deploy to. It must be an ArNS name that the wallet used to authenticate has ownership or controller privileges over, otherwise the deployment will fail at authentication in the ao process that controls the ArNS name.Undernames The --arns-name flag MUST be the top level name, not and undername. That is, if you want to deploy to undername_arnsname you must set --arns-name arnsname and not --arns-name undername_arnsname.There is the additional, optional flag --undername. If you want to deploy your app to an undername on an ArNS name, provide that name with this flag.--arns-name arnsname --undername undername Testnet Permaweb-deploy supports both Mainnet and Testnet deployments. By default, it will deploy to Mainnet. To deploy to Testnet, you can provide the --ario-process flag as "testnet". If not provided, deployments will default to Mainnet.Providing Arweave Wallet Keys While using permaweb-deploy, you will be uploading data to Arweave using Turbo, as well as performing protected actions on an Arweave Name Token. Because of this, you will need to provide the keys to an Arweave wallet in order for the actions to be successful. The wallet must contain Turbo Credits to pay for the upload, and it must either be a controller or the owner of the ArNS name you are trying to update.permaweb-deploy requires your wallet keyfile be encoded in base64 format. You can convert a local keyfile to base64, and copy the new value to your clipboard by using one of the below commands, depending on your operating system:Linux Mac Windows (CMD) Be sure to replace wallet.json with the path to your chosen wallet keyfile. Once you have this value saved to your clipboard, you can move on to the next step.Create Github Secrets Anyone who has your wallet keyfile (including the base64 formatted keyfile) has full control over your wallet and any of its assets. Because of this, you do not want to include it directly in your package.json script. Instead, keep the value safe by storing it in a github secret. You will create the secrets in the settings tab on your github repo, and the secrets will act as environmental variables in the github actions workflow.You will need to create 1 secret DEPLOY_KEY: This is the base64 encoded version of your Arweave wallet keyfile.Create Action Workflow Github Actions allow you to perform specific actions whenever you push code to github. They are handled by using .yaml files provided in <root-of-project>/.github/workflows.To get started, create a new file named deploy.yaml in the workflows directory, then paste the below inside of it:The above tells github to perform these actions when you push new code to the branch main It then sets up a vps with nodejs v 20. When that is complete, it installs dependencies for your project using npm (You will need to add a step to install yarn if that is your preferred package manager), and runs your deploy script, which builds your static folder and then runs permaweb-deploy. It also loads your github secrets into environmental variables that can be used by your deploy script.Deploying App With the above setup complete, the only thing you need to do to deploy a new version of a permasite app to Arweave is push the updated code to branch main on github. Everything else is fully automated.

---

# 98. Gateway Architecture - ARIO Docs

Document Number: 98
Source: https://docs.ar.io/gateways
Words: 650
Extraction Method: html

Overview Gateways are the workhorses of the AR.IO Network.
Their primary role is to act as a bridge between the Arweave network and the outside world.
This means that a gateway's main task is to make it easier for users to interact with the Arweave network by simplifying the technical processes of writing, reading, and discovering data on the blockweave in a trust-minimized fashion.Gateway functions The functions of an AR.IO gateway are broken down into the following categories:Writing data involves:Proxying base layer transaction headers to one or more healthy and active Arweave nodes (miners) to facilitate inclusion in the mempools of as many nodes as possible.Proxying chunks for base layer Arweave transactions to Arweave nodes to help facilitate storage and replication of the chunks on the blockweave.Receiving and bundling so-called bundled data items (e.g., ANS-104 spec) as base layer transactions.Reading involves retrieving:Transaction headers for a base layer Arweave transaction.Individual data chunks for a base layer Arweave transaction.Blocks from the blockweave.Storage pricing rates for data from the Arweave node network.Contiguous streams of chunks representing an entire base layer transaction.Bundled data items (e.g., ANS-104).Wallet information (e.g., token balance).Discovering data involves:Facilitating efficient, structured queries for base layer transactions, bundled data items, and wallet data by:examining incoming streams of data (i.e., directly ingested transactions and data items, blocks emitted by the chain, etc.).managing index data in a database or analogous data store.Parsing and executing user queries.Facilitating friendly-path routing via Arweave manifest indexing.Including other benefits and capabilities such as:Facilitating friendly-subdomain-name routing to Arweave transactions via a direct integration with the Arweave Name System (ArNS).Providing the modularity and configurability necessary for operating extensible gateways that can be deployed at small or large scales to meet the needs of specific applications, use cases, communities, or business models.Providing pluggable means for consuming telemetry data for internal and external monitoring and alerting.Facilitating configurable content moderation policies.Providing connectivity to a decentralized network of other AR.IO gateways, enabling data sharing and other shared workloads.AR.IO Gateway Benefits AR.IO gateways provide many new benefits and capabilities beyond general Arweave gateways:Providing the modularity and configurability necessary for operating extensible gateways that can be deployed at small or large scales to meet the needs of specific applications, use cases, communities, or business models.Providing pluggable means for consuming telemetry data for internal and external monitoring and alerting.Facilitating friendly-subdomain-name routing to Arweave transactions via a direct integration with the Arweave Name System (ArNS).Facilitating configurable content moderation policies.Providing connectivity to a decentralized network of other AR.IO gateways, enabling data sharing and other shared workloads.Gateway Modularity A design principle of AR.IO gateways is that their core components should be interchangeable with compatible implementations.The core services in the gateway are written in Typescript, with flexible interfaces to the various subsystems and databases. This allows operators to customize their gateway to meet their specific requirements. Gateway services can be turned on or off depending on the operator's needs. For example, an operator might choose to have their gateway serve data, but not actively index Layer 2 bundled data. This flexibility also allows operators to utilize the technologies that are appropriate for the scale and environments in which they operate.For example, small scale operators might want to use low-overhead relational databases to power their indexing while larger scale operators might opt to use cloud-native, horizontally scalable databases. Analogous examples for storage and caching exist as well.ARNS Indexing and Routing The Arweave Name System’s (ArNS) state is managed by the ARIO token’s smart contract. AR.IO gateways shall perform the following minimum functions relative to ArNS:Actively track state changes in the contract.Maintain up-to-date indexes for routing configurations based on the state of the ARIO contract as well as the states of the Arweave Name Token (ANT) contracts to which each name is affiliated.Manage the expiration of stale records.Facilitate ArNS routing based on the subdomains specified on incoming requests where appropriate.Provide a custom HTTP response header for ArNS requests indicating the corresponding Arweave transaction ID.

---

# 99. ARIO Docs

Document Number: 99
Source: https://docs.ar.io/gateways/advanced
Words: 783
Extraction Method: html

Advanced Configuration Overview The Getting Started guides for windows and linux contain all the information needed to start your AR.IO Gateway node successfully with basic configurations. There are also ever expanding advanced configuration options that allow you to run your node in a way that is customized to your specific use case.Most of the below options can be added to your .env file in order to customize its operation. Any changes made to your .env  require you to stop the docker containers running your node, and restarting them with the --build flag in order for the changes to take effect. See ENV for a complete list of environmental variables you can set.Data Storage Location You can set a custom location for your AR.IO Gateway to save the data it pulls from the Arweave network. There are three primary types of data stored, and you can set a unique storage location for each of these independently. These are "chunks data", "contiguous data", and "headers data". The custom location for each of these can be set in your.env file like this:Be sure to replace "<file path>" with the path to the location where you would like the data stored. If these values are omitted, the data will be stored in the "data" directory inside your Gateway code repository.Admin API Key HTTP endpoints under "/ar-io/admin" are protected by an admin API key. These endpoints allow you to get certain analytics data or make adjustments to your node as it's running. When your node starts, it reads your environmental variables to see if a key is set. If not, a random key is generated. The key name is ADMIN_API_KEY and it should be set in your .env file like this:ADMIN_API_KEY=SUPER_SECRET_PASSWORD View examples of the admin endpoints here Wallet Association In order to participate in the greater AR.IO network, Gateway nodes need to associate themselves with an Arweave wallet. This can be configured by setting the AR_IO_WALLET key value in your .env file.AR_IO_WALLET=1seRanklLU_1VTGowDZdD7s_-7k1qowT6oeFZHUZiZo Unbundling AR.IO Gateway nodes support unbundling and indexing ANS-104 bundle data. This is disabled by default, but can be turned on with several different configuration options. You can set these configurations with the ANS104_UNBUNDLE_FILTER and ANS104_INDEX_FILTER keys in your.env:The following types of filters are supported:Content Moderation You are able to set your Gateway to block specific transactions or data-items you don't want to serve. Unlike previous configuration options in this list, blocking content can be achieved without the need to add to your.env file and rebuild your Gateway. Instead, make a PUT request to your Gateway at /ar-io/admin/block-data. As this is an admin endpoint, you will need to have configured your ADMIN_API_KEY. Using curl as an example, the request should be formatted as follows:id (string):  This will be the transaction ID of the content you want to add to your block list.notes (string): Internal notes regarding why a particular ID is blocked.source (string): Identifier of a particular source of IDs to block. (e.g. the name of a block list) notes and source are used for documentation only, and have no effect on your block list itself.Contiguous Data Cleanup Transaction data on Arweave is stored in a chunked manner. It is commonly retrieved, however, in the the transaction data's original, contiguous form with all of its component chunks assembled end-to-end. Gateways cache contiguous representations of the transaction data to assist in various workloads, including serving transaction data to clients, allowing for efficient utilization of valuable system resources. Gateway operators will need to determine for themselves the best balance between disk space and other resource usage based on the size of their gateway and their particular use case.Contiguous data cache cleanup can be enabled using the CONTIGUOUS_DATA_CACHE_CLEANUP_THRESHOLD environmental variable. This variable sets the number of seconds from the creation of a file in the contiguous data cache after which that file will be deleted. For example:CONTIGUOUS_DATA_CACHE_CLEANUP_THRESHOLD=10000 will clear items from the contiguous data cache after ten thousand (10,000) seconds.ArNS Resolver Gateways, by default, forward requests to resolve ArNS names to arweave.dev. Starting with Release 9 gateways can instead build and maintain their own local cache. Doing so removes external dependencies and allows faster resolution.View the code for the ArNS resolver service here: https://github.com/ar-io/arns-resolver NOTE: The ArNS resolver is still an experimental feature. It is possible it may behave in unexpected ways when presented with rare edge case scenarios.In order to enable the local ArNS resolver, three environmental variables will need to be set:RUN_RESOLVER is a boolean representing an on/off switch for the local resolver.TRUSTED_ARNS_RESOLVER_TYPE sets the method the gateway uses for resolving ArNS names. Use resolver for the local resolver, or gateway for default functionality.TRUSTED_ARNS_RESOLVER_URL is the url a gateway will use to request ArNS name resolution.

---

# 100. Normalized Addresses - ARIO Docs

Document Number: 100
Source: https://docs.ar.io/concepts/normalized-addresses
Words: 618
Extraction Method: html

Overview Different blockchains use different formats for the public keys of wallets, and the native addresses for those wallets. In most cases, when a system in the Arweave ecosystem needs to display the wallet address of a wallet from a different blockchain, for instance in the Owner.address value of an AO process spawned by an ETH wallet, that address will be normalized into the format recognized by Arweave. Specifically, a 43 character base64url representation of the sha256 hash of the public key. This is done to prevent potential errors by systems in the Arweave ecosystem that expect these values to be a certain size and conform to a specific format.Essentially, normalized addresses are a way to represent public keys and wallet addresses from other blockchains in a way that is familiar to systems in the Arweave ecosystem.A tool for easily obtaining a normalized addresses from public keys can be found at ar://normalize-my-key At A Glance Public Keys and Addresses Crypto wallets consist of two separate components. The public keys, which are public knowledge and can be seen by anyone, and the private keys, which only the owner of a wallet should have access to. Crypto wallet addresses are derived from the public key.Encoded Public Keys It is important to note that all crypto wallet public and private keys are
binary data. The values provided below for Arweave and Ethereum/Polygon public
keys are base64url and hex encoded representations of that binary data
respectively.Arweave The public key for an Arweave wallet is the n field of the JWK json file.0jkGWDFYI3DHEWaXhZitjTg67T-enQwXs50lTDrMhy2qb619_91drv_50J5PwrOYJiMmYhiEA5ojMvrrAFY-Dm1bJbJfVBU1kIsPho2tFcXnbSOa2_1bovAys0ckJU07wkbmIUpzp3trdxYReB4jayMMOXWw9B8xS0v81zFmK3IbCtL9N6WNTMONOSMATHFQrGqtDhDUqKyIsQZCBPFvfGykRWaLWzbtAUrApprqG9hfExQzppNsw0gsftNSHZ1emC5tC2fuib6FhQw9TE2ge9tUjEZNALcVZvopTtTX0H2gEfnRJ48UNeV3SKggjXcoPVeivmqXuPBGncXWWq1pHR-Xs4zSLA5Mgcw_tQJc4FIER0i7hUlZXoc991ZHyOvAC-GlHWzQwvrlY11oD38pB47NkHN2WVPtUCAtyYQe5TE6Xznd9kPgqqvVUkV0s0suh5vINGoiPEnMjyhYEN7eOmJRIJ_A87IJesbdPRV4ZzBsqPbd02RG3ZuVpc3gI1xKvwH1WS05XI8eWK-BbvB3oxB7WjaQTWcfBWhMEULiwx-SucuyAzPAw3i6Wjtq61TcL9SdWhmOf9_yo-Np052tj7MQ66nmgdOH_MEKYjAdFypxTsRQoSLbv28HEcSjwx8u3pY0q0gKMK_5X2XKJrp2i2GB_fVgbcpH9YsgrYxh1Q8 The public wallet address for that wallet is 9ODOd-_ZT9oWoRMVmmD4G5f9Z6MjvYxO3Nen-T5OXvU, this is obtained by decoding the public key from base64url to normalize padding, sha256 hashing the result, and then base64url encoding that.Ethereum/Polygon The public key for an EVM wallet (Ethereum, Polygon/Matic) is derived from its private key, using the Elliptic Curve Digital Signature Algorithm, or ECDSA.0xb5d96e5533334a630af9d50b226011d44b9879c3165ffee0601bb0bac621e0047c302d4b72e4b1ca145043940c53093021825726cacdbf1d0a0e8ff2e70a4037 The public wallet address is 0x084af408C8E492aC52dc0Ec76514A7deF8D5F03f, this is obtained by removing the first byte from the public key, Keccak-256 hashing the remainder, taking the the last 20 bytes (40 hexadecimal characters) and prepending 0x to it.Solana A Solana wallet is an array of 64 bytes. The first 32 bytes are the private key, and the last 32 bytes are the public key. Below is the public key portion of a Solana wallet:[172, 175, 23, 95, 23, 124, 38, 171, 25, 20, 245, 213, 59, 9, 18, 89, 46, 70, 135, 84, 137, 205, 251, 95, 8, 226, 233, 46, 78, 34, 212, 86] The public wallet address for this wallet is Cd5yb4mvbuQyyJgAkriFZbWQivh2zM68KGZX8Ksn1L85, this is derived by base58 encoding the public key bytes.Normalizing Addresses As shown in the above examples, the format of public keys, and the resulting derived wallet addresses, vary widely between blockchains. Arweave manages this by applying the same derivation methods that Arweave uses for its own wallets to the public keys from other chains.Ethereum/Polygon The leading 0x and uncompressed flag 04 (if present) is removed from the public key of an EVM wallet, and then the remainder is base64url encoded to obtain the Arweave normalized public key. Continuing with the same public key in the above example, the normalized public key would be:2W5VMzNKYwr51QsiYBHUS5h5wxZf_uBgG7C6xiHgBHwwLUty5LHKFFBDlAxTCTAhglcmys2_HQoOj_LnCkA3 This value is what is used as the GraphQL tag owner value for data items being uploaded to Arweave using an EVM wallet. The normalized address is then derived from this value by sha256 hashing it, and then base64url encoding the result:5JtuS4yOFtUX2Rg3UU7AgBaUqh4s8wyyNTZk9UrzI-Q Solana The normalized public key for Solana wallets are derived similarly. The 32 byte public key is base64url encoded:rK8XXxd8JqsZFPXVOwkSWS5Gh1SJzftfCOLpLk4i1FY Again, this value is used for the GraphQl tag owner when uploading data. It can then be sha256 hashed, and base64url encoded again to derive the normalized address:K8kpPM1RID8ZM2sjF5mYy0rP4gXSRDbrwPUd9Qths64

---

# 101. AO Compute Unit (CU) - ARIO Docs

Document Number: 101
Source: https://docs.ar.io/gateways/cu
Words: 1013
Extraction Method: html

Overview An AO Compute Unit (CU) is a critical component in the AO ecosystem responsible for executing AO processes and maintaining their state. CUs serve as the computational backbone of the AO network by:Processing Messages: CUs receive and process messages sent to AO processes Executing WASM Modules: CUs run the WebAssembly (WASM) code that defines process behavior Maintaining State: CUs track and update the state of AO processes Creating Checkpoints: CUs periodically save process state to the Arweave network as checkpoints Running a CU alongside your gateway allows you to:Process AO requests locally rather than relying on external services Improve response times for AO-related queries Contribute computational resources to the AO network Ensure your gateway has reliable access to AO functionality For more detailed information about Compute Units, please refer to the AO Cookbook: Units.System Requirements Before deploying a CU, ensure your system meets the following requirements:Recommended: At least 16GB RAM for optimal CU operation Minimum: 4GB RAM is possible with adjusted memory limits (see resource allocation settings) At least 100GB disk space dedicated to CU operation These requirements are separate from your gateway requirements Running a CU is resource-intensive. Make sure your system has sufficient resources to handle both the gateway and the CU. While you can run a CU with less than the recommended RAM, you'll need to adjust the memory limits accordingly.Deploying an AO CU Step 1: Navigate to Gateway Directory First, navigate to the root directory of your gateway:Step 2: Configure Environment Variables Copy the example environment file:Default.env.ao.example Contents The default .env.ao.example file contains the following settings:These default settings are configured to work with a gateway running on the same machine, but you'll need to modify them as described below.Open the .env.ao file in your preferred text editor:Configure the following settings:CU_WALLET: Replace '[wallet json here]' with the JSON from an Arweave wallet.The entire JSON must be placed on a single line for proper registration.PROCESS_CHECKPOINT_TRUSTED_OWNERS: This is a comma-separated list of trusted wallet addresses:PROCESS_CHECKPOINT_TRUSTED_OWNERS=fcoN_xJeisVsPXA-trzVAuIiqO3ydLQxM-L4XbrQKzY Adding Your Own Wallet If you are uploading your own checkpoints, you should add your own CU wallet address after the default value, separated by a comma:PROCESS_CHECKPOINT_TRUSTED_OWNERS=fcoN_xJeisVsPXA-trzVAuIiqO3ydLQxM-L4XbrQKzY,YOUR_WALLET_ADDRESS_HERE This allows your CU to trust checkpoints from both the official source and your own wallet.GATEWAY_URL: By default, this is set to use your own gateway:GATEWAY_URL=http://envoy:3000 A gateway must be set to index all ANS-104 data items from AO or the CU will not operate properly. Most users will want to set this to:GATEWAY_URL=https://arweave.net UPLOADER_URL: By default, this is set to use a bundler sidecar run by your gateway:UPLOADER_URL=http://envoy:3000/bundler Important: Checkpoint Uploads Require Payment Checkpoints are uploaded to Arweave, so the upload must be paid for. You must ensure your wallet has sufficient funds:If using https://up.arweave.net (recommended), your CU_WALLET must contain Turbo Credits If using your own bundler or another service, you'll need the appropriate token (AR or other) Without proper funding, checkpoints will fail to upload and your CU may not function correctly The simplest option for most users is to use:UPLOADER_URL=https://up.arweave.net This requires your CU_WALLET to contain Turbo Credits.Optional: Disable Checkpoint Creation: If you want to disable checkpoint uploads, add:DISABLE_PROCESS_CHECKPOINT_CREATION=true Example of a Completed.env.ao File Here's an example of what your completed .env.ao file might look like with common settings:After making your changes, save and exit the nano editor:Press Ctrl+X to exit Press Y to confirm saving changes Press Enter to confirm the filename Optional Resource Allocation Settings You can fine-tune the CU's resource usage by adding these optional environment variables:PROCESS_WASM_MEMORY_MAX_LIMIT: Sets the maximum memory limit (in bytes) for WASM processes.Important Memory Requirement To work with the AR.IO process, PROCESS_WASM_MEMORY_MAX_LIMIT must be at least 17179869184 (16GB).Note: This doesn't mean your server needs 16GB of RAM. This is the maximum memory limit the CU will support for processes. Most processes don't use their maximum allocated memory.You can set this value to 16GB even if your server only has 4GB of RAM. However, if a process requires more memory than your server has available, the CU will fail when evaluating messages that need more memory.WASM_EVALUATION_MAX_WORKERS: Sets the maximum number of worker threads for WASM evaluation.Worker Thread Configuration This will default to (available CPUs - 1) if not specified. If you're running a gateway and unbundling on the same server, consider setting this to 2 or less to avoid overloading your CPU.PROCESS_WASM_COMPUTE_MAX_LIMIT: The maximum Compute-Limit, in bytes, supported for ao processes (defaults to 9 billion) PROCESS_WASM_COMPUTE_MAX_LIMIT=9000000000 NODE_OPTIONS: Sets Node.js memory allocation for the Docker container.Resource Tuning Start with conservative values and monitor performance. You can adjust these settings based on your system's capabilities and the CU's performance.Step 3: Start the CU Container Once your environment file is configured, start the CU container:This command uses the following flags:--env-file .env.ao: Specifies the environment file to use -f docker-compose.ao.yaml: Specifies the Docker Compose file to use up: Creates and starts the containers -d: Runs containers in detached mode (background) Step 4: Check the Logs To check the logs of your CU container:This command uses the following flags:-f: Follows the log output (continuous display) --tail=20: Shows only the last 20 lines of logs Exit the logs by pressing Ctrl+C.Connecting Your Gateway to the CU To make your gateway use your local CU:Add the following line to your gateway's .env file:AO_CU_URL=http://ao-cu:6363 This assumes the CU is running on the same machine as the gateway.Restart your gateway:Accessing Your CU Once properly set up and connected to your gateway, you can access your CU via:https://<your-gateway-domain>/ao/cu This endpoint allows you to interact with your CU directly through your gateway's domain.Important Notes Initial Processing Time: A CU will need to process AO history before it can give valid responses. This process can take several hours.Gateway Fallback: A gateway on release 27 or above will fallback to arweave.net if its default CU is not responding quickly enough, so gateway operations will not be significantly impacted during the initial processing.Monitoring Progress: Check the CU logs after pointing a gateway at it to watch the process of working through AO history:Resource Usage: Running a CU is resource-intensive. Monitor your system's performance to ensure it can handle both the gateway and CU workloads.

---

# 102. ARIO Gateway Environment Variables - ARIO Docs

Document Number: 102
Source: https://docs.ar.io/gateways/env
Words: 1251
Extraction Method: html

Environmental Variables Overview The AR.IO Gateway allows configuration customization through environmental variables. These variables dictate the gateway's behavior, from block synchronization settings to log formatting. Detailed below is a table enumerating all available environmental variables, their respective types, default values, and a brief description. Note that certain variables, such as SANDBOX_PROTOCOL, rely on others (e.g., ARNS_ROOT_HOST) to function effectively. Ensure proper understanding of these dependencies when configuring.Variables ← Swipe to see more → ENV Name Type Default Value Description GRAPHQL_HOST String arweave.net Host for GraphQL queries. You may use any available gateway that
supports GQL queries. If omitted, your node can support GQL queries on
locally indexed transactions, but only L1 transactions are indexed by
default.GRAPHQL_PORT Number 443 Port for GraphQL queries. Used in conjunction with GRAPHQL_HOST to set
up the proxy for GQL queries.START_HEIGHT Number or "Infinity" 0 Starting block height for node synchronization (0 = start from genesis
block) STOP_HEIGHT Number or "Infinity" "Infinity" Stop block height for node synchronization (Infinity = keep syncing
until stopped) TRUSTED_NODE_URL String " https://arweave.net " Arweave node to use for fetching data TRUSTED_GATEWAY_URL String " https://arweave.net " Arweave node to use for proxying reqeusts TRUSTED_GATEWAYS_URLS String TRUSTED_GATEWAY_URL A JSON map of gateways and priority TRUSTED_GATEWAYS_REQUEST_TIMEOUT_MS String "10000" Request timeout in milliseconds for trusted gateways TRUSTED_ARNS_GATEWAY_URL String "https:// NAME.arweave.dev" ArNS gateway WEIGHTED_PEERS_TEMPERATURE_DELTA Number 0.1 Any positive number above 0, best to keep 1 or less. Used to determine
the sensitivity of which the probability of failing or succeeding peers
decreases or increases.INSTANCE_ID String "" Adds an "INSTANCE_ID" field to output logs LOG_FORMAT String "simple" Sets the format of output logs, accepts "simple" and "json" SKIP_CACHE Boolean false If true, skips the local cache and always fetches headers from the node PORT Number 4000 AR.IO node exposed port number SIMULATED_REQUEST_FAILURE_RATE Number 0 Number from 0 to 1, representing the probability of a request failing AR_IO_WALLET String "" Arweave wallet address used for staking and rewards ADMIN_API_KEY String Generated API key used for admin API requests (if not set, it is generated and
logged into the console) ADMIN_API_KEY_FILE String Generated Alternative way to set the API key used for admin API requests via
filepath, it takes precedence over ADMIN_API_KEY if defined BACKFILL_BUNDLE_RECORDS Boolean false If true, AR.IO node will start indexing missing bundles FILTER_CHANGE_REPROCESS Boolean false If true, all indexed bundles will be reprocessed with the new filters
(you can use this when you change the filters) ON_DEMAND_RETRIEVAL_ORDER String s3,trusted-gateways,chunks,tx-data Data source retrieval order for on-demand data requests BACKGROUND_RETRIEVAL_ORDER String chunks,s3,trusted-gateways,chunks,tx-data Data source retrieval order for background data requests (i.e.,
unbundling) ANS104_UNBUNDLE_FILTER String {"never": true} Only bundles compliant with this filter will be unbundled ANS104_INDEX_FILTER String {"never": true} Only bundles compliant with this filter will be indexed ANS104_DOWNLOAD_WORKERS String 5 Sets the number of ANS-104 bundles to attempt to download in parallel ANS104_UNBUNDLE_WORKERS Number 0, or 1 if filters are set Sets the number of workers used to handle unbundling DATA_ITEM_FLUSH_COUNT_THRESHOLD Number 1000 Sets the number of new data items indexed before flushing to stable data
items MAX_FLUSH_INTERVAL_SECONDS Number 600 Sets the maximum time interval in seconds before flushing to stable data
items WRITE_ANS104_DATA_ITEM_DB_SIGNATURES Boolean false If true, the data item signatures will be written to the database WRITE_TRANSACTION_DB_SIGNATURES Boolean true If true, the transactions signatures will be written to the database ENABLE_DATA_DB_WAL_CLEANUP Boolean false If true, the data database WAL cleanup worker will be enabled ENABLE_BACKGROUND_DATA_VERIFICATION Boolean false If true, unverified data will be verified in background MAX_DATA_ITEM_QUEUE_SIZE Number 100000 Sets the maximum number of data items to queue for indexing before
skipping indexing new data items ARNS_ROOT_HOST String undefined Domain name for ArNS host SANDBOX_PROTOCOL String undefined Protocol setting in process of creating sandbox domains in ArNS
(ARNS_ROOT_HOST needs to be set for this env to have any effect) accepts
"http" or "https" START_WRITERS Boolean true If true, start indexing blocks, tx, ANS104 bundles RUN_OBSERVER Boolean true If true, run observer (ARIO processes), requires WALLET env var to be
set WALLET String N/A Wallet jwk file path for observer ario process LMDB_BLOCK_STORE_COMPRESSION String "gzip" Accepts 'gzip', 'brotli', or 'none'. Compresses new blocks with
specified algorithm before storing them in the local header store. Note:
Changing this after blocks have been stored locally will require re-sync
or remove local data to apply new compression setting to previously
stored blocks.LMDB_BUNDLE_STORE_COMPRESSION String "gzip" Accepts 'gzip', 'brotli', or 'none'. Compresses new bundles with
specified algorithm before storing them in the local bundle store. Note:
Changing this after bundles have been stored locally will require
re-indexing to apply new compression setting to previously stored
bundles.LMDB_DATA_ITEM_STORE_COMPRESSION String "gzip" Accepts 'gzip', 'brotli', or 'none'. Compresses new data items with
specified algorithm before storing them in the local data item store.
Note: Changing this after data items have been stored locally will
require re-indexing to apply new compression setting to previously
stored data items.LMDB_TX_STORE_COMPRESSION String "gzip" Accepts 'gzip', 'brotli', or 'none'. Compresses new transactions with
specified algorithm before storing them in the local transaction store.
Note: Changing this after transactions have been stored locally will
require re-sync or remove local data to apply new compression setting to
previously stored transactions.LMDB_DATA_STORE_COMPRESSION String "gzip" Accepts 'gzip', 'brotli', or 'none'. Compresses new data with specified
algorithm before storing them in the local data store. Note: Changing
this after data has been stored locally will require re-sync or remove
local data to apply new compression setting to previously stored data.CONTIGUOUS_DATA_CACHE_CLEANUP_THRESHOLD Number 1000 Sets the number of contiguous data items to cache before cleaning up ENABLE_FS_HEADER_CACHE_CLEANUP Boolean true If true, enable header cache cleanup for the fs cache (this will prune
headers that are older than HEADER_CACHE_CLEANUP_THRESHOLD) HEADER_CACHE_CLEANUP_THRESHOLD Number 2000 Sets the height threshold for which to clean up headers CHUNK_DATA_CACHE_CLEANUP_THRESHOLD Number 250000 Sets the number of chunks to cache before cleaning up MANIFEST_CACHE_CLEANUP_THRESHOLD Number 250000 Sets the number of data items to cache before cleaning up manifest cache ANS104_DATA_INDEX_CACHE_CLEANUP_THRESHOLD Number 50000 Sets the number of data items to cache before cleaning up ANS-104 data
index cache REDIS_CACHE_URL String undefined Redis cache URL for external caching of data items, chunks, and tx
headers REDIS_CACHE_TTL_SECONDS Number 3600 TTL in seconds for Redis cache entries AWS_S3_BUCKET String undefined AWS S3 bucket to save/retrieve block files AWS_REGION String us-east-1 AWS S3 bucket region AWS_ENDPOINT String " https://s3.amazonaws.com " AWS S3 bucket endpoint AWS_ACCESS_KEY_ID String undefined AWS S3 bucket access key AWS_SECRET_ACCESS_KEY String undefined AWS S3 secret key MIN_CONFIRMATIONS Number 10 Minimum number of confirmations needed for a transaction to be returned
by the /tx endpoint INDEX_BLOCKS Boolean true If true, the gateway will index blocks as they're synced INDEX_TX Boolean true If true, the gateway will index transactions as they're synced INDEX_DATA_ITEMS Boolean true If true, the gateway will index data items as they're synced INDEX_TX_OFFSET_LISTS Boolean true If true, the gateway will index the chunks of block data and transaction
data offsets ENABLE_MEMPOOL_WATCHER Boolean false If true, the gateway will watch the mempool for new transactions and
save the txs headers ENABLE_WEBHOOKS Boolean false If true, allows the gateway to act as a client and execute webhooks when
local state changes WEBHOOK_TARGET_SERVERS String "" Comma separated list of target webhook servers (URLs) WEBHOOK_INDEX_FILTER String {"never": true} Webhook events are emitted only if incoming transactions satisfy the
specified filter WEBHOOK_BLOCK_FILTER String {"never": true} Block webhook events are emitted only if incoming block satisfies the
specified filter PROMETHEUS_METRICS_ENABLED Boolean false If true, the gateway will expose Prometheus compatible metrics via the
/metrics endpoint NODE_ENV String development Node.js environment setting LOG_LEVEL String info Log verbosity level ← Swipe to see more →

---

# 103. ARIO Docs

Document Number: 103
Source: https://docs.ar.io/gateways/ar-io-node/advanced-config.html
Words: 783
Extraction Method: html

Advanced Configuration Overview The Getting Started guides for windows and linux contain all the information needed to start your AR.IO Gateway node successfully with basic configurations. There are also ever expanding advanced configuration options that allow you to run your node in a way that is customized to your specific use case.Most of the below options can be added to your .env file in order to customize its operation. Any changes made to your .env  require you to stop the docker containers running your node, and restarting them with the --build flag in order for the changes to take effect. See ENV for a complete list of environmental variables you can set.Data Storage Location You can set a custom location for your AR.IO Gateway to save the data it pulls from the Arweave network. There are three primary types of data stored, and you can set a unique storage location for each of these independently. These are "chunks data", "contiguous data", and "headers data". The custom location for each of these can be set in your.env file like this:Be sure to replace "<file path>" with the path to the location where you would like the data stored. If these values are omitted, the data will be stored in the "data" directory inside your Gateway code repository.Admin API Key HTTP endpoints under "/ar-io/admin" are protected by an admin API key. These endpoints allow you to get certain analytics data or make adjustments to your node as it's running. When your node starts, it reads your environmental variables to see if a key is set. If not, a random key is generated. The key name is ADMIN_API_KEY and it should be set in your .env file like this:ADMIN_API_KEY=SUPER_SECRET_PASSWORD View examples of the admin endpoints here Wallet Association In order to participate in the greater AR.IO network, Gateway nodes need to associate themselves with an Arweave wallet. This can be configured by setting the AR_IO_WALLET key value in your .env file.AR_IO_WALLET=1seRanklLU_1VTGowDZdD7s_-7k1qowT6oeFZHUZiZo Unbundling AR.IO Gateway nodes support unbundling and indexing ANS-104 bundle data. This is disabled by default, but can be turned on with several different configuration options. You can set these configurations with the ANS104_UNBUNDLE_FILTER and ANS104_INDEX_FILTER keys in your.env:The following types of filters are supported:Content Moderation You are able to set your Gateway to block specific transactions or data-items you don't want to serve. Unlike previous configuration options in this list, blocking content can be achieved without the need to add to your.env file and rebuild your Gateway. Instead, make a PUT request to your Gateway at /ar-io/admin/block-data. As this is an admin endpoint, you will need to have configured your ADMIN_API_KEY. Using curl as an example, the request should be formatted as follows:id (string):  This will be the transaction ID of the content you want to add to your block list.notes (string): Internal notes regarding why a particular ID is blocked.source (string): Identifier of a particular source of IDs to block. (e.g. the name of a block list) notes and source are used for documentation only, and have no effect on your block list itself.Contiguous Data Cleanup Transaction data on Arweave is stored in a chunked manner. It is commonly retrieved, however, in the the transaction data's original, contiguous form with all of its component chunks assembled end-to-end. Gateways cache contiguous representations of the transaction data to assist in various workloads, including serving transaction data to clients, allowing for efficient utilization of valuable system resources. Gateway operators will need to determine for themselves the best balance between disk space and other resource usage based on the size of their gateway and their particular use case.Contiguous data cache cleanup can be enabled using the CONTIGUOUS_DATA_CACHE_CLEANUP_THRESHOLD environmental variable. This variable sets the number of seconds from the creation of a file in the contiguous data cache after which that file will be deleted. For example:CONTIGUOUS_DATA_CACHE_CLEANUP_THRESHOLD=10000 will clear items from the contiguous data cache after ten thousand (10,000) seconds.ArNS Resolver Gateways, by default, forward requests to resolve ArNS names to arweave.dev. Starting with Release 9 gateways can instead build and maintain their own local cache. Doing so removes external dependencies and allows faster resolution.View the code for the ArNS resolver service here: https://github.com/ar-io/arns-resolver NOTE: The ArNS resolver is still an experimental feature. It is possible it may behave in unexpected ways when presented with rare edge case scenarios.In order to enable the local ArNS resolver, three environmental variables will need to be set:RUN_RESOLVER is a boolean representing an on/off switch for the local resolver.TRUSTED_ARNS_RESOLVER_TYPE sets the method the gateway uses for resolving ArNS names. Use resolver for the local resolver, or gateway for default functionality.TRUSTED_ARNS_RESOLVER_URL is the url a gateway will use to request ArNS name resolution.

---

# 104. ARIO Docs

Document Number: 104
Source: https://docs.ar.io/gateways/bundler
Words: 808
Extraction Method: html

Bundler Overview A Turbo ANS-104 data item bundler can be run alongside an AR.IO gateway. This allows gateways the ability to accept data items to be submit to the Arweave blockweave.The bundler service can be easily run inside Docker in the same way that the gateway is. It utilizes a separate docker compose file for configuration and deployment, which also allows for the use of a separate file for environmental variables specific to the bundler service. Additionally, the separation allows operators to spin their bundler service up or down at any time without affecting their core gateway service. Despite the use of separate docker compose files, the bundler service shares a docker network with the AR.IO gateway, and so is able to directly interact with the gateway service and data.Getting Started NOTE: The bundler service relies on GraphQL indexing of recently bundled and uploaded data to manage its pipeline operations. The AR.IO gateway should have its indexes synced up to Arweave's current block height before starting the bundler's service stack.Environmental Variables Environmental variables must be provided for the bundler to function and integrate properly with an existing AR.IO gateway. The gateway repository provides a .env.bundler.example file that can be renamed to .env.bundler and used as a starting point. It contains the following:BUNDLER_ARWEAVE_WALLET must be the entire jwk of an Arweave wallet's keyfile, stringified. All uploads of bundled data items to Arweave will be signed and paid for by this wallet, so it must maintain a balance of AR tokens sufficient to handle the uploads.BUNDLER_ARWEAVE_ADDRESS must be the normalized public address for the provided Arweave wallet.APP_NAME is a GraphQL tag that will be added to uploaded bundles.The remaining lines in the .env.bundler.example file control settings that allow the bundler service to share data with the AR.IO gateway. Data sharing of contiguous data between a bundler and a gateway allows the gateway to serve optimistically cached data without waiting for it to fully settle on chain.Managing Bundler Access By default, the bundler will only accept data items uploaded by data item signers whose normalized wallet addresses are in the ALLOW_LISTED_ADDRESSES list. This is an additional environmental variable that can be added to your .env.bundler file, and must be a comma separated list of normalized public wallet addresses for wallets that should be allowed to bundle and upload data through your gateway.ALLOW_LISTED_ADDRESSES=<address1>,<address2> The following permissioning configurations schemes are also possible:Indexing Bundlers submit data to the Arweave network as an ANS-104 data item bundle. This means it is several transactions wrapped into one. A gateway will need to unbundle these transactions in order to index them. A gateway should include the following ANS-104 filters in order to unbundle and index transactions from a particular bundler:$BUNDLER_ARWEAVE_ADDRESS should be replaced with the normalized public wallet address associated with the bundler.NOTE: The above filters must be placed in the .env file for the core gateway service, not the bundler.Gateways handle data item indexing asynchronously. This means they establish a queue of items to index, and work on processing the queue in the background while the gateway continues with its normal operations. If a gateway has broad indexing filters, there can be some latency in indexing data items from the bundler while the gateway works through its queue.Optimistic Indexing Gateway operators control access to their optimistic data item indexing API via an admin key that must be supplied by all bundling clients in order for their requests to be accepted. This key should be made available in the environment configuration files for BOTH the core gateway, and the bundler, and should be provided as AR_IO_ADMIN_KEY:NOTE: If a gateway is started without providing the admin key, a random string will be generated to protect the gateway's admin endpoints. This can be reset by restarting the gateway with the admin key provided in the .env file.Starting and Stopping the Bundler Starting The bundler service is designed to run in conjunction with an AR.IO gateway, and so relies on the ar-io-network network created in Docker when the core gateway services are spun up. It is possible to spin up the bundler while the core services are down, but the network must exist in Docker.To start the bundler, specify the env and docker-compose files being used in a docker compose up command:The -d flag runs the command in "detached" mode, so it will run in the background without requiring the terminal to remain active.Stopping To spin the bundler service down, specify the docker-compose file in a docker compose down command:logs While the bundler service is running in detached mode, logs can be checked by specifying the docker-compose file in a docker compose logs command:-f runs the command in "follow" mode, so the terminal will continue to watch and display new logs.--tail= defines the number of logs to display that existed prior to running the command. 0 displays only new logs.

---

# 105. Gateway Network - ARIO Docs

Document Number: 105
Source: https://docs.ar.io/gateways/gateway-network
Words: 651
Extraction Method: html

Overview The AR.IO Network consists of AR.IO gateway nodes, which are identified by their registered Arweave wallet addresses and either their IP addresses or hostnames, as stored in the network's smart contract Gateway Address Registry (GAR).These nodes adhere to the AR.IO Network’s protocols, creating a collaborative environment of gateway nodes that vary in scale and specialization.
The network promotes a fundamental level of service quality and trust minimization among its participants.Being part of the network grants AR.IO gateways an array of advantages, such as:Simplified advertising of services and discovery by end users via the Gateway Address Registry.More rapid bootstrapping of key gateway operational data due to prioritized data request fulfillment among gateways joined to the network.Sharing of data processing results.Auditability and transparency through the use of AGPL-3 licenses, which mandate public disclosure of any software changes, thereby reinforcing the network's integrity and reliability.Improved network reliability and performance through an incentive protocol, which uses a system of evaluations and rewards to encourage high-quality service from gateways.Eligibility to accept delegated staking improving a gateway’s discoverability and reward opportunities.Gateway Address Registry (GAR) Any gateway operator that wishes to join the AR.IO Network must register their node in the AR.IO smart contract’s “Gateway Address Registry”, known as the GAR.
Registration involves staking a minimum amount of ARIO tokens and providing additional metadata describing the gateway service offered.After joining the network, the operator’s gateway can be easily discovered by permaweb apps, its health can be observed, and it can participate in data sharing protocols.
A gateway becomes eligible to participate in the network’s incentive protocol in the epoch following the one they joined in.The GAR advertises the specific attributes of each gateway including its stake, delegates, settings and services.
This enables permaweb apps and users to discover which gateways are currently available and meet their needs.
Apps that read the GAR can sort and filter it using the gateway metadata, for example, ranking gateways with the highest stake, reward performance, or feature set at the top of the list.
This would allow users to prefer the higher staked, more rewarded gateways with certain capabilities over lower staked, less rewarded gateways.Data Sharing A key advantage and incentive for networked AR.IO gateways over standalone gateways is their ability to preferentially share various kinds of Arweave data among one another.
Each gateway advertises its registered Arweave wallet address, so other network participants know who they are.Gateways can identify AR.IO Network peers by evaluating the Gateway Address Registry (GAR) within the AR.IO smart contract.
They utilize that peer list to request as-yet-uncached data on behalf of their requesting clients or in service of their internal workflows.
This can include requests for transaction header data, data items, and chunks. The Arweave Network shall act as the backstop for all block data, transaction header data, and chunk data.Additionally, gateways that receive requests for cache-missed data from other gateways can provide a higher quality of service to other AR.IO gateways than that which is provided to general users, apps, and infrastructure.
However, gateways are not forced to share data with one another and can choose not to share their data if the intended recipient is acting maliciously.
Such behaviors might include failure to reciprocate in data sharing, engaging in dishonest activities / observation, or distributing invalid data.Data Verification Gateway data verification is achieved by linking content hashes of transactions and data items to data roots on the Arweave base layer chain.
Gateways index the chain from a trusted Arweave node and compute data roots for the base layer transaction data they download, ensuring that their data aligns with what was originally uploaded to Arweave.
For base layer bundles that have already been verified, gateways compute hashes of individual data items, establishing a connection between the data root, the verified bundle, and the data items it contains.
Gateways then expose these hashes and their verification status to users via HTTP headers on data responses.

---

# 106. ARIO Gateway Grafana - ARIO Docs

Document Number: 106
Source: https://docs.ar.io/gateways/grafana
Words: 695
Extraction Method: html

Grafana Analytics Overview AR.IO gateways track a significant number of performance and operation metrics using Prometheus.
A Grafana sidecar can be deployed to visualize these metrics, and provide an easy way to monitor the health of the gateway.
The Grafana sidecar is deployed as a separate docker container that uses the same network as the gateway, and is deployed in a similar manner.Deploying Grafana The file that controls the deployment of the Grafana sidecar is docker-compose.grafana.yaml. So to deploy Grafana, run the following command:The -f flag is used to specify the path to the docker-compose file, and the up -d flag is used to deploy the container in detached mode.Terminal Location This command assumes that you are running the command from the root directory of the gateway. If you are running the command from a different directory, you will need to adjust the path to the docker-compose file.Checking the logs To check the logs of the Grafana sidecar, run the following command:The -f flag is used to follow the logs, and the --tail=25 flag is used to specify the number of lines to show from the end of the logs, in this case 25.Exit the logs by pressing Ctrl+C.In some cases, the Grafana sidecar may encounter permission errors. There are two primary solutions to this issue:Modify Directory Permissions The simplest solution is to modify the permissions of the directory that contains the Grafana data.This will give the grafana user ownership of the directory and all its contents.Terminal Location This command assumes that you are running the command from the root directory of the gateway. If you are running the command from a different directory, you will need to adjust the path to the docker-compose file.Check the logs again to ensure that the issue is resolved.Change the Grafana User The second solution is to change the user that Grafana runs as. This can be done by modifying the docker-compose.grafana.yaml file to use a different user. It is suggested to use "root" or "0" to ensure that the container has the necessary permissions.In any editor, open the docker-compose.grafana.yaml file and add "user: root" to the grafana service.Once this is done, restart the Grafana sidecar by running the following command:Check the logs again to ensure that the issue is resolved.Configure Nginx The Grafana sidecar is deployed on the same network as the gateway, and can be accessed in a browser by navigating to http://localhost:1024 from the machine running the gateway.
In order to be able to access Grafana from outside the network running the gateway, Nginx, which is already used to route gateway traffic, can be configured to route Grafana traffic to the correct port.In any editor, open the relevant Nginx configuration file. If the setup guide configuration was used, that file will be located at /etc/nginx/sites-available/default.Add the following block to the configuration file inside the server block for https (443) traffic:The full configuration file should look like this:Be sure to replace <domain> with the domain of the gateway.Once the configuration is saved, test the configuration by running the following command:This will print out a message indicating that the configuration is valid.Then, restart Nginx by running the following command:Once this is done, Grafana can be accessed by navigating to https://<domain>/grafana in a browser.Accessing Grafana To access Grafana, navigate to https://<domain>/grafana in a browser.The default credentials are:Username: admin Password: admin Once logged in for the first time, you will be prompted to change the password.Credential Reset Updated credentials may be lost if the Grafana sidecar is restarted. Be sure to log into Grafana immediately after every start up to ensure Grafana cannot be accessed with the default credentials.Dashboards The Grafana sidecar comes preloaded with three dashboards:ar-io-node: Contains general gateway metrics, like the last block indexed, ArNS resolution times, and CPU usage.ar-io-node bundle indexing: Contains metrics related to bundle indexing, like the number of bundles and data items indexed.ar-io-node queue lengths: Contains metrics related to the queue lengths of the gateway, like Arweave Client requests and transaction importer data.Additional dashboards can be added in order to monitor different aspects of the gateway.The Grafana landing page contains tutorials for how to configure dashboards, as well as additional features such as alerting.

---

# 107. Join the Gateway Network - ARIO Docs

Document Number: 107
Source: https://docs.ar.io/gateways/join-network
Words: 469
Extraction Method: html

Join the AR.IO Network Prerequisites Must have a fully functional AR.IO gateway.This includes the ability to resolve ArNS subdomains.Follow installation instructions for windows or linux and get help from the ar.io community.Gateway must be associated with an Arweave Wallet.Learn about creating Arweave wallets here Arweave wallet must be funded with enough ARIO tokens to meet the minimum stake for gateway operators.Joining Via Network Portal The simplest method for joining a new gateway to the Gateway Address Registry (GAR) is to use the Network Portal   .The Network portal has a prominent "Start your own gateway" button That will open a form where configurations can be set for your gateway in the network. Start Your Gateway  Start Gateway Form Start Gateway Form The form is used to set basic configurations for a gateway when joining the network. It contains the following fields:Label: This is a friendly name for a gateway. It can be a maximum of 64 characters.Address: This is the fully qualified domain name of the gateway. That is, the standard web address used to access the gateway. i.e. arweave.net. The form prefills the https:// protocol prefix, and www should not be included. Gateways DO support using subdomains as their address, so long as the gateway is properly configured.Observer Wallet: This is the public wallet address of the wallet used for the gateway's observer. By default, the primary gateway wallet address is filled in this space; however, a different wallet may be utilized if desired for operational reasons.Properties ID: This is an Arweave Transaction Id for a JSON object that contains additional details about the gateway. The gateway network has not yet incorporated these properties into standard gateway participation, and so the space may safely be left as the default value. The contents of the default properties Id can be viewed here    Stake: This is the amount of ARIO tokens to be staked to the gateway. It must be at least the network minimum.Delegated Stake: This toggle enables or disables delegated staking on a gateway. This may be changed later.Minimum Delegated Stake: This is the minimum number of ARIO tokens that a delegate must stake in order to stake to a gateway. The network minimum is 10 ARIO.Reward Share Ratio: The percentage of gateway rewards that will be distributed to delegated stakers.Note: A description of the gateway. It can be a maximum of 256 characters.Once all required fields of the form are completed, the "Confirm" button will become available. Clicking this will prompt a signature from the connected Arweave wallet in order to complete the joining process.Joining Programmatically Joining the network can also be completed programmatically through the AR.IO SDK. This is done using the join-network method on the ARIO class.The method must be called after authenticating the ARIO class using the wallet to be associated with the new gateway.

---

# 108. ARIO Node Filtering System - ARIO Docs

Document Number: 108
Source: https://docs.ar.io/gateways/filters
Words: 541
Extraction Method: html

The AR.IO Node filtering system provides a flexible way to match and filter items based on various criteria. The system is built around JSON-based filter definitions that can be combined to create both simple and complex matching patterns.Unbundling and Indexing Filters When processing bundles, the AR.IO Node applies two filters obtained from environment variables:The ANS104_UNBUNDLE_FILTER determines which base layer transactions and data items, in the case of bundles nested in other bundles, are processed, and the ANS104_INDEX_FILTER determines which data items within the processed bundles are indexed for querying.Webhook Filters There are also two filters available that are used to trigger webhooks. When a transaction is processed that matches one of the webhook filters, the gateway will send a webhook to the specified WEBHOOK_TARGET_SERVERS urls containing the transaction data.The WEBHOOK_INDEX_FILTER is used to trigger a webhook when a transaction is indexed. The WEBHOOK_BLOCK_FILTER is used to trigger a webhook when a block is processed.Important Notes All tag names and values are base64url-decoded before matching Owner addresses are automatically converted from owner public keys Empty or undefined filters default to "never match" Tag matching requires all specified tags to match Attribute matching requires all specified attributes to match The filter system supports nested logical operations to any depth, allowing for very precise control over what data gets processed All these filters can be used in various contexts within the AR.IO Node, such as configuring webhook triggers, controlling ANS-104 bundle processing, or setting up data indexing rules. The filtering system is designed to be intuitive yet powerful, allowing for precise control over which items get processed while maintaining readable and maintainable filter definitions.Filter Construction.env formatting While the filters below are displayed on multiple lines for readability, they must be stored in the .env file as a single line for proper processing.Basic Filters The simplest filters you can use "always" and "never" filters. The "never" filter is the default behavior and will match nothing, while the "always" filter matches everything.Tag Filters Tag filters allow you to match items based on their tags in three different ways. You can match exact tag values, check for the presence of a tag regardless of its value, or match tags whose values start with specific text. All tag values are automatically base64url-decoded before matching.Attribute Filters Attribute filtering allows you to match items based on their metadata properties. The system automatically handles owner public key to address conversion, making it easy to filter by owner address. You can combine multiple attributes in a single filter:Nested Bundle Filter The isNestedBundle filter is a specialized filter that checks whether a data item is part of a nested bundle structure. It's particularly useful when you need to identify or process data items in bundles that are contained within other bundles. The filter checks for the presence of a parent_id field in the item.Note: When processing nested bundles, be sure to include filters that match the nested bundles in both ANS104_UNBUNDLE_FILTER and ANS104_INDEX_FILTER. The bundle data items (nested bundles) need to be indexed to be matched by the unbundle filter.Complex Filters Using Logical Operators For more complex scenarios, the system provides logical operators (AND, OR, NOT) that can be combined to create sophisticated filtering patterns. These operators can be nested to any depth:

---

# 109. ARIO Docs

Document Number: 109
Source: https://docs.ar.io/gateways/linux-setup
Words: 1278
Extraction Method: html

Linux Installation Instructions Overview The following instructions will guide you through the process of installing the AR.IO node on a Linux machine, specifically Ubuntu 22.04.3 desktop on a home computer. Actual steps may differ slightly on different versions or distributions. This guide will cover how to set up your node, point a domain name to your home network, and create an nginx server for routing traffic to your node. No prior coding experience is required.System Requirements Please note, The AR.IO Node software is still in development and testing, all system requirements are subject to change.External storage devices should be formatted as ext4.Minimum requirements The hardware specifications listed below represent the minimum system requirements at which the AR.IO Node has been tested. While your Node may still operate on systems with lesser specifications, please note that AR.IO cannot guarantee performance or functionality under those conditions. Use below-minimum hardware at your own risk.4 core CPU 4 GB Ram 500 GB storage (SSD recommended) Stable 50 Mbps internet connection Recommended 12 core CPU 32 GB Ram 2 TB SSD storage Stable 1 Gbps internet connection Install Packages If you would like to quickly install all required and suggested packages, you can run the following 4 commands in your terminal, and skip to installing the Node.Required packages Update your software:Enable your firewall and open necessary ports:Install nginx:Install git:Install Docker:Test Docker installation:Install Certbot:Suggested packages These packages are not required to run a node in its basic form. However, they will become necessary for more advanced usage or customization.Install ssh (optional, for remote access to your Linux machine):Install NVM (Node Version Manager):Install Node.js:Install Yarn:Install build tools:Install SQLite:Install the Node Navigate to the desired installation location:NOTE: Your indexing databases will be created in the project directory unless otherwise specified in your.env file, not your Docker environment. So, if you are using an external hard drive, you should install the node directly to that external drive.Clone the ar-io-node repository and navigate into it:Create an environment file:Paste the following content into the new file, replacing <your-domain> with the domain address you are using to access the node, and <your-public-wallet-address> with the public address of your Arweave wallet, save, and exit:The GRAPHQL values set the proxy for GQL queries to arweave.net, You may use any available gateway that supports GQL queries. If omitted, your node can support GQL queries on locally indexed transactions, but only L1 transactions are indexed by default.START_HEIGHT is an optional line. It sets the block number where your node will start downloading and indexing transactions headers. Omitting this line will begin indexing at block 0.RUN_OBSERVER turns on the Observer to generate Network Compliance Reports. This is required for full participation in the AR.IO Network. Set to false to run your gateway without Observer.ARNS_ROOT_HOST sets the starting point for resolving ARNS names, which are accessed as a subdomain of a gateway. It should be set to the url you are pointing to your node, excluding any protocol prefix. For example, use node-ar.io and not https://node-ar.io. If you are using a subdomain to access your node and do not set this value, the node will not understand incoming requests.AR_IO_WALLET is optional, and sets the wallet you want associated with your Gateway. An associated wallet is required to join the AR.IO network.OBSERVER_WALLET is the public address of the wallet used to sign Observer transactions. This is required for Observer to run, but may be omitted if you are running a gateway outside of the AR.IO network and do not plan to run Observer. You will need to supply the keyfile to this wallet in the next step.More advanced configuration options can be found at ar.io/docs Supply Your Observer Wallet Keyfile:If you are running Observer, you need to provide a wallet keyfile in order to sign report upload transactions. The keyfile must be saved in the wallets directory in the root of the repository. Name the file <Observer-Wallet-Address>.json, replacing "<Observer-Wallet-Address>" with the public address of the wallet. This should match your OBSERVER_WALLET environmental variable.Learn more about creating Arweave wallets and obtaining keyfiles here Payment For Observer Report Uploads By default, the Observer will use Turbo Credits to pay for uploading reports to Arweave. This allows reports under 100kb to be uploaded for free, but larger reports will fail if the Observer wallet does not contain Credits. Including REPORT_DATA_SINK=arweave in your .env file will configure the Observer to use AR tokens instead of Turbo Credits, without any free limit.Start the Docker container:Explanation of flags:up: Start the Docker containers.-d: Run the containers as background processes (detached mode).NOTE: Effective with Release #3, it is no longer required to include the --build flag when starting your gateway. Docker will automatically build using the image specified in the docker-compose.yaml file.To ensure your node is running correctly, follow the next two steps.Check the logs for errors:Explanation of flags:-f: Follow the logs in real time.--tail=0: Ignore all logs from before running the command.NOTE: Previous versions of these instructions advised checking a gateway's ability to fetch content using localhost. Subsequent security updates prevent this without first unsetting ARNS_ROOT_HOST in your .env.Set up Networking The following guide assumes you are running your node on a local home computer.Register a Domain Name:
Choose a domain registrar (e.g., Namecheap) to register a domain name.Point the Domain at Your Home Internet:Obtain your public IP address by visiting https://www.whatsmyip.org/ or running:Create an A record with your registrar for your domain and wildcard subdomains, using your public IP address. For example, if your domain is "ar.io," create a record for "ar.io" and "*.ar.io." Set up Port Forwarding:Obtain the local IP address of the machine where the node is installed by running:If there are multiple lines of output, choose the one starting with 192 (usually).Enter your router's IP address in the address bar of a browser (e.g., 192.168.0.1).If you're unsure of your router's IP address, consult your router's documentation or contact your Internet Service Provider (ISP).Navigate to the port forwarding settings in your router configuration.The exact steps may vary depending on your router model. Consult your router's documentation or support for detailed steps.Set up port forwarding rules to forward incoming traffic on ports 80 (HTTP) and 443 (HTTPS) to the same ports on the machine running your node. You may also forward port 22 if you want to enable SSH access to your node from outside your home network.Create SSL (HTTPS) Certificates for Your Domain:Follow the instructions to create the required TXT records for your domain in your chosen registrar. Use a DNS checker to verify the propagation of each record.Email Notifications Previous versions of these instructions advised providing an email address to Certbot. As of June 2025, LetsEncrypt (the certificate authority used by Certbot) no longer supports email notifications.IMPORTANT: Wild card subdomain (*.<your-domain>.com) cannot auto renew without obtaining an API key from your domain registrar. Not all registrars offer this. Certbot certificates expire every 90 days. Be sure to consult with your chosen registrar to see if they offer an API for this purpose, or run the above command again to renew your certificates. You will receive an email warning at the address you provided to remind you when it is time to renew.Configure nginx:
nginx is a free and open-source web server and reverse proxy server. It will handle incoming traffic, provide SSL certificates, and redirect the traffic to your node.Open the default configuration file:Replace the file's contents with the following configuration (replace "<your-domain>" when necessary):Save and exit nano.Test the configuration:If there are no errors, restart nginx:Your node should now be running and connected to the internet. Test it by entering https://<your-domain>/3lyxgbgEvqNSvJrTX2J7CfRychUD5KClFhhVLyTPNCQ in your browser.Note: If you encounter any issues during the installation process, please seek assistance from the AR.IO community.

---

# 110. Observation and Incentives - ARIO Docs

Document Number: 110
Source: https://docs.ar.io/gateways/observer
Words: 1721
Extraction Method: html

Observation and Incentives (OIP) Overview The Observation and Incentive Protocol is designed to maintain and enhance the operational integrity of gateways on the AR.IO Network.
It achieves this through a combination of incentivizing gateways for good performance and tasking those gateways to fulfill the role of "observers".
The protocol is intentionally simple and adaptable, employing a smart contract-based method for onchain “voting” to assess peer performance while being flexible on how that performance is measured.
This setup permits gateway and observer nodes to experiment and evolve best practices for performance evaluation, all while operating within the bounds of the network's immutable smart contract, thus eliminating the need for frequent contract updates (forks).In this protocol, observers evaluate their gateway peers' performance to resolve ArNS names.
Their aim is to ensure each gateway in the network accurately resolves a subset of names and assigning a pass / fail score based on their findings.A key component of the protocol is its reward mechanism.
This system is predicated on gateway performance and compliance with observation duties.
Gateways that excel are tagged as "Functional Gateways" and earn rewards, while those that do not meet the criteria, “Deficient Gateways” risk facing penalties – namely, the lack of rewards.Funds for incentive rewards are derived from the protocol balance, which consists of ARIO tokens initially allocated at network genesis as well as those collected from ArNS asset purchases.
Every epoch, this balance is utilized to distribute rewards to qualifying gateways and observers based on certain performance metrics.Observation Protocol The Observation protocol is organized around daily epochs, periods of time that are broken into an observation reporting and tallying phase.
The protocol is followed across each epoch, promoting consistent healthy network activity that can form pro-social behaviors and react to malicious circumstances.Onchain Reports The to-be-evaluated ArNS names include a set of two (2) names randomly determined by the protocol, known as “prescribed names”, which are common across all observers within the epoch, as well as a set of eight (8) “chosen names” picked at the discretion of each individual observer.
“Prescribed names” are assigned to act as a common denominator / baseline while “chosen names” allow each observer to evaluate names that may be important to their operation.Observers shall upload their completed reports (in JSON format) to the Arweave network as an onchain audit trail.
In addition, observers shall submit an interaction to the AR.IO smart contract detailing each gateway that they observed to have “failed” their assessments.
These “votes” are tallied and used to determine the reward distribution.Selection of Observers The observer selection process commences at the beginning of each epoch and employs a random-weighted selection method.
By combining random selection with weighted criteria like stake, tenure, and past rewards, the process aims to ensure both fairness and acknowledgment of consistent performance.
This method allows for a systematic yet randomized approach to selecting gateways for observation tasks.Criteria for Selection Up to fifty (50) gateways can be chosen as observers per epoch.
If the GAR is below that amount, then every gateway is designated as an observer for that epoch.
If there are greater than 50, then randomized selection shall be utilized.The weighted selection criteria will consider the following for each gateway:Stake Weight (SW): This factor considers how financially committed a gateway is to the network. It is the ratio of the total amount of ARIO tokens staked by the gateway (plus any delegated stake) relative to the network minimum and is expressed as:SW = (Gateway Stake + Delegated Stake) / (Minimum Network Join Stake) Tenure Weight (TW): This factor considers how long a gateway has been part of the network, with a maximum value capped at four (4). This means that the maximum value is achieved after 2-years of participation in the network. It is calculated as:TW = (Gateway Network Tenure) / (6-months) Gateway Performance Ratio Weight (GPRW): This factor is a proxy for a gateway’s performance at resolving ArNS names. The weight represents the ratio of epochs in which a gateway received rewards for correctly resolving names relative to their total time on the network. To prevent division by zero conditions, it is calculated as:GPRW = (1 + Passed Epochs) / (1 + Participated Epochs) Observer Performance Ratio Weight (OPRW): This factor is a proxy for a gateway’s performance at fulfilling observation duties. The weight reflects the ratio of epochs in which a gateway, as an observer, successfully submitted observation reports relative to their total periods of service as an observer. To prevent division by zero conditions thus unfairly harming a newly joined gateway, it is calculated as:OPRW = (1 + Submitted Epochs) / (1 + Selected Epochs) Weight Calculation and Normalization For each gateway, a composite weight (CW) is computed, combining the Stake Weight, Tenure Weight, Gateway Performance Ratio Weight, and Observer Performance Ratio Weight.The formula used is:CW = SW x TW x GPRW x OPRW These weights are then normalized across the network to create a continuous range, allowing for proportional random selection based on the weighted scores.
The normalized composite weight (N_CW) for each gateway indicates its likelihood of being chosen as an observer and is calculated by dividing the gateway's CW by the sum of all CWs.
Any gateway with a composite weight equal to zero shall be ineligible for selection as an observer during the associated epoch.Random Selection Process The selection of observers is randomized within the framework of these weights.
A set of unique random numbers is generated with entropy within the total range of normalized weights.
For each random number, the gateway whose normalized weight range encompasses this number is selected.
This system ensures that while gateways with higher weights are more likely to be chosen, all gateways maintain a non-zero chance of selection, preserving both fairness and meritocracy in the observer assignment process.
The current epoch’s selected / prescribed observers as well as prescribed ArNS names to be evaluated shall be saved in the contract state at the beginning of the epoch to ensure that any activities during that epoch do not affect the selection of observers or awards distribution.Performance Evaluation Consider the following classifications:Functional or Passed Gateways: are gateways that meet or surpass the network’s performance and quality standards.Deficient or Failed Gateways: are gateways that fall short of the network's performance expectations.Functional or Submitted Observers: are selected observers who diligently perform their duties and submit observation reports and contract interactions.Deficient or Failed Observers: are selected observers who do not fulfill their duty of submitting observation reports and contract interactions.At the end of an epoch, the smart contract will assess the results from the observers and determine a pass / fail score for each gateway:If greater than or equal to 50% of submitted observer contract interactions indicate a PASS score, then that gateway is considered Functional and eligible for gateway rewards.Else, if greater than 50% of submitted observer contract interactions indicate a FAIL score, then that gateway is considered Deficient and ineligible for gateway rewards.These results will determine how reward distributions are made for that epoch.
Rewards shall be distributed after forty (40) minutes (approx. twenty (20) Arweave blocks) in the following epoch have elapsed.
This delay ensures that all observation contract interactions are safely confirmed by the Arweave network without risk of “forking out” prior to the evaluation and reward distribution process.Reward Distribution Each epoch, a portion of the protocol balance is earmarked for distribution as rewards.
This value shall begin at 0.1% per epoch for the first year of operation, then linearly decline down to and stabilize at 0.05% over the following 6 months.
From this allocation, two distinct reward categories are derived:Base Gateway Reward (BGR): This is the portion of the reward allocated to each Functional Gateway within the network and is calculated as:BGR = [Epoch Reward Allocation x 90% / Total Gateways in the Network] Base Observer Reward (BOR): Observers, due to their additional responsibilities, have a separate reward calculated as:BOR = [Epoch Reward Allocation x 10% / Total Selected Observers for the Epoch] Distribution Based on Performance The reward distribution is contingent on the performance classifications derived from the Performance Evaluation:Functional Gateways: Gateways that meet the performance criteria receive the Base Gateway Reward.Deficient Gateways: Gateways falling short in performance do not receive any gateway rewards.Functional Observers: Observers that fulfilled their duty receive the Base Observer Reward.Deficient Observers: Observers failing to meet their responsibilities do not receive observer rewards. Furthermore, if they are also Functional Gateways, their gateway reward is reduced by 25% for that epoch as a consequence for not performing their observation duty.Gateways shall be given the option to have their reward tokens “auto-staked” to their existing stake or sent to their wallet as unlocked tokens. The default setting shall be “auto-staked”.Distribution to Delegates The protocol will automatically distribute a Functional Gateway’s shared rewards with its delegates.
The distribution will consider the gateway’s total reward for the period (including observation rewards), the gateway’s “Delegate Reward Share Ratio”, and each delegate’s stake proportional to the total delegation.
Each individual delegate reward is calculated as:Unlike gateways, token reward distributions to delegated stakers will only be “auto-staked” in that they will be automatically added to the delegate’s existing stake associated with the rewarded gateway.
The delegated staker is then free to withdraw their staked rewards at any time (subject to withdrawal delays).Undistributed Rewards In cases where rewards are not distributed, either due to the inactivity or deficiency of gateways or observers, the allocated tokens shall remain in the protocol balance and carry forward to the next epoch.
This mechanism is in place to discourage observers from frivolously marking their peers as offline in hopes of attaining a higher portion of the reward pool.
Note that if a gateway (and its delegates) leaves the network or a delegate fully withdraws stake from a gateway, they become ineligible to receive rewards within the corresponding epoch and the earmarked rewards will not be distributed.Handling Deficient Gateways To maintain network efficiency and reduce contract state bloat, gateways that are marked as deficient, and thus fail to receive rewards,
for thirty (30) consecutive epochs will automatically trigger a “Network Leave” action and be subjesct to the associated stake withdrawal durations for both gateway stake and any delegated stake.
In addition, the gateway shall have its minimum network-join stake slashed by 100%. The slashed stake shall be immediately sent to the protocol balance.

---

# 111. Content Moderation - ARIO Docs

Document Number: 111
Source: https://docs.ar.io/gateways/moderation
Words: 543
Extraction Method: html

Overview Arweave is a network designed for permanent storage of data. It is a practical impossibility for data to be wholly removed from the network once it has been uploaded.The AR.IO Network has adopted Arweave's voluntary content moderation model, whereby every participant of the network has the autonomy to decide which content they want to (or can legally) store, serve, and see. Each gateway operating on the network has the right and ability to blocklist any content, ArNS name, or address that is deemed in violation of its content policies or is non-compliant with local regulations.NOTE Overly restrictive content policies may impact a gateway's likelihood of
receiving protocol rewards.Gateway operators may set content to be blocked by their gateway by submitting a Put request to their gateway defining the content to be blocked. This requires that the ADMIN_API_KEY environmental variable to be set in order to authenticate the moderation request.The simplest method for submitting moderation requests to a gateway is to use curl in a terminal.Authentication Moderation requests must contain the gateway's ADMIN_API_KEY in the request Header, as Authorization: Bearer.For example, if a gateway's ADMIN_API_KEY is set to secret, any request must contain Authorization: Bearer secret in the Header.Block Data Specific data items can be blocked by a gateway operator by submitting a Put request containing a json object with three keys:id: The Arweave transaction Id of the data item to be blocked.notes: Any note the gateway operator wants to leave him/herself as to the reason the content is blocked.source: A note as to where the content was identified as requiring moderation. i.e. a public block list.Requests to block data must be submitted to the gateway's /ar-io/admin/block-data endpoint.Unblock Data At this time, blocked data items can only be unblocked by manually deleting the corresponding row from the data/sqlite/moderation.db database.
The Arweave transaction Id of the blocked data item is stored in the database as raw bytes, which sqlite3 accepts as a BLOB (Binary Large OBject), and so cannot be accessed easily using the original transaction Id, which is a base64url.
Sqlite3 is able to interact with a hexadecimal representation of the BLOB, by using a BLOB literal. To do so, wrap a hexadecimal representation of the Arweave transaction Id in single quotes, and prepend an X i.e. X'de5cb181b804bea352bc9ad35f627b09f472721503e4a0a51618552f24cf3424'.Where possible, consider using the notes or source values to identify rows for deletion rather than the id.Block ArNS Name ArNS names can be blocked so that a gateway will refuse to serve their associated content even if the name holder updates the Arweave transaction Id that the name points at.This is done via an authenticated PUT request to the endpoint /ar-io/admin/block-name containing a json object with three keys:name: The ArNS name to be blocked.notes: Any note the gateway operator wants to leave him/herself as to the reason the content is blocked.source: A note as to where the content was identified as requiring moderation. i.e. a public block list.Undernames For moderation purposes, each undername of an ArNS name is treated as a separate name and must be moderated separately.Unblock ArNS Name Gateway operators can unblock ArNS names that were previously blocked.This is done via an authenticated PUT request to the endpoint /ar-io/admin/unblock-name containing a json object with a single key:name: The ArNS name to be unblocked

---

# 112. Parquet and ClickHouse Usage Guide - ARIO Docs

Document Number: 112
Source: https://docs.ar.io/gateways/parquet
Words: 1482
Extraction Method: html

Overview AR.IO gateway Release 33 introduces a new configuration option for using Parquet files and ClickHouse to improve performance and scalability of your AR.IO gateway for large datasets.This guide will walk you through the process of setting up ClickHouse with your AR.IO gateway, and importing Parquet files to bootstrap your ClickHouse database.What is Parquet?Apache Parquet is a columnar storage file format designed for efficient data storage and retrieval. Unlike row-based storage formats like SQLite, Parquet organizes data by column rather than by row, which provides several advantages for analytical workloads:Efficient compression: Similar data is stored together, leading to better compression ratios Columnar access: You can read only the columns you need, reducing I/O operations Predicate pushdown: Filter operations can be pushed down to the storage layer, improving query performance Current Integration with AR.IO Gateways In the current AR.IO gateway implementation, Parquet and ClickHouse run alongside SQLite rather than replacing it. This parallel architecture allows each database to handle what it does best:SQLite continues to handle transaction writes and updates ClickHouse with Parquet files is optimized for fast query performance, especially with large datasets The gateway continues to operate with SQLite just as it always has, maintaining all of its normal functionality. Periodically, the gateway will export batches of data from SQLite to Parquet files, which are then imported into ClickHouse. This batch-oriented approach is much more efficient than attempting to synchronize the databases in real-time, as it leverages Parquet's strength in handling large, immutable data sets.Note that despite Parquet's efficient compression, gateways may not see significant disk space reduction in all cases. While bundled transaction data is exported to Parquet, L1 data remains in SQLite. Without substantial unbundling and indexing filters, minimal data gets exported to Parquet, limiting potential storage savings.With ClickHouse integration enabled, GraphQL queries are primarily routed to ClickHouse, leveraging its superior performance for large datasets. This significantly improves response times while maintaining SQLite's reliability for transaction processing.Parquet vs. SQLite in AR.IO Gateways While SQLite is excellent for transactional workloads and small to medium datasets, it faces challenges with very large datasets:Benefits for Gateway Operators Implementing Parquet and ClickHouse alongside SQLite in your AR.IO gateway offers several key advantages:Dramatically improved query performance for GraphQL endpoints, especially for large result sets Reduced storage requirements through efficient columnar compression Better scalability for growing datasets Faster bootstrapping of new gateways through Parquet file imports Reduced load on SQLite by offloading query operations to ClickHouse The primary focus of the Parquet/ClickHouse integration is the significant speed improvement for querying large datasets. Gateway operators managing significant volumes of data will notice substantial performance gains when using this configuration.Storage Considerations While Parquet files offer more efficient compression for the data they contain, it's important to understand the storage impact:Bundled transaction data is exported to Parquet and removed from SQLite, potentially saving space L1 data remains in SQLite regardless of Parquet configuration Space savings are highly dependent on your unbundling filters - without substantial unbundling configurations, minimal data gets exported to Parquet The more data you unbundle and export to Parquet, the greater the potential storage efficiency For gateway operators, this means proper filter configuration is crucial to realize storage benefits. The primary advantage remains significantly improved query performance for large datasets, with potential space savings as a secondary benefit depending on your specific configuration.The following sections will guide you through setting up ClickHouse with your AR.IO gateway, exporting data from SQLite to Parquet, and importing Parquet files to bootstrap your ClickHouse database.Note The below instructions are designed to be used in a linux environment. Windows and MacOS users must modify the instructions to use the appropriate package manager/ command syntax for their platform.Unless otherwise specified, all commands should be run from the root directory of the gateway.Installing ClickHouse ClickHouse is a powerful, open-source analytical database that excels at handling large datasets and complex queries. It is the tool used by the gateway to integrate with the Parquet format. To integrate ClickHouse with your AR.IO gateway, follow these steps:It is recommended to use official pre-compiled deb packages for Debian or Ubuntu. Run these commands to install packages:This will verify the installation package from official sources and enable installation via apt-get.This will perform the actual installation of the ClickHouse server and client.During installation, you will be prompted to set a password for the default user. This is required to connect to the ClickHouse server.Advanced users may also choose to create a designated user account in clickhouse for the gateway to use, but the default gateway configuration will assume the default user.Configure Gateway to use ClickHouse Because the gateway will be accessing ClickHouse, host address andthe password for the selected user must be provided. This is done via the CLICKHOUSE_PASSWORD environment variable.Update your.env file with the following:If you set a specific user account for the gateway to use, you can set the CLICKHOUSE_USER environment variable to the username.CLICKHOUSE_USER=<your-username> If omitted, the gateway will use the default user.Additionally, The Parquet file provided below contains an unbundled data set that includes all data items uploaded via an ArDrive product, including Turbo. Because of this, it is recommended to include unbundling filters that match, or expand, this configuration.ANS104_UNBUNDLE_FILTER='{ "and": [ { "not": { "or": [ { "tags": [ { "name": "Bundler-App-Name", "value": "Warp" } ] }, { "tags": [ { "name": "Bundler-App-Name", "value": "Redstone" } ] }, { "tags": [ { "name": "Bundler-App-Name", "value": "KYVE" } ] }, { "tags": [ { "name": "Bundler-App-Name", "value": "AO" } ] }, { "attributes": { "owner_address": "-OXcT1sVRSA5eGwt2k6Yuz8-3e3g9WJi5uSE99CWqsBs" } }, { "attributes": { "owner_address": "ZE0N-8P9gXkhtK-07PQu9d8me5tGDxa_i4Mee5RzVYg" } }, { "attributes": { "owner_address": "6DTqSgzXVErOuLhaP0fmAjqF4yzXkvth58asTxP3pNw" } } ] } }, { "tags": [ { "name": "App-Name", "valueStartsWith": "ArDrive" } ] } ] }'
ANS104_INDEX_FILTER='{ "tags": [ { "name": "App-Name", "value": "ArDrive-App" } ] }' Lastly, you must have a gateway admin password set. This is used for the periodic export of data from SQLite to Parquet.ADMIN_API_KEY=<example> Once the.env file is updated, restart the gateway to apply the changes.A Parquet archive file is available for download from ar://JVmsuD2EmFkhitzWN71oi9woADE4WUfvrbBYgremCBM   . This file contains an unbundled data set that includes all data items uploaded via an ArDrive product, current to April 23, 2025, and compressed using tar.gz.To download the file, run the following command:or visit the url https://arweave.net/JVmsuD2EmFkhitzWN71oi9woADE4WUfvrbBYgremCBM    and download the file manually.Note If downloaded manually, it will download as a binary file named JVmsuD2EmFkhitzWN71oi9woADE4WUfvrbBYgremCBM. This is normal and must be converted to a tar.gz file by renaming it to 2025-04-23-ardrive-ans104-parquet.tar.gz.It should also be placed in the root directory of the gateway.The downloaded file will be approximately 3.5GB in size.Extracting and Importing the Parquet File With the parquet file downloaded and placed in the root directory of the gateway, you can extract the file and import it into ClickHouse.This will extract the file into a directory named 2025-04-23-ardrive-ans104-parquet, and take a while to complete.Next, if you do not already have a data/parquet directory, you must create it. Release 33 does not have this directory by default, but future Releases will. You can create the directory by using the following command:or by starting the gateway ClickHouse container with the following command:Note Depending on your system configurations, allowing the gateway to create the directory may result in the directory being created with incorrect permissions. If this is the case, you can remove the restrictions by running the following command:With the directory created, you can now move the extracted parquet files into it.When this is complete, you can run the import script to import the parquet files into ClickHouse.If you haven't done so already, start the ClickHouse container with the following command:Then run the import script with the following command:./scripts/clickhouse-import This process will take several minutes, and will output the progress of the import.Verifying Successful Import To verify that the import was successful, run the following commands:Being sure to replace <your-password> with the password you set for the selected ClickHouse user.This should return a count of the number of unique transactions in the parquet file, which is 32712311.You can also verify that the data is being served by the gateway's GraphQL endpoint by ensuring the gateway is not proxying its GraphQL queries (Make sure GRAPHQL_HOST is not set) and running the following command:Starting and Stopping the Gateway with ClickHouse The gateway ClickHouse container is run as a "profile" in the main docker compose file. That means you must specify the profile when starting or stopping the gateway if you want to include the ClickHouse container in the commands.To start the gateway with the ClickHouse profile, run the following command:This will start all of the containers normally covered by the docker compose up command, but will also start the ClickHouse container.To stop the gateway with the ClickHouse profile, run the following command:This will stop all of the containers normally covered by the docker compose down command, but will also stop the ClickHouse container.To start or stop only the ClickHouse container, you can use the following commands:and

---

# 113. Optimizing Data Handling in ARIO Gateway - ARIO Docs

Document Number: 113
Source: https://docs.ar.io/gateways/optimize-data
Words: 968
Extraction Method: html

The AR.IO Gateway provides powerful tools for optimizing how you access and serve specific types of data. By configuring filters and worker settings, you can focus your gateway on efficiently handling the data that matters most to your use case, ensuring quick and reliable access to relevant information.Understanding the Filtering System The AR.IO Gateway uses two filters to control how ANS104 data items are processed:ANS104_UNBUNDLE_FILTER: Controls which bundles are processed and unbundled ANS104_INDEX_FILTER: Controls which data items from unbundled bundles are stored in the database for querying These filters are configured through environment variables:By default, the gateway processes no bundles and indexes no data items. This allows you to selectively enable processing for the specific data types you need.For a detailed explanation of how to construct these filters, see our Filtering System documentation.Key Environment Variables Several environment variables control how your gateway processes data:Data Item Flushing The gateway uses a two-stage storage system for indexed data items:Temporary Storage: Newly indexed data items are first stored in a temporary table Stable Storage: Data items are periodically "flushed" from temporary to stable storage This process is controlled by two environment variables:DATA_ITEM_FLUSH_COUNT_THRESHOLD: Number of items to queue before flushing (default: 1000) MAX_FLUSH_INTERVAL_SECONDS: Maximum time between flushes (default: 600 seconds) The gateway will flush data items when either:The number of items in temporary storage reaches the threshold The time since the last flush exceeds the interval This batching approach helps optimize database performance by reducing the number of write operations.GraphQL Configuration The GRAPHQL_HOST setting determines how your gateway handles GraphQL queries. You have two options:Using arweave.net (Recommended for new gateways) Proxies queries to a gateway with a complete index of the blockweave Provides immediate access to all historical data No need to wait for local indexing May introduce additional latency from proxying Local-only Queries (Unset GRAPHQL_HOST) Responds to queries using only locally indexed data Faster response times for indexed data Requires complete local indexing (can take weeks for L1 transactions) No proxying overhead Only returns data that matches your indexing filters Note: Even with GRAPHQL_HOST set to arweave.net, your gateway will still maintain its own index based on your filters. This allows for quick access to frequently requested data while ensuring availability of all historical data.Common Use Cases Optimizing for Specific Data Types By configuring your filters and workers appropriately, you can optimize your gateway for different types of data:High-Volume Data: Configure workers to handle large amounts of data efficiently Specific Applications: Filter for particular app names or content types Filter Examples The AR.IO Gateway uses two distinct filters to control how ANS104 bundle data is processed:ANS104_UNBUNDLE_FILTER: Determines which bundles (including nested bundles) are unbundled ANS104_INDEX_FILTER: Determines which data items within a bundle have their data indexed Here are some practical examples of how to configure these filters for specific use cases:Specific Application Data This configuration demonstrates how to focus your gateway on data from a specific application. In this example, we show how to process and index all ArDrive-related transactions, but you can adapt this pattern for any application, using the App-Name tag. This approach is perfect for:Building application-specific services Creating application data archives Running application-focused analytics Supporting application infrastructure Reducing processing overhead by focusing only on relevant data In this example, the index filter uses the ArFS tag to only index ArFS-compliant data, which is a specific aspect of ArDrive applications. Index filters can be adjusted for any application's needs - the App-Name tag is particularly useful here, as data items within a bundle can have a different App-Name than the bundle that contains them.Personal Data Gateway This configuration is designed for users who want to run a personal gateway that only processes their own ArDrive data. It:Excludes common specific use case bundlers to reduce unnecessary processing Only indexes data owned by your wallet address Includes all ArDrive and Turbo app data Perfect for personal data management Personal Data Gateway All ArDrive Bundles (Excluding Common Bundlers) This configuration is useful for gateways that want to process ArDrive data while avoiding common bundlers. It's ideal for:Reducing processing overhead by excluding known bundlers Maintaining a clean dataset focused on direct ArDrive transactions Optimizing storage and processing resources Supporting ArDrive infrastructure with reduced resource requirements All ArDrive Bundles (Excluding Common Bundlers) Important Filter Considerations When configuring your filters, keep these points in mind:The unbundle filter determines which bundles are processed and unbundled The index filter determines which data items from unbundled bundles are indexed in the database When filtering by owner addresses, use the modulus of the Arweave public address in the unbundle filter Common Bundler Exclusions When configuring filters, you may want to exclude data from common bundlers:Bundler Addresses:Irys Node 1: -OXcT1sVRSA5eGwt2k6Yuz8-3e3g9WJi5uSE99CWqsBs Irys Node 2: ZE0N-8P9gXkhtK-07PQu9d8me5tGDxa_i4Mee5RzVYg Irys Node 3: 6DTqSgzXVErOuLhaP0fmAjqF4yzXkvth58asTxP3pNw Bundler App Names:Warp Redstone Kyve AO ArDrive Best Practices Start Small: Begin with conservative worker counts and adjust based on system performance Monitor Resources: Watch system memory and CPU usage when adjusting worker counts Reprocess Bundles with New Filters: Use FILTER_CHANGE_REPROCESS to reprocess bundles after changing filters Regular Maintenance: Enable background verification and cleanup features Performance Considerations When optimizing your gateway, consider these factors:System Resources: Worker counts should be balanced against available CPU cores and memory Storage Space: Indexing filters affect database size and query performance Network Bandwidth: Unbundling workers can generate significant network traffic Query Performance: More indexed data means larger databases but better query capabilities Review the Filtering System documentation for detailed filter syntax Check your system resources to determine optimal worker counts Start with basic filters and gradually refine based on your needs Monitor system performance and adjust settings as needed Optimization Strategy Focus on configuring your gateway to efficiently handle the specific data types you need. The default state processes no data, so you can selectively enable processing for your use case without worrying about unnecessary resource usage.

---

# 114. ARIO Node Release Notes - ARIO Docs

Document Number: 114
Source: https://docs.ar.io/gateways/release-notes
Words: 7699
Extraction Method: html

AR.IO Release Notes Overview Welcome to the documentation page for the AR.IO gateway release notes. Here, you will find detailed information about each version of the AR.IO gateway, including the enhancements, bug fixes, and any other changes introduced in every release. This page serves as a comprehensive resource to keep you informed about the latest developments and updates in the AR.IO gateway. For those interested in exploring the source code, each release's code is readily accessible at our GitHub repository: AR.IO gateway change logs. Stay updated with the continuous improvements and advancements in the AR.IO gateway by referring to this page for all release-related information.[Release 41] - 2025-06-30 Added Added preferred chunk GET node URLs configuration via PREFERRED_CHUNK_GET_NODE_URLS environment variable to enable chunk-specific peer prioritization. Preferred URLs receive a weight of 100 for prioritization and the system selects 10 peers per attempt by default.Added hash validation for peer data fetching by including X-AR-IO-Expected-Digest header in peer requests when hash is available, validating peer responses against expected hash, and immediately rejecting mismatched data.Added DOCKER_NETWORK_NAME environment variable to configure the Docker network name used by Docker Compose.Added draft guide for running a community gateway.Added draft data verification architecture document.Changed Removed trusted node fallback for chunk retrieval. Chunks are now retrieved exclusively from peers, with the retry count increased from 3 to 50 to ensure reliability without the trusted node fallback.Fixed Fixed inverted logic preventing symlink creation in FsChunkDataStore.Fixed Content-Length header for range requests and 304 responses, properly setting header for single and multipart range requests and removing entity headers from 304 Not Modified responses per RFC 7232.Fixed MaxListenersExceeded warnings by adding setMaxListeners to read-through data cache.Fixed potential memory leaks in read-through data cache by using once instead of on for error and end event listeners.[Release 40] - 2025-06-23 This is an optional release that primarily improves caching when data is fetched from peers.Added Added experimental flush-to-stable script for manual database maintenance. This script allows operators to manually flush stable chain and data item tables, mirroring the logic of StandaloneSqliteDatabase.flushStableDataItems. WARNING: This script is experimental and directly modifies database contents. Use with caution and ensure proper backups before running.Changed Replaced yesql with custom SQL loader that handles comments better, improving SQL file parsing and maintenance.Switched to SPDX license headers to reduce LLM token usage, making the codebase more efficient for AI-assisted development.Improved untrusted data handling and hash validation in cache operations. The cache now allows caching when a hash is available for validation even for untrusted data sources, but only finalizes the cache when the computed hash matches a known trusted hash. This prevents cache poisoning while still allowing data caching from untrusted sources when the data can be validated.[Release 39] - 2025-06-17 This release enhances observability and reliability with new cache metrics, improved data verification capabilities, and automatic failover between chain data sources. The addition of ArNS-aware headers enables better data prioritization across the gateway network. This is a recommended but not urgent upgrade.Added Added filesystem cache metrics with cycle-based tracking. Two new Prometheus metrics track cache utilization: cache_objects_total (number of objects in cache) and cache_size_bytes (total cache size in bytes). Both metrics include store_type and data_type labels to differentiate between cache types (e.g., headers, contiguous_data). Metrics are updated after each complete cache scan cycle, providing accurate visibility into filesystem cache usage.Added X-AR-IO-Data-Id header to all data responses. This header shows the actual data ID being served, whether from a direct ID request or manifest path resolution, providing transparency about the content being delivered.Added automatic data item indexing when data verification is enabled. When ENABLE_BACKGROUND_DATA_VERIFICATION is set to true, the system now automatically enables data item indexing (ANS104_UNBUNDLE_FILTER) with an always: true filter if no filter is explicitly configured. This ensures bundles are unbundled to verify that data items are actually contained in the bundle associated with the Arweave transaction's data root.Added ArNS headers to outbound gateway requests to enable data prioritization. The generateRequestAttributes function now includes ArNS context headers (X-ArNS-Name, X-ArNS-Basename, X-ArNS-Record) in requests to other gateways and Arweave nodes, allowing downstream gateways to effectively prioritize ArNS data requests.Added configurable Docker Compose host port environment variables (CORE_PORT, ENVOY_PORT, CLICKHOUSE_PORT, CLICKHOUSE_PORT_2, CLICKHOUSE_PORT_3, OBSERVER_PORT) to allow flexible port mapping while maintaining container-internal port compatibility and security.Added Envoy aggregate cluster configuration for automatic failover between primary and fallback chain data sources. The primary cluster (default: arweave.net:443) uses passive outlier detection while the fallback cluster (default: peers.arweave.xyz:1984) uses active health checks. This enables zero-downtime failover between HTTPS and HTTP endpoints with configurable FALLBACK_NODE_HOST and FALLBACK_NODE_PORT environment variables.Changed Streamlined background data retrieval to reduce reliance on centralized sources. The default BACKGROUND_RETRIEVAL_ORDER now only includes chunks,s3, removing trusted-gateways and tx-data from the default configuration. This prioritizes verifiable chunk data and S3 storage for background operations like unbundling.Removed ar-io.net from default trusted gateways list and removed TRUSTED_GATEWAY_URL default value to reduce load on ar-io.net now that P2P data retrieval is re-enabled. Existing deployments with TRUSTED_GATEWAY_URL explicitly set will continue to work for backwards compatibility.[Release 38] - 2025-06-09 This release focuses on data integrity and security improvements, introducing trusted data verification and enhanced header information for data requests. Upgrading to this release is recommended but not urgent.Added Added X-AR-IO-Trusted header to indicate data source trustworthiness in responses. This header helps clients understand whether data comes from a trusted source and works alongside the existing X-AR-IO-Verified header to provide data integrity information. The system now filters peer data by requiring peers to indicate their content is either verified or trusted, protecting against misconfigured peers that may inadvertently serve unintended content (e.g., provider default landing pages) instead of actual Arweave data.Added If-None-Match header support for HTTP conditional requests enabling better client-side caching efficiency. When clients send an If-None-Match header that matches the ETag, the gateway returns a 304 Not Modified response with an empty body, reducing bandwidth usage and improving performance.Added digest and hash headers for data HEAD requests to enable client-side data integrity verification.Added EC2 IMDS (instance-profile) credential support for S3 data access, improving AWS authentication in cloud environments.Added trusted data flag to prevent caching of data from untrusted sources, ensuring only verified and reliable content is stored locally while still allowing serving of untrusted data when necessary.Changed Re-enabled ar-io-peers as fallback data source in configuration for improved data availability.Updated trusted node configuration to use arweave.net as the default trusted node URL.Updated ETag header format to use properly quoted strings (e.g., "hash" instead of hash) following HTTP/1.1 specification standards for improved compatibility with caching proxies and clients.[Release 37] - 2025-06-03 This is a recommended release due to the included observer robustness improvements. It also adds an important new feature - data verification for preferred ArNS names. When preferred ArNS names are set, the bundles containing the data they point to will be locally unbundled (verifying data item signatures), and the data root for the bundle will be compared to the data root in the Arweave chain (establishing that the data is on Arweave). To enable this feature, set your preferred ArNS names, turn on unbundling by setting ANS104_DOWNLOAD_WORKERS and ANS104_UNBUNDLE_WORKERS both to 1, and set your ANS104_INDEX_FILTER to a filter that will match the data items for your preferred names. If you don't know the filter, use {"always": true}, but be aware this will index the entire bundle for the IDs related to your preferred names.Note: this release contains migrations to data.db. If your node appears unresponsive please check core service logs to determine whether migrations are running and wait for them to finish.Added Added prioritized data verification system for preferred ArNS names, focusing computational resources on high-priority content while enabling flexible root transaction discovery through GraphQL fallback support.Added verification retry prioritization system with tracking of retry counts, priority levels, and attempt timestamps to ensure bundles do not get stuck retrying forever.Added improved observer functionality with best-of-2 observations and higher compression for more reliable network monitoring.Added MAX_VERIFICATION_RETRIES environment variable (default: 5) to limit verification retry attempts and prevent infinite loops for consistently failing data items.Added retry logic with exponential backoff for GraphQL queries to handle rate limiting (429) and server errors with improved resilience when querying trusted gateways for root bundle IDs.Changed Updated dependencies: replaced deprecated express-prometheus-middleware with the actively maintained express-prom-bundle library and updated prom-client to v15.1.3 for better compatibility and security.Updated Linux setup documentation to use modern package installation methods, replacing apt-key yarn installation with npm global install and updating Node.js/nvm versions.Improved route metrics normalization with explicit whitelist function for better granularity and proper handling of dynamic segments.Fixed Fixed docker-compose configuration to use correct NODE_MAX_OLD_SPACE_SIZE environment variable name.Fixed production TypeScript build configuration to exclude correct "test" directory path.Fixed Parquet exporter to properly handle data item block_transaction_index exports, preventing NULL value issues.Fixed bundles system to copy root_parent_offset when flushing data items to maintain data integrity.Fixed ClickHouse auto-import script to handle Parquet export not_started status properly.Fixed docker-compose ClickHouse configuration to not pass conflicting PARQUET_PATH environment variable to container scripts.Fixed verification process for data items that have not been unbundled by adding queue bundle support and removing bundle join constraint to ensure proper verification of data items without indexed root parents.[Release 36] - 2025-05-27 This is a recommended but not essential upgrade. The most important changes are the preferred ArNS caching feature for improved performance on frequently accessed content and the observer's 80% failure threshold to prevent invalid reports during network issues.Added Added preferred ArNS caching functionality that allows configuring lists of ArNS names to be cached longer via PREFERRED_ARNS_NAMES and PREFERRED_ARNS_BASE_NAMES environment variables. When configured, these names will be cleaned from the filesystem cache after PREFERRED_ARNS_CONTIGUOUS_DATA_CACHE_CLEANUP_THRESHOLD instead of the standard cleanup threshold (CONTIGUOUS_DATA_CACHE_CLEANUP_THRESHOLD). This is accomplished by maintaining an MRU (Most Recently Used) list of ArNS names in the contiguous metadata cache. When filesystem cleanup runs, it checks this list to determine which cleanup threshold to apply. This feature enables gateway operators to ensure popular or important ArNS names remain cached longer, improving performance for frequently accessed content.Added ArNS headers to responses: X-ArNS-Name, X-ArNS-Basename, and X-ArNS-Record to help identify which ArNS names were used in the resolution.Changed Updated observer to prevent report submission when failure rate exceeds 80%. This threshold helps guard against both poorly operated observers and widespread network issues. In the case of a widespread network issue, the assumption is that most gateway operators are well intentioned and will work together to troubleshoot and restore both observations and network stability, rather than submitting reports that would penalize functioning gateways.Updated default trusted gateway in docker-compose Envoy configuration to ar-io.net for improved robustness and alignment with core service configuration.Improved range request performance by passing ranges directly to getData implementations rather than streaming all data and extracting ranges.Fixed Fixed missing cache headers (X-Cache and other data headers) in range request responses to ensure consistent cache header behavior across all request types.Fixed async streaming for multipart range requests by using async iteration instead of synchronous reads, preventing potential data loss.Fixed ArNS resolution to properly exclude www subdomain from resolution logic.Fixed test reliability issues by properly awaiting stream completion before making assertions.Fixed chunk broadcasting to not await peer broadcasts, as they are best-effort operations.[Release 35] - 2025-05-19 This is a low upgrade priority release. It contains a small caching improvement and routing fix. Upgrading to help test it is appreciated but not essential.Changed Adjusted filesystem data expiration to be based on last request times rather than file access times which may be inaccurate.Adjusted CORS headers to include content-* headers.Fixed Fixed regex used to expose /api-docs when an apex ArNS name is set.[Release 34] - 2025-05-05 Given the resilience provided by adding a second trusted gateway URL, it is recommended that everyone upgrade to this release.Added Added peer list endpoints for retrieving information about Arweave peers and ar.io gateway peers.Added ar-io.net as a secondary trusted gateway to increase data retrieval resilience by eliminating a single point of failure.Added circuit breaker for Arweave peer chunk posting.Changed Created directories for DuckDB and Parquet to help avoid permission issues by the directories being created by containers.Fixed Fixed GraphQL ClickHouse error when returning block ID and timestamp.Fixed the tx-chunks-data-source to throw a proper error (resulting in a 404) when the first chunk is missing rather than streaming a partial response.[Release 33] - 2025-05-05 Added Added a [Parquet and ClickHouse usage guide]. Using ArDrive as an example, it provides step by step instructions about how to bulk load Parquet and configure continuous ingest of bundled data items into ClickHouse. This allows the ar-io-node to support performant GraphQL queries on larger data sets and facilitates sharing indexing work across gateways via distribution of Parquet files.Added support for configurable ArNS 404 pages using either:ARNS_NOT_FOUND_TX_ID: Transaction ID for custom 404 content ARNS_NOT_FOUND_ARNS_NAME: ArNS name to resolve for 404 content Added experimental /chunk/ GET route for serving chunk data by absolute offset either the local cache.Added support for AWS_SESSION_TOKEN in the S3 client configuration.Expanded ArNS OTEL tracing to improve resolution behavior observability.Added support for setting a ClickHouse username and password via the CLICKHOUSE_USERNAME and CLICKHOUSE_PASSWORD environment variable. When using ClickHouse, CLICKHOUSE_PASSWORD should always be set. However, CLICKHOUSE_USERNAME can be left unset. The username default will be used in that case.Added support for configuring the port used to connect to ClickHouse via the CLICKHOUSE_PORT environment variable.Changed Disabled ClickHouse import timing logging by default. It can be enabled via environment variable - DEBUG when running the service standalone or CLICKHOUSE_DEBUG when using Docker Compose Upgraded to ClickHouse 25.4.Fixed Ensure .env is read in clickhouse-import script.[Release 32] - 2025-04-22 Changed Reenabled parallel ArNS resolution with removal of misplaced global limit. Refer to release 30 notes for more details on configuration and rationale.Added a timeout for the last ArNS resolver in ARNS_RESOLVER_PRIORITY_ORDER. It defaults to 30 seconds and is configurable using ARNS_COMPOSITE_LAST_RESOLVER_TIMEOUT_MS. This helps prevent promise build up if the last resolver stalls.Fixed Fixed apex ArNS name handling when a subdomain is present in ARNS_ROOT_HOST.Fixed a case where fork recovery could stall due to early flushing of unstable chain data.Restored observer logs by removing unintentional default log level override in docker-compose.yaml.[Release 31] - 2025-04-11 Changed Improved peer TX header fetching by fetching from a wider range of peers and up/down weighting peers based on success/failure.Fixed Rolled back parallel ArNS resolution changes that were causing ArNS resolution to slow down over time.[Release 30] - 2025-04-04 Added Added support for filtering Winston logs with a new LOG_FILTER environment variable.Example filter: {"attributes":{"class":"ArweaveCompositeClient"}} to only show logs from that class.Use CORE_LOG_FILTER environment variable when running with docker-compose.Added parallel ArNS resolution capability.Configured via ARNS_MAX_CONCURRENT_RESOLUTIONS (default: 1).This foundation enables future enhancements to ArNS resolution and should generally not be adjusted at present.Changed Improved ClickHouse auto-import script with better error handling and continuous operation through errors.Reduced maximum header request rate per second to trusted node to load on community gateways.Optimized single owner and recipient queries on ClickHouse with specialized sorted tables.Used ID sorted ClickHouse table for ID queries to improve performance.Fixed Fixed data alignment in Parquet file name height boundaries to ensure consistent import boundaries.Removed trailing slashes from AO URLs to prevent issues when passing them to the SDK.Only prune SQLite data when ClickHouse import succeeds to prevent data loss during exports.[Release 29] - 2025-03-21 Changed Temporarily default to trusted gateway ArNS resolution to reduce CU load as much possible. On-demand CU resolution is still available as a fallback and the order can be modified by setting ARNS_RESOLVER_PRIORITY_ORDER.Remove duplicate network process call in on-demand resolver.Don't wait for network process debounces in the on-demand resolver.Slow network process dry runs no longer block fallback to next resolver.Added Added support for separate CUs URLs for the network and ANT processes via the NETWORK_AO_CU_URL and ANT_AO_CU_URL process URLs respectively. If either is missing the AO_CU_URL is used instead with a fallback to the SDK default URL if AO_CU_URL is also unspecified.Added CU URLs to on-demand ArNS resolver logs.Added circuit breakers for AR.IO network process CU dry runs. By default they use a 1 minute timeout and open after 30% failure over a 10 minute window and reset after 20 minutes.Fixed Owners in GraphQL results are now correctly retrieved from data based on offsets when using ClickHouse.[Release 28] - 2025-03-17 Changed Raised name not found name list refresh interval to 2 minutes to reduce load on CUs. This increases the maximum amount of time a user may wait for a new name to be available. Future releases will introduce other changes to mitigate this delay.Adjusted composite ArNS resolver to never timeout resolutions from the last ArNS resolver in the resolution list.Added Added support for serving a given ID or ArNS name from the apex domain of a gateway. If using an ID, set the APEX_TX_ID environment variable. If using an ArNS name, set the APEX_ARNS_NAME environment variable.Added BUNDLE_REPAIR_UPDATE_TIMESTAMPS_INTERVAL_SECONDS, BUNDLE_REPAIR_BACKFILL_INTERVAL_SECONDS, and BUNDLE_REPAIR_FILTER_REPROCESS_INTERVAL_SECONDS environment variables to control the interval for retrying failed bundles, backfilling bundle records, and reprocessing bundles after a filter change. Note: the latter two are rarely used. Queuing bundles for reprocessing via the /ar-io/admin/queue-bundle endpoint is usually preferable to automatic reprocessing as it is faster and offers more control over the reprocessing behavior.Fixed Signatures in GraphQL results are now correctly retrieved from data based on offsets when using ClickHouse.Adjusted exported Parquet file names to align with expectations of ClickHouse import script.Ensured that bundle indexing status is properly reset when bundles are manually queued after an unbundling filter change has been made.[Release 27] - 2025-02-20 Changed Set process IDs for mainnet.Increase default AO CU WASM memory limit to 17179869184 to support mainnet
process.[Release 26] - 2025-02-13 Added Added a per resolver timeout in the composite ArNS resolver. When the
composite resolver attempts resolution it is applied to each resolution
attempt. It is configurable via the ARNS_COMPOSITE_RESOLVER_TIMEOUT_MS and
defaults to 3 seconds in order to allow a fallback attempt before the default
observer timeout of 5 seconds.Added a TURBO_UPLOAD_SERVICE_URL environment variable to support
configuration of the bundler used by the observer (TurboSDK defaults are
used if not set).Added a REPORT_DATA_SINK environment variable that enables switching the
method used to post observer reports. With the default, turbo, it sends
data items via a Turbo compatible bundler. Switching it to arweave will
post base layer transactions directly to Arweave instead.Added a /ar-io/admin/bundle-status/<id> endpoint that returns the counters
and timestamps from the bundles row in data.db. This can be used for
monitoring unbundling progress and scripting (e.g., to skip requeuing already
queued bundles).Added more complete documentation for filters.Changed Use arweave.net as the default GraphQL URL for AO CUs since most gateways
will not have a complete local AO data item index.Use a default timeout of 5 seconds when refreshing Arweave peers to prevent
stalled peer refreshes.Cache selected gateway peer weights for the amount of time specified by the GATEWAY_PEERS_WEIGHTS_CACHE_DURATION_MS environment variable with a default
of 5 seconds to avoid expensive peer weight recomputation on each request.Chunk broadcasts to primary nodes occur in parallel with a concurrency limit
defaulting to 2 and configurable via the CHUNK_POST_CONCURRENCY_LIMIT environment variable.Added circuit breakers for primary chunk node POSTs to avoid overwhelming
chunk nodes when they are slow to respond.Fixed Properly cleanup timeout and event listener when terminating the data
root computation worker.Count chunk broadcast exceptions as errors in the arweave_chunk_broadcast_total metric.[Release 25] - 2025-02-07 Added Added support for indexing and querying ECDSA signed Arweave transactions.Expanded the OpenAPI specification to cover the entire gateway API and
commonly used Arweave node routes.ArNS undername record count limits are now enforced. Undernames are sorted
based on their ANT configured priority with a fallback to name comparisons
when priorities conflict or are left unspecified. Enforcement is enabled by
default but can be disabled by setting the ARNS_RESOLVER_ENFORCE_UNDERNAME_LIMIT to false.Changed Renamed the ario-peer data source to ar-io-peers for consistency and
clarity. ario-peer will continue to work for backwards compatibility but is
considered deprecated.Use AR.IO gateway peers from the ar.io gateway address registry (GAR) as the
last fallback for fetching data when responding to client data requests. This
has the benefit of making the network more resilient to trusted gateway
disruptions, but it can also result in nodes serving data from less trusted
sources if it is not found in the trusted gateway. This can be disabled by
using a custom ON_DEMAND_RETRIEVAL_ORDER that does not include ar-io-peers.Arweave data chunk requests are sent to the trusted node first with a
fallback to Arweave peers when chunks are unavailable on the trusted node.
This provides good performance by default with a fallback in case there are
issues retrieving chunks from the trusted node.Increased the observer socket timeout to 5 seconds to accommodate initial
slow responses for uncached ArNS resolutions.Disabled writing base layer Arweave signatures to the SQLite DB by default to
save disk space. When signatures are required to satisfy GraphQL requests,
they are retrieved from headers on the trusted node.Fixed Updated dependencies to address security issues.Improved reliability of failed bundle indexing retries.Fixed failure to compute data roots for verification for base layer data
larger than 2GiB.Fixed observer healthcheck by correcting node.js path in healthcheck script.[Release 24] - 2025-02-03 Added Added a ARNS_ANT_STATE_CACHE_HIT_REFRESH_WINDOW_SECONDS environment
variable that determines the number of seconds before the end of the TTL at
which to start attempting to refresh the ANT state.Added a TRUSTED_GATEWAYS_REQUEST_TIMEOUT_MS environment that defaults to
10,000 and sets the number of milliseconds to wait before timing out request
to trusted gateways.Added BUNDLE_REPAIR_RETRY_INTERVAL_SECONDS and BUNDLE_REPAIR_RETRY_BATCH_SIZE environment variables to control the time
between queuing batches of bundle retries and the number of data items
retrieved when constructing batches of bundles to retry.Added support for configuring the ar.io SDK log level via the AR_IO_SDK_LOG_LEVEL environment variable.Added a request_chunk_total Prometheus counter with status, source (a
URL) and source_type (trusted or peer) labels to track success/failure
of chunk retrieval in the Arweave network per source.Added a get_chunk_total Prometheus metric to count chunk retrieval
success/failure per chunk.Added arns_cache_hit_total and arns_cache_miss_total Prometheus counters
to track ArNS cache hits and misses for individual names respectively.Added arns_name_cache_hit_total and arns_name_cache_miss_total Prometheus
counters to track ArNS name list cache hits and misses
respectively.Added a arns_resolution_duration_ms Prometheus metric that tracks summary
statistics for the amount of time it takes to resolve ArNS names.Changed In addition to the trusted node, the Arweave network is now searched for
chunks by default. All chunks retrieved are verified against data roots
indexed from a trusted Arweave node to ensure their validity.Default to a 24 hour cache TTL for the ArNS name cache. Record TTLs still
override this, but in cases where resolution via AO CU is slow or fails, the
cache will be used. In the case of slow resolution, CU based resolution will
proceed in the background and update the cache upon completion.Switched to the ioredis library for better TLS support.Updated minor dependency minor versions (more dependencies will be updated in
the next release).Bundles imports will no longer be re-attempted for bundles that have already
been fully unbundled using the current filters if they are matched or
manually queued again.Replaced references docker-compose in the docs with the more modern docker compose.Fixed Ensure duplicate data item IDs are ignored when comparing counts to determine
if a bundle has been fully unbundled.Fixed worker threads failing to shut down properly when the main process
stopped.Ensure bundle import attempt counts are incremented when bundles are skipped
to avoid repeatedly attempting to import skipped bundles.Use observe that correctly ensure failing gateways are penalized in the AR.IO
AO process.[Release 23] - 2025-01-13 Added Added FS_CLEANUP_WORKER_BATCH_SIZE,FS_CLEANUP_WORKER_BATCH_PAUSE_DURATION, and FS_CLEANUP_WORKER_RESTART_PAUSE_DURATION environment variables to allow
configuration of number of contiguous data files cleaned up per batch, the
pause between each batch, and the pause before restarting the entire cleanup
process again.Added data_items_unbundled_total Prometheus metric that counts the total
number of data items unbundled, including those that did not match the
unbundling filter.Added a parent_type label that can be one of transaction or data_item to data item indexing metrics.Added a files_cleaned_total total Prometheus metric to enable monitoring of
contiguous data cleanup.Added support for specifying the admin API via a file specified by the ADMIN_API_KEY_FILE environment variable.Added experimental support for posting chunks in a non-blocking way to
secondary nodes specified via a comma separate list in the SECONDARY_CHUNK_POST_URLS environment variable.Changed Renamed the parent_type lable to contiguous_data_type on bundle metrics
to more accurately reflect the meaning of the label.Reduced the maximum time to refresh the ArNS name list to 10 seconds to
minimize delays in ArNS availability after a new name is registered.Changed /ar-io/admin/queue-bundle to wait for bundles rows to be written
to the DB before responding to ensure that errors that occur due to DB
contention are not silently ignored.Data items are now flushed even when block indexing is stopped. This allows
for indexing batches of data items using the admin API with block indexing
disabled.Adjust services in docker-compose to use unless-stopped as their restart
policy. This guards against missing restarts in the case where service
containers exit with a success status even when they shouldn't.Fixed Added missing created_at field in blocked_names table.Fixed broken ArNS undername resolution.[Release 22] - 2024-12-18 Added Added the ability to block and unblock ArNS names (e.g., to comply with hosting provider TOS). To block a name, POST { "name": "<name to block>" } to /ar-io/admin/block-name. To unblock a name, POST { "name": "<name to unblock>" } to /ar-io/admin/unblock-name.Changed Return an HTTP 429 response to POSTs to /ar-io/admin/queue-bundle when the bundle data import queue is full so that scripts queuing bundles can wait rather than overflowing it.Fixed Adjust ArNS length limit from <= 48 to <= 51 to match the limit enforced by the AO process.[Release 21] - 2024-12-05 Added Added a ClickHouse auto-import service. When enabled, it calls the Parquet export API, imports the exported Parquet into ClickHouse, moves the Parquet files to an imported subdirectory, and deletes data items in SQLite up to where the Parquet export ended. To use it, run Docker Compose with the clickhouse profile, set the CLICKHOUSE_URL to http://clickhouse:8123, and ensure you have set an ADMIN_KEY. Using this configuration, the core service will also combine results from ClickHouse and SQLite when querying transaction data via GraphQL. Note: if you have a large number of data items in SQLite, the first export and subsequent delete may take an extended period. Also, this functionality is considered experimental. We expect there are still bugs to be found in it and we may make breaking changes to the ClickHouse schema in the future. If you choose to use it in production (not yet recommended), we suggest backing up copies of the Parquet files found in data/parquet/imported so that they can be reimported if anything goes wrong or future changes require it.Added a background data verification process that will attempt to recompute data roots for bundles and compare them to data roots indexed from Arweave nodes. When the data roots match, all descendant data items will be marked as verified. This enables verification of data initially retrieived from sources, like other gateways, that serve contiguous data instead of verifiable chunks. Data verification can be enabled by setting the ENABLE_BACKGROUND_DATA_VERIFICATION environment variable to true. The interval between attempts to verify batches of bundles is configurable using the BACKGROUND_DATA_VERIFICATION_INTERVAL_SECONDS environment variable.Added a CHUNK_POST_MIN_SUCCESS_COUNT environment variable to configure how many Arweave nodes must accept a chunk before a chunk broadcast is considered successful.Added arweave_chunk_post_total and arweave_chunk_broadcast_total Prometheus metrics to respectively track the number of successful chunk POSTs to Arweave nodes and the number of chunks successfully broadcast.When resolving ArNS names, the entire list of names is now cached instead of individually checking whether each name exists. This reduces the load on AO CUs since the entire list can be reused across multiple requests for different names. Note: due to the default 5 minute interval between name list refreshes, newly registered may now take longer to resolver after initial registration. We intend to make further caching refinements to address this in the future.Added support for multiple prioritized trusted gateways configurable by setting the TRUSTED_GATEWAYS_URLS environment variable to a JSON value containing a mapping of gateway hosts to priorities. Data requests are sent to other gateways in ascending priority order. If multiple gateways share the same priority, all the gateways with the same priority are tried in a random order before continuing on to the next priority.Added support for caching contiguous data in S3. It is enabled by default when the AWS_S3_CONTIGUOUS_DATA_BUCKET and AWS_S3_CONTIGUOUS_DATA_PREFIX environment variables are set.Changed trusted-gateway was changed to trusted-gateways in ON_DEMAND_RETRIEVAL_ORDER and BACKGROUND_RETRIEVAL_ORDER.Renamed the S3 contiguous environment variables - AWS_S3_BUCKET to AWS_S3_CONTIGUOUS_DATA_BUCKET and AWS_S3_PREFIX to AWS_S3_CONTIGUOUS_DATA_PREFIX.[Release 20] - 2024-11-15 Added Exposed the core service chunk POST endpoint via Envoy. It accepts a Arweave data chunk and broadcasts it to either the comma separated list of URLs specified by the CHUNK_POST_URLs environment variable or, if none are specified, the /chunk path on URL specified by the TRUST_GATEWAY_URL environment variable.Added a X-AR-IO-Root-Transaction-Id HTTP header to data responses containing the root base layer transaction ID for the ID in question if it's been indexed.Added a X-AR-IO-Data-Item-Data-Offset HTTP header containing the offset of the data item relative to the root bundle base layer transaction for it. In conjunction with X-AR-IO-Root-Transaction-Id, it enables retrieving data for data item IDs from base layer data using first a HEAD request to retrieve the root ID and data offset followed by a range request into the root bundle. This greatly increases the likelihood of retriving data item data by ID since only an index into the base layer and Arweave chunk availability is needed for this access method to succeed.Added an experimental ClickHouse service to docker-compose.yaml (available via the clickhouse profile). This will be used as a supplemental GraphQL DB in upcoming releases.Added a data item indexing healthcheck that can be enabled by setting the RUN_AUTOHEAL environment variable to true. When enabled, it will restart the core service if no data items have been indexed since the value specified by the MAX_EXPECTED_DATA_ITEM_INDEXING_INTERVAL_SECONDS environment variable.[Release 19] - 2024-10-21 Fixed Adjusted data item flushing to use the bundle DB worker instead of the core DB worker to prevent write contention and failed flushes under heavy unbundling load.Added Added X-AR-IO-Digest, X-AR-IO-Stable, X-AR-IO-Verified, and ETag headers. X-AR-IO-Digest contains a base64 URL encoded representation of the SHA-256 hash of the data item data. It may be empty if the gateway has not previously cached the data locally. X-AR-IO-Stable contains either true or false depending on whether the associated Arweave transaction is more than 18 blocks old or not. X-AR-IO-Verified contains either true if the gateway has verified the data root of the L1 transaction or the L1 root parent of the data item or false if it has not. ETag contains the same value a X-AR-IO-Digest and is used to improve HTTP caching efficiency.Added support for using a different data source for on-demand and background data retrieval. Background data retrieval is used when unbundling. The background retrieval data source order is configurable using the BACKGROUND_RETRIEVAL_ORDER environment variable and defaults to chunks,s3,trusted-gateway,tx-data. Priority is given to chunk retrieval since chunks are verifiable.Added an /ar-io/admin/export-parquet/status to support monitoring of in-progress Parquet export status.Added sqlite_in_flight_ops Prometheus metric with worker (core, bundles, data, or moderation) and role (read or write) labels to support monitoring the number of in-flight DB operations.Added experimental Grafana and Prometheus based observability stack. See the "Monitoring and Observability" section of the README for more details.Changed Bundle data is now retrieved as chunks from Arweave nodes by default so that data roots can be compared against the chain (see entry about background retrieval above).Changed observer configuration to use 8 instead of 5 chosen names. These are combined with 2 names prescribed from the contract for a total of 10 names observed each epoch to provide increased ArNS observation coverage.Verification status is set on data items when unbundling a parent that has already been verified.[Release 18] - 2024-10-01 Fixed Improved performance of data attributes query that was preventing data.db WAL flushing.Added Added WAL sqlite_wal_checkpoint_pages Prometheus metric to help monitor WAL flushing.Added a POST /ar-io/admin/export-parquet endpoint that can be used to export the contents of the SQLite3 core and bundle DBs as Parquet. To trigger an export, POST JSON containing outputDir, startHeight, endHeight, and maxFileRows keys. The resulting Parquet files can then be queried directly using DuckDB or loaded into another system (e.g. ClickHouse). Scripts will be provided to help automate the latter in a future release.Added ARNS_RESOLVER_OVERRIDE_TTL_SECONDS that can be used to force ArNS names to refresh before their TTLs expire.Added a GET /ar-io/resolver/:name endpoint that returns an ArNS resolution for the given name.Changed Removed ArNS resolver service in favor of integrated resolver. If a standalone resolver is still desired, the core service can be run with the START_WRITERS environment variable set to false. This will disable indexing while preserving resolver functionality.Deduplicated writes to data.db to improve performance and reduce WAL growth rate.[Release 17] - 2024-09-09 Notes This release includes a LONG RUNNING MIGRATION. Your node may appear unresponsive while it is running. It is best to wait for it to complete. If it fails or is interrupted, removing your SQLite DBs (in data/sqlite by default) should resolve the issue, provided you are willing to lose your GraphQL index and let your node rebuild it.Fixed Use the correct environment variable to populate WEBHOOK_BLOCK_FILTER in docker-compose.yaml.Don't cache data regions retrieved to satisfy range requests to avoid unnecessary storage overhead and prevent inserting invalid ID to hash mappings into the data DB.Added Added a new ClickHouse based DB backend. It can be used in combination with the SQLite DB backend to enable batch loading of historical data from Parquet. It also opens up the possibility of higher DB performance and scalability. In its current state it should be considered a technology preview. It won't be useful to most users until we either provide Parquet files to load into it or automate flushing of the SQLite DB to it (both are planned in future release). It is not intended to be standalone solution. It supports bulk loading and efficient GraphQL querying of transactions and data items, but it relies on SQLite (or potentially another OLTP in the future) to index recent data. These limitations allow greatly simplified schema and query construction. Querying the new ClickHouse DB for transaction and data items via GraphQL is enabled by setting the CLICKHOUSE_URL environment variable.Added the ability to skip storing transaction signatures in the DB by setting WRITE_TRANSACTION_DB_SIGNATURES to false. Missing signatures are fetched from the trusted Arweave node when needed for GraphQL results.Added a Redis backed signature cache to support retrieving optimistically indexed data item signatures in GraphQL queries when writing data items signatures to the DB has been disabled.Added on-demand and composite ArNS resolvers. The on-demand resolver fetches results directly from an AO CU. The composite resolver attempts resolution in the order specified by the ARNS_RESOLVER_PRIORITY_ORDER environment variable (defaults to on-demand,gateway).Added a queue_length Prometheus metric to fasciliate monitoring queues and inform future optimizations Added SQLite WAL cleanup worker to help manage the size of the data.db-wal file. Future improvements to data.db usage are also planned to further improve WAL management.Changed Handle data requests by ID on ArNS sites. This enables ArNS sites to use relative links to data by ID.Replaced ARNS_RESOLVER_TYPE with ARNS_RESOLVER_PRIORITY_ORDER (defaults to on-demand,gateway).Introduced unbundling back pressure. When either data item data or GraphQL indexing queue depths are more than the value specified by the MAX_DATA_ITEM_QUEUE_SIZE environment variable (defaults to 100000), unbundling is paused until the queues length falls bellow that threshold. This prevents the gateway from running out of memory when the unbundling rate exceeds the indexing rate while avoiding wasteful bundle reprocessing.Prioritized optimistic data item indexing by inserting optimistic data items at the front of the indexing queues.Prioritized nested bundle indexing by inserting nested bundles at the front of the unbundling queue.[Release 16] - 2024-08-09 Fixed Fixed promise leak caused by missing await when saving data items to the DB.Modified ArNS middleware to not attempt resolution when receiving requests for a different hostname than the one specified by ARNS_ROOT_HOST.Added Added support for returning Content-Encoding HTTP headers based on user specified Content-Encoding tags.Added isNestedBundle filter enables that matches any nested bundle when indexing. This enables composite unbundling filters that match a set of L1 tags and bundles nested under them.Added ability to skip writing ANS-104 signatures to the DB and load them based on offsets from the data instead. This significantly reduces the size of the bundles DB. It can be enabled by setting the WRITE_ANS104_DATA_ITEM_DB_SIGNATURES environment variable to false.Added data_item_data_indexed_total Prometheus counter to count data items with data attributes indexed.Changed Queue data attributes writes when serving data rather than writing them syncronously.Reduced the default data indexer count to 1 to lessen the load on the data DB.Switched a number of overly verbose info logs to debug level.Removed docker-compose on-failure restart limits to ensure that services restart no matter how many times they fail.Modified the data_items_indexed_total Prometheus counter to count data items indexed for GraphQL querying instead of data attributes.Increased aggressiveness of contiguous data cleanup. It now pauses 5 seconds instead of 10 seconds per batch and runs every 4 hours instead of every 24 hours.[Release 15] - 2024-07-19 Fixed Fixed query error that was preventing bundles from being marked as fully imported in the database.Added Adjusted data item indexing to record data item signature types in the DB. This helps distinguish between signatures using different key formats, and will enable querying by signature type in the future.Adjusted data item indexing to record offsets for data items within bundles and signatures and owners within data items. In the future this will allow us to avoid saving owners and signatures in the DB and thus considerably reduce the size of the bundles DB.Added ARNS_CACHE_TTL_MS environment variable to control the TTL of ARNS cache entries (defaults to 1 hour).Added support for multiple ranges in a single HTTP range request.Added experimental chunk POST endpoint that broadcasts chunks to the comma-separate list of URLS in the CHUNK_BROADCAST_URLS environment variable. It is available at /chunk on the internal gateway service port (4000 by default) but is not yet exposed through Envoy.Added support for running an AO CU adjacent to the gateway (see README.md for details).Added X-ArNS-Process-Id to ArNS resolved name headers.Added a set of AO_... environment variables for specifying which AO URLs should be used (see docker-compose.yaml for the complete list). The AO_CU_URL is of particular use since the core and resolver services only perform AO reads and only the CU is needed for reads.Changed Split the monolithic docker-compose.yaml into docker-compose.yaml, docker-compose.bundler.yaml, and docker-compose.ao.yaml (see README for details).Replaced references to 'docker-compose' with 'docker compose' in the docs since the former is mostly deprecated.Reduce max fork depth from 50 to 18 inline to reflect Arweave 2.7.2 protocol changes.Increased the aggressiveness of bundle reprocessing by reducing reprocessing interval from 10 minutes to 5 minutes and raising reprocessing batch size from 100 to 1000.Use a patched version of Litestream to work around insufficient S3 multipart upload size in the upstream version.[Release 14] - 2024-06-26 Fixed Correctly handle manifest index after paths.[Release 13] - 2024-06-24 Added Added support for optimistically reading data items uploaded using the integrated Turbo bundler via the LocalStack S3 interface.Added X-AR-IO-Origin-Node-Release header to outbound data requests.Added hops, origin, and originNodeRelease query params to outbound data requests.Added support for fallback in v0.2 manifests that is used if no path in the manifest is matched.Changed Updated Observer to read prescribed names from and write observations to the ar.io AO network process.Updated Resolver to read from the ar.io AO network process.Fixed Modified optimistic indexing of data items to use a null parent_id when inserting into the DB instead of a placeholder value. This prevents unexpected non-null bundledIn values in GraphQL results for optimistically indexed data items.Modified GraphQl query logic to require an ID for single block GraphQL queries. Previously queries missing an ID were returning an internal SQLite error. This represents a small departure from arweave.net's query logic which returns the latest block for these queries. We recommend querying blocks instead of block in cases where the latest block is desired.Adjusted Observer health check to reflect port change to 5050.Security Modified docker-compose.yaml to only expose Redis, PostgreSQL, and LocalStack ports internally. This protects gateways that neglect to deploy behind a firewall, reverse proxy, or load balancer.[Release 12] - 2024-06-05 Added Added /ar-io/admin/queue-data-item endpoint for queuing data item headers for indexing before the bundles containing them are processed. This allows trusted bundlers to make their data items quickly available to be queried via GraphQL without having to wait for bundle data submission or unbundling.Added experimental support for retrieving contiguous data from S3. See AWS_* environment variables documentation for configuration details. In conjuction with a local Turbo bundler this allows optimistic bundle (but not yet data item) retrieval.Add experimental support for fetching data from gateway peers. It can be enabled by adding ario-peer to ON_DEMAND_RETRIEVAL_ORDER. Note: do not expect this work reliably yet! This functionality is in active development and will be improved in future releases.Add import_attempt_count to bundle records to enable future bundle import retry optimizations.Changed Removed version from docker-compose.yaml to avoid warnings with recent versions of docker-compose.Switched default observer port from 5000 to 5050 to avoid conflict on OS X. Since Envoy is used to provide external access to the observer API this should have no user visible effect.[Release 11] - 2024-05-21 Added Added arweave_tx_fetch_total Prometheus metric to track counts of transaction headers fetched from the trusted node and Arweave network peers.Changed Revert to using unnamed bind mounts due to cross platform issues with named volumes.[Release 10] - 2024-05-20 Added Added experimental support for streaming SQLite backups to S3 (and compatible services) using Litestream. Start the service using the docker-compose "litestream" profile to use it, and see the AR_IO_SQLITE_BACKUP_* environment variables documentation for further details.Added /ar-io/admin/queue-bundle endpoint for queueing bundles for import for import before they're in the mempool. In the future this will enable optimistic indexing when combined with a local trusted bundler.Added support for triggering webhooks when blocks are imported matching the filter specified by the WEBHOOK_BLOCK_FILTER environment variable.Added experimental support for indexing transactions and related data items from the mempool. Enable it by setting ENABLE_MEMPOOL_WATCHER to 'true'.Made on-demand data caching circuit breakers configurable via the GET_DATA_CIRCUIT_BREAKER_TIMEOUT_MS environment variable. This allows gateway operators to decide how much latency they will tolerate when serving data in exchange for more complete data indexing and caching.Rename cache header from X-Cached to X-Cache to mimic typical CDN practices.Add X-AR-IO-Hops and X-AR-IO-Origin headers in preparation for future peer-to-peer functionality.Upgrade to Node.js v20 and switch to native test runner.[Release 9] - 2024-04-10 Added Added experimental Farcaster Frames support, enabling simple Arweave based Frames with button navigation. Transaction and data item data is now served under /local/farcaster/frame/<ID>. /local is used as a prefix to indicate this functionality is both experimental and local to a particular gateway rather than part of the global gateway API. Both GET and POST requests are supported.Added an experimental local ArNS resolver. When enabled it removes dependence on arweave.net for ArNS resolution! Enable it by setting RUN_RESOLVER=TRUE, TRUSTED_ARNS_RESOLVER_TYPE=resolver, and TRUSTED_ARNS_RESOLVER_URL=http://resolver:6000 in your .env file.Added an X-Cached header to data responses to indicate when data is served from the local cache rather than being retrieved from an external source. This is helpful for interfacing with external systems, debugging, and end-to-end testing.Save hashes for unbundled data items during indexing. This enables reduction in data storage via hash based deduplication as well as more efficient peer-to-peer data retrieval in the future.[Release 8] - 2024-03-14 Added Added GraphQL SQL query debug logging to support trouble-shooting and performance optimization.Added support for indexing data items (not GraphQL querying) based solely on tag name. (example use case: indexing all IPFS CID tagged data items).Changes Observer data sampling now uses randomized ranges to generate content hashes.Reference gateway ArNS resolutions are now cached to improve report generation performance.Contract interactions are now tested before posting using dryWrite to avoid submitting interactions that would fail./ar-io/observer/info now reports INVALID for wallets that fail to load.Fixed Fix data caching failure caused by incorrect method name in getData circuit breakers.Fix healthcheck when ARNS_ROOT_HOST includes a subdomain.[Release 7] - 2024 - 02 - 14 Added Add support for notifying other services of transactions and data items using webhooks (see README for details).Add support for filter negation (particularly useful for excluding large bundles from indexint).Improve unbundling throughput by decoupling data fetching from unbundling.Add Envoy and core service ARM builds.Changed Improve resouce cleanup and shutdown behavior.Don't save Redis data to disk by default to help prevent memory issues on startup for small gateways.Reduce the amount of data sampled from large files by the observer.Ensure block poa2 field is not chached to reduce memory consumption.[Release 6] - 2024-01-29 Fixed Update observer to improve reliability of contract state synchronization and evaluation.[Release 5] - 2024-01-25 Added Added transaction offset indexing to support future data retrieval capabilities.Enabled IPv6 support in Envoy config.Added ability to configure observer report generation interval via the REPORT_GENERATION_INTERVAL_MS environmental variable. (Intended primarily for development and testing) Changed Updated observer to properly handle FQDN conflicts.Renamed most created_at columns to index to indexed_at for consistency and clarity.Fixed Updated LMDB version to remove Buffer workaround and fix occasional block cache errors.[Release 4] - 2024-01-11 Added Added circuit breakers around data index access to reduce impact of DB access contention under heavy requests loads.Added support for configuring data source priority via the ON_DEMAND_RETRIEVAL_ORDER environment variable.Updated observer to a version that retrieves epoch start and duration from contract state.Changed Set the Redis max memory eviction policy to allkeys-lru.Reduced default Redis max memory from 2GB to 256MB.Improved predictability and performance of GraphQL queries.Eliminated unbundling worker threads when filters are configured to skip indexing ANS-104 bundles.Reduced the default number of ANS-104 worker threads from 2 to 1 when unbundling is enabled to conserve memory.Increased nodejs max old space size to 8GB when ANS-104 workers > 1.Fixed Adjusted paths for chunks indexed by data root to include the full data root.[Release 3] - 2023-12-05 Added Support range requests (PR 61, PR 64) Note: serving multiple ranges in a single request is not yet supported.Release number in /ar-io/info response.Redis header cache implementation (PR 62).New default header cache (replaces old FS cache).LMDB header cache implementation (PR 60).Intended for use in development only.Enable by setting CHAIN_CACHE_TYPE=lmdb.Filesystem header cache cleanup worker (PR 68).Enabled by default to cleanup old filesystem cache now that Redis is the new default.Support for parallel ANS-104 unbundling (PR 65).Changed Used pinned container images tags for releases.Default to Redis header cache when running via docker-compose.Default to LMDB header cache when running via yarn start.Fixed Correct GraphQL pagination for transactions with duplicate tags.

---

# 115. ARIO Node Release Notes - ARIO Docs

Document Number: 115
Source: https://docs.ar.io/gateways/release-notes#
Words: 7699
Extraction Method: html

AR.IO Release Notes Overview Welcome to the documentation page for the AR.IO gateway release notes. Here, you will find detailed information about each version of the AR.IO gateway, including the enhancements, bug fixes, and any other changes introduced in every release. This page serves as a comprehensive resource to keep you informed about the latest developments and updates in the AR.IO gateway. For those interested in exploring the source code, each release's code is readily accessible at our GitHub repository: AR.IO gateway change logs. Stay updated with the continuous improvements and advancements in the AR.IO gateway by referring to this page for all release-related information.[Release 41] - 2025-06-30 Added Added preferred chunk GET node URLs configuration via PREFERRED_CHUNK_GET_NODE_URLS environment variable to enable chunk-specific peer prioritization. Preferred URLs receive a weight of 100 for prioritization and the system selects 10 peers per attempt by default.Added hash validation for peer data fetching by including X-AR-IO-Expected-Digest header in peer requests when hash is available, validating peer responses against expected hash, and immediately rejecting mismatched data.Added DOCKER_NETWORK_NAME environment variable to configure the Docker network name used by Docker Compose.Added draft guide for running a community gateway.Added draft data verification architecture document.Changed Removed trusted node fallback for chunk retrieval. Chunks are now retrieved exclusively from peers, with the retry count increased from 3 to 50 to ensure reliability without the trusted node fallback.Fixed Fixed inverted logic preventing symlink creation in FsChunkDataStore.Fixed Content-Length header for range requests and 304 responses, properly setting header for single and multipart range requests and removing entity headers from 304 Not Modified responses per RFC 7232.Fixed MaxListenersExceeded warnings by adding setMaxListeners to read-through data cache.Fixed potential memory leaks in read-through data cache by using once instead of on for error and end event listeners.[Release 40] - 2025-06-23 This is an optional release that primarily improves caching when data is fetched from peers.Added Added experimental flush-to-stable script for manual database maintenance. This script allows operators to manually flush stable chain and data item tables, mirroring the logic of StandaloneSqliteDatabase.flushStableDataItems. WARNING: This script is experimental and directly modifies database contents. Use with caution and ensure proper backups before running.Changed Replaced yesql with custom SQL loader that handles comments better, improving SQL file parsing and maintenance.Switched to SPDX license headers to reduce LLM token usage, making the codebase more efficient for AI-assisted development.Improved untrusted data handling and hash validation in cache operations. The cache now allows caching when a hash is available for validation even for untrusted data sources, but only finalizes the cache when the computed hash matches a known trusted hash. This prevents cache poisoning while still allowing data caching from untrusted sources when the data can be validated.[Release 39] - 2025-06-17 This release enhances observability and reliability with new cache metrics, improved data verification capabilities, and automatic failover between chain data sources. The addition of ArNS-aware headers enables better data prioritization across the gateway network. This is a recommended but not urgent upgrade.Added Added filesystem cache metrics with cycle-based tracking. Two new Prometheus metrics track cache utilization: cache_objects_total (number of objects in cache) and cache_size_bytes (total cache size in bytes). Both metrics include store_type and data_type labels to differentiate between cache types (e.g., headers, contiguous_data). Metrics are updated after each complete cache scan cycle, providing accurate visibility into filesystem cache usage.Added X-AR-IO-Data-Id header to all data responses. This header shows the actual data ID being served, whether from a direct ID request or manifest path resolution, providing transparency about the content being delivered.Added automatic data item indexing when data verification is enabled. When ENABLE_BACKGROUND_DATA_VERIFICATION is set to true, the system now automatically enables data item indexing (ANS104_UNBUNDLE_FILTER) with an always: true filter if no filter is explicitly configured. This ensures bundles are unbundled to verify that data items are actually contained in the bundle associated with the Arweave transaction's data root.Added ArNS headers to outbound gateway requests to enable data prioritization. The generateRequestAttributes function now includes ArNS context headers (X-ArNS-Name, X-ArNS-Basename, X-ArNS-Record) in requests to other gateways and Arweave nodes, allowing downstream gateways to effectively prioritize ArNS data requests.Added configurable Docker Compose host port environment variables (CORE_PORT, ENVOY_PORT, CLICKHOUSE_PORT, CLICKHOUSE_PORT_2, CLICKHOUSE_PORT_3, OBSERVER_PORT) to allow flexible port mapping while maintaining container-internal port compatibility and security.Added Envoy aggregate cluster configuration for automatic failover between primary and fallback chain data sources. The primary cluster (default: arweave.net:443) uses passive outlier detection while the fallback cluster (default: peers.arweave.xyz:1984) uses active health checks. This enables zero-downtime failover between HTTPS and HTTP endpoints with configurable FALLBACK_NODE_HOST and FALLBACK_NODE_PORT environment variables.Changed Streamlined background data retrieval to reduce reliance on centralized sources. The default BACKGROUND_RETRIEVAL_ORDER now only includes chunks,s3, removing trusted-gateways and tx-data from the default configuration. This prioritizes verifiable chunk data and S3 storage for background operations like unbundling.Removed ar-io.net from default trusted gateways list and removed TRUSTED_GATEWAY_URL default value to reduce load on ar-io.net now that P2P data retrieval is re-enabled. Existing deployments with TRUSTED_GATEWAY_URL explicitly set will continue to work for backwards compatibility.[Release 38] - 2025-06-09 This release focuses on data integrity and security improvements, introducing trusted data verification and enhanced header information for data requests. Upgrading to this release is recommended but not urgent.Added Added X-AR-IO-Trusted header to indicate data source trustworthiness in responses. This header helps clients understand whether data comes from a trusted source and works alongside the existing X-AR-IO-Verified header to provide data integrity information. The system now filters peer data by requiring peers to indicate their content is either verified or trusted, protecting against misconfigured peers that may inadvertently serve unintended content (e.g., provider default landing pages) instead of actual Arweave data.Added If-None-Match header support for HTTP conditional requests enabling better client-side caching efficiency. When clients send an If-None-Match header that matches the ETag, the gateway returns a 304 Not Modified response with an empty body, reducing bandwidth usage and improving performance.Added digest and hash headers for data HEAD requests to enable client-side data integrity verification.Added EC2 IMDS (instance-profile) credential support for S3 data access, improving AWS authentication in cloud environments.Added trusted data flag to prevent caching of data from untrusted sources, ensuring only verified and reliable content is stored locally while still allowing serving of untrusted data when necessary.Changed Re-enabled ar-io-peers as fallback data source in configuration for improved data availability.Updated trusted node configuration to use arweave.net as the default trusted node URL.Updated ETag header format to use properly quoted strings (e.g., "hash" instead of hash) following HTTP/1.1 specification standards for improved compatibility with caching proxies and clients.[Release 37] - 2025-06-03 This is a recommended release due to the included observer robustness improvements. It also adds an important new feature - data verification for preferred ArNS names. When preferred ArNS names are set, the bundles containing the data they point to will be locally unbundled (verifying data item signatures), and the data root for the bundle will be compared to the data root in the Arweave chain (establishing that the data is on Arweave). To enable this feature, set your preferred ArNS names, turn on unbundling by setting ANS104_DOWNLOAD_WORKERS and ANS104_UNBUNDLE_WORKERS both to 1, and set your ANS104_INDEX_FILTER to a filter that will match the data items for your preferred names. If you don't know the filter, use {"always": true}, but be aware this will index the entire bundle for the IDs related to your preferred names.Note: this release contains migrations to data.db. If your node appears unresponsive please check core service logs to determine whether migrations are running and wait for them to finish.Added Added prioritized data verification system for preferred ArNS names, focusing computational resources on high-priority content while enabling flexible root transaction discovery through GraphQL fallback support.Added verification retry prioritization system with tracking of retry counts, priority levels, and attempt timestamps to ensure bundles do not get stuck retrying forever.Added improved observer functionality with best-of-2 observations and higher compression for more reliable network monitoring.Added MAX_VERIFICATION_RETRIES environment variable (default: 5) to limit verification retry attempts and prevent infinite loops for consistently failing data items.Added retry logic with exponential backoff for GraphQL queries to handle rate limiting (429) and server errors with improved resilience when querying trusted gateways for root bundle IDs.Changed Updated dependencies: replaced deprecated express-prometheus-middleware with the actively maintained express-prom-bundle library and updated prom-client to v15.1.3 for better compatibility and security.Updated Linux setup documentation to use modern package installation methods, replacing apt-key yarn installation with npm global install and updating Node.js/nvm versions.Improved route metrics normalization with explicit whitelist function for better granularity and proper handling of dynamic segments.Fixed Fixed docker-compose configuration to use correct NODE_MAX_OLD_SPACE_SIZE environment variable name.Fixed production TypeScript build configuration to exclude correct "test" directory path.Fixed Parquet exporter to properly handle data item block_transaction_index exports, preventing NULL value issues.Fixed bundles system to copy root_parent_offset when flushing data items to maintain data integrity.Fixed ClickHouse auto-import script to handle Parquet export not_started status properly.Fixed docker-compose ClickHouse configuration to not pass conflicting PARQUET_PATH environment variable to container scripts.Fixed verification process for data items that have not been unbundled by adding queue bundle support and removing bundle join constraint to ensure proper verification of data items without indexed root parents.[Release 36] - 2025-05-27 This is a recommended but not essential upgrade. The most important changes are the preferred ArNS caching feature for improved performance on frequently accessed content and the observer's 80% failure threshold to prevent invalid reports during network issues.Added Added preferred ArNS caching functionality that allows configuring lists of ArNS names to be cached longer via PREFERRED_ARNS_NAMES and PREFERRED_ARNS_BASE_NAMES environment variables. When configured, these names will be cleaned from the filesystem cache after PREFERRED_ARNS_CONTIGUOUS_DATA_CACHE_CLEANUP_THRESHOLD instead of the standard cleanup threshold (CONTIGUOUS_DATA_CACHE_CLEANUP_THRESHOLD). This is accomplished by maintaining an MRU (Most Recently Used) list of ArNS names in the contiguous metadata cache. When filesystem cleanup runs, it checks this list to determine which cleanup threshold to apply. This feature enables gateway operators to ensure popular or important ArNS names remain cached longer, improving performance for frequently accessed content.Added ArNS headers to responses: X-ArNS-Name, X-ArNS-Basename, and X-ArNS-Record to help identify which ArNS names were used in the resolution.Changed Updated observer to prevent report submission when failure rate exceeds 80%. This threshold helps guard against both poorly operated observers and widespread network issues. In the case of a widespread network issue, the assumption is that most gateway operators are well intentioned and will work together to troubleshoot and restore both observations and network stability, rather than submitting reports that would penalize functioning gateways.Updated default trusted gateway in docker-compose Envoy configuration to ar-io.net for improved robustness and alignment with core service configuration.Improved range request performance by passing ranges directly to getData implementations rather than streaming all data and extracting ranges.Fixed Fixed missing cache headers (X-Cache and other data headers) in range request responses to ensure consistent cache header behavior across all request types.Fixed async streaming for multipart range requests by using async iteration instead of synchronous reads, preventing potential data loss.Fixed ArNS resolution to properly exclude www subdomain from resolution logic.Fixed test reliability issues by properly awaiting stream completion before making assertions.Fixed chunk broadcasting to not await peer broadcasts, as they are best-effort operations.[Release 35] - 2025-05-19 This is a low upgrade priority release. It contains a small caching improvement and routing fix. Upgrading to help test it is appreciated but not essential.Changed Adjusted filesystem data expiration to be based on last request times rather than file access times which may be inaccurate.Adjusted CORS headers to include content-* headers.Fixed Fixed regex used to expose /api-docs when an apex ArNS name is set.[Release 34] - 2025-05-05 Given the resilience provided by adding a second trusted gateway URL, it is recommended that everyone upgrade to this release.Added Added peer list endpoints for retrieving information about Arweave peers and ar.io gateway peers.Added ar-io.net as a secondary trusted gateway to increase data retrieval resilience by eliminating a single point of failure.Added circuit breaker for Arweave peer chunk posting.Changed Created directories for DuckDB and Parquet to help avoid permission issues by the directories being created by containers.Fixed Fixed GraphQL ClickHouse error when returning block ID and timestamp.Fixed the tx-chunks-data-source to throw a proper error (resulting in a 404) when the first chunk is missing rather than streaming a partial response.[Release 33] - 2025-05-05 Added Added a [Parquet and ClickHouse usage guide]. Using ArDrive as an example, it provides step by step instructions about how to bulk load Parquet and configure continuous ingest of bundled data items into ClickHouse. This allows the ar-io-node to support performant GraphQL queries on larger data sets and facilitates sharing indexing work across gateways via distribution of Parquet files.Added support for configurable ArNS 404 pages using either:ARNS_NOT_FOUND_TX_ID: Transaction ID for custom 404 content ARNS_NOT_FOUND_ARNS_NAME: ArNS name to resolve for 404 content Added experimental /chunk/ GET route for serving chunk data by absolute offset either the local cache.Added support for AWS_SESSION_TOKEN in the S3 client configuration.Expanded ArNS OTEL tracing to improve resolution behavior observability.Added support for setting a ClickHouse username and password via the CLICKHOUSE_USERNAME and CLICKHOUSE_PASSWORD environment variable. When using ClickHouse, CLICKHOUSE_PASSWORD should always be set. However, CLICKHOUSE_USERNAME can be left unset. The username default will be used in that case.Added support for configuring the port used to connect to ClickHouse via the CLICKHOUSE_PORT environment variable.Changed Disabled ClickHouse import timing logging by default. It can be enabled via environment variable - DEBUG when running the service standalone or CLICKHOUSE_DEBUG when using Docker Compose Upgraded to ClickHouse 25.4.Fixed Ensure .env is read in clickhouse-import script.[Release 32] - 2025-04-22 Changed Reenabled parallel ArNS resolution with removal of misplaced global limit. Refer to release 30 notes for more details on configuration and rationale.Added a timeout for the last ArNS resolver in ARNS_RESOLVER_PRIORITY_ORDER. It defaults to 30 seconds and is configurable using ARNS_COMPOSITE_LAST_RESOLVER_TIMEOUT_MS. This helps prevent promise build up if the last resolver stalls.Fixed Fixed apex ArNS name handling when a subdomain is present in ARNS_ROOT_HOST.Fixed a case where fork recovery could stall due to early flushing of unstable chain data.Restored observer logs by removing unintentional default log level override in docker-compose.yaml.[Release 31] - 2025-04-11 Changed Improved peer TX header fetching by fetching from a wider range of peers and up/down weighting peers based on success/failure.Fixed Rolled back parallel ArNS resolution changes that were causing ArNS resolution to slow down over time.[Release 30] - 2025-04-04 Added Added support for filtering Winston logs with a new LOG_FILTER environment variable.Example filter: {"attributes":{"class":"ArweaveCompositeClient"}} to only show logs from that class.Use CORE_LOG_FILTER environment variable when running with docker-compose.Added parallel ArNS resolution capability.Configured via ARNS_MAX_CONCURRENT_RESOLUTIONS (default: 1).This foundation enables future enhancements to ArNS resolution and should generally not be adjusted at present.Changed Improved ClickHouse auto-import script with better error handling and continuous operation through errors.Reduced maximum header request rate per second to trusted node to load on community gateways.Optimized single owner and recipient queries on ClickHouse with specialized sorted tables.Used ID sorted ClickHouse table for ID queries to improve performance.Fixed Fixed data alignment in Parquet file name height boundaries to ensure consistent import boundaries.Removed trailing slashes from AO URLs to prevent issues when passing them to the SDK.Only prune SQLite data when ClickHouse import succeeds to prevent data loss during exports.[Release 29] - 2025-03-21 Changed Temporarily default to trusted gateway ArNS resolution to reduce CU load as much possible. On-demand CU resolution is still available as a fallback and the order can be modified by setting ARNS_RESOLVER_PRIORITY_ORDER.Remove duplicate network process call in on-demand resolver.Don't wait for network process debounces in the on-demand resolver.Slow network process dry runs no longer block fallback to next resolver.Added Added support for separate CUs URLs for the network and ANT processes via the NETWORK_AO_CU_URL and ANT_AO_CU_URL process URLs respectively. If either is missing the AO_CU_URL is used instead with a fallback to the SDK default URL if AO_CU_URL is also unspecified.Added CU URLs to on-demand ArNS resolver logs.Added circuit breakers for AR.IO network process CU dry runs. By default they use a 1 minute timeout and open after 30% failure over a 10 minute window and reset after 20 minutes.Fixed Owners in GraphQL results are now correctly retrieved from data based on offsets when using ClickHouse.[Release 28] - 2025-03-17 Changed Raised name not found name list refresh interval to 2 minutes to reduce load on CUs. This increases the maximum amount of time a user may wait for a new name to be available. Future releases will introduce other changes to mitigate this delay.Adjusted composite ArNS resolver to never timeout resolutions from the last ArNS resolver in the resolution list.Added Added support for serving a given ID or ArNS name from the apex domain of a gateway. If using an ID, set the APEX_TX_ID environment variable. If using an ArNS name, set the APEX_ARNS_NAME environment variable.Added BUNDLE_REPAIR_UPDATE_TIMESTAMPS_INTERVAL_SECONDS, BUNDLE_REPAIR_BACKFILL_INTERVAL_SECONDS, and BUNDLE_REPAIR_FILTER_REPROCESS_INTERVAL_SECONDS environment variables to control the interval for retrying failed bundles, backfilling bundle records, and reprocessing bundles after a filter change. Note: the latter two are rarely used. Queuing bundles for reprocessing via the /ar-io/admin/queue-bundle endpoint is usually preferable to automatic reprocessing as it is faster and offers more control over the reprocessing behavior.Fixed Signatures in GraphQL results are now correctly retrieved from data based on offsets when using ClickHouse.Adjusted exported Parquet file names to align with expectations of ClickHouse import script.Ensured that bundle indexing status is properly reset when bundles are manually queued after an unbundling filter change has been made.[Release 27] - 2025-02-20 Changed Set process IDs for mainnet.Increase default AO CU WASM memory limit to 17179869184 to support mainnet
process.[Release 26] - 2025-02-13 Added Added a per resolver timeout in the composite ArNS resolver. When the
composite resolver attempts resolution it is applied to each resolution
attempt. It is configurable via the ARNS_COMPOSITE_RESOLVER_TIMEOUT_MS and
defaults to 3 seconds in order to allow a fallback attempt before the default
observer timeout of 5 seconds.Added a TURBO_UPLOAD_SERVICE_URL environment variable to support
configuration of the bundler used by the observer (TurboSDK defaults are
used if not set).Added a REPORT_DATA_SINK environment variable that enables switching the
method used to post observer reports. With the default, turbo, it sends
data items via a Turbo compatible bundler. Switching it to arweave will
post base layer transactions directly to Arweave instead.Added a /ar-io/admin/bundle-status/<id> endpoint that returns the counters
and timestamps from the bundles row in data.db. This can be used for
monitoring unbundling progress and scripting (e.g., to skip requeuing already
queued bundles).Added more complete documentation for filters.Changed Use arweave.net as the default GraphQL URL for AO CUs since most gateways
will not have a complete local AO data item index.Use a default timeout of 5 seconds when refreshing Arweave peers to prevent
stalled peer refreshes.Cache selected gateway peer weights for the amount of time specified by the GATEWAY_PEERS_WEIGHTS_CACHE_DURATION_MS environment variable with a default
of 5 seconds to avoid expensive peer weight recomputation on each request.Chunk broadcasts to primary nodes occur in parallel with a concurrency limit
defaulting to 2 and configurable via the CHUNK_POST_CONCURRENCY_LIMIT environment variable.Added circuit breakers for primary chunk node POSTs to avoid overwhelming
chunk nodes when they are slow to respond.Fixed Properly cleanup timeout and event listener when terminating the data
root computation worker.Count chunk broadcast exceptions as errors in the arweave_chunk_broadcast_total metric.[Release 25] - 2025-02-07 Added Added support for indexing and querying ECDSA signed Arweave transactions.Expanded the OpenAPI specification to cover the entire gateway API and
commonly used Arweave node routes.ArNS undername record count limits are now enforced. Undernames are sorted
based on their ANT configured priority with a fallback to name comparisons
when priorities conflict or are left unspecified. Enforcement is enabled by
default but can be disabled by setting the ARNS_RESOLVER_ENFORCE_UNDERNAME_LIMIT to false.Changed Renamed the ario-peer data source to ar-io-peers for consistency and
clarity. ario-peer will continue to work for backwards compatibility but is
considered deprecated.Use AR.IO gateway peers from the ar.io gateway address registry (GAR) as the
last fallback for fetching data when responding to client data requests. This
has the benefit of making the network more resilient to trusted gateway
disruptions, but it can also result in nodes serving data from less trusted
sources if it is not found in the trusted gateway. This can be disabled by
using a custom ON_DEMAND_RETRIEVAL_ORDER that does not include ar-io-peers.Arweave data chunk requests are sent to the trusted node first with a
fallback to Arweave peers when chunks are unavailable on the trusted node.
This provides good performance by default with a fallback in case there are
issues retrieving chunks from the trusted node.Increased the observer socket timeout to 5 seconds to accommodate initial
slow responses for uncached ArNS resolutions.Disabled writing base layer Arweave signatures to the SQLite DB by default to
save disk space. When signatures are required to satisfy GraphQL requests,
they are retrieved from headers on the trusted node.Fixed Updated dependencies to address security issues.Improved reliability of failed bundle indexing retries.Fixed failure to compute data roots for verification for base layer data
larger than 2GiB.Fixed observer healthcheck by correcting node.js path in healthcheck script.[Release 24] - 2025-02-03 Added Added a ARNS_ANT_STATE_CACHE_HIT_REFRESH_WINDOW_SECONDS environment
variable that determines the number of seconds before the end of the TTL at
which to start attempting to refresh the ANT state.Added a TRUSTED_GATEWAYS_REQUEST_TIMEOUT_MS environment that defaults to
10,000 and sets the number of milliseconds to wait before timing out request
to trusted gateways.Added BUNDLE_REPAIR_RETRY_INTERVAL_SECONDS and BUNDLE_REPAIR_RETRY_BATCH_SIZE environment variables to control the time
between queuing batches of bundle retries and the number of data items
retrieved when constructing batches of bundles to retry.Added support for configuring the ar.io SDK log level via the AR_IO_SDK_LOG_LEVEL environment variable.Added a request_chunk_total Prometheus counter with status, source (a
URL) and source_type (trusted or peer) labels to track success/failure
of chunk retrieval in the Arweave network per source.Added a get_chunk_total Prometheus metric to count chunk retrieval
success/failure per chunk.Added arns_cache_hit_total and arns_cache_miss_total Prometheus counters
to track ArNS cache hits and misses for individual names respectively.Added arns_name_cache_hit_total and arns_name_cache_miss_total Prometheus
counters to track ArNS name list cache hits and misses
respectively.Added a arns_resolution_duration_ms Prometheus metric that tracks summary
statistics for the amount of time it takes to resolve ArNS names.Changed In addition to the trusted node, the Arweave network is now searched for
chunks by default. All chunks retrieved are verified against data roots
indexed from a trusted Arweave node to ensure their validity.Default to a 24 hour cache TTL for the ArNS name cache. Record TTLs still
override this, but in cases where resolution via AO CU is slow or fails, the
cache will be used. In the case of slow resolution, CU based resolution will
proceed in the background and update the cache upon completion.Switched to the ioredis library for better TLS support.Updated minor dependency minor versions (more dependencies will be updated in
the next release).Bundles imports will no longer be re-attempted for bundles that have already
been fully unbundled using the current filters if they are matched or
manually queued again.Replaced references docker-compose in the docs with the more modern docker compose.Fixed Ensure duplicate data item IDs are ignored when comparing counts to determine
if a bundle has been fully unbundled.Fixed worker threads failing to shut down properly when the main process
stopped.Ensure bundle import attempt counts are incremented when bundles are skipped
to avoid repeatedly attempting to import skipped bundles.Use observe that correctly ensure failing gateways are penalized in the AR.IO
AO process.[Release 23] - 2025-01-13 Added Added FS_CLEANUP_WORKER_BATCH_SIZE,FS_CLEANUP_WORKER_BATCH_PAUSE_DURATION, and FS_CLEANUP_WORKER_RESTART_PAUSE_DURATION environment variables to allow
configuration of number of contiguous data files cleaned up per batch, the
pause between each batch, and the pause before restarting the entire cleanup
process again.Added data_items_unbundled_total Prometheus metric that counts the total
number of data items unbundled, including those that did not match the
unbundling filter.Added a parent_type label that can be one of transaction or data_item to data item indexing metrics.Added a files_cleaned_total total Prometheus metric to enable monitoring of
contiguous data cleanup.Added support for specifying the admin API via a file specified by the ADMIN_API_KEY_FILE environment variable.Added experimental support for posting chunks in a non-blocking way to
secondary nodes specified via a comma separate list in the SECONDARY_CHUNK_POST_URLS environment variable.Changed Renamed the parent_type lable to contiguous_data_type on bundle metrics
to more accurately reflect the meaning of the label.Reduced the maximum time to refresh the ArNS name list to 10 seconds to
minimize delays in ArNS availability after a new name is registered.Changed /ar-io/admin/queue-bundle to wait for bundles rows to be written
to the DB before responding to ensure that errors that occur due to DB
contention are not silently ignored.Data items are now flushed even when block indexing is stopped. This allows
for indexing batches of data items using the admin API with block indexing
disabled.Adjust services in docker-compose to use unless-stopped as their restart
policy. This guards against missing restarts in the case where service
containers exit with a success status even when they shouldn't.Fixed Added missing created_at field in blocked_names table.Fixed broken ArNS undername resolution.[Release 22] - 2024-12-18 Added Added the ability to block and unblock ArNS names (e.g., to comply with hosting provider TOS). To block a name, POST { "name": "<name to block>" } to /ar-io/admin/block-name. To unblock a name, POST { "name": "<name to unblock>" } to /ar-io/admin/unblock-name.Changed Return an HTTP 429 response to POSTs to /ar-io/admin/queue-bundle when the bundle data import queue is full so that scripts queuing bundles can wait rather than overflowing it.Fixed Adjust ArNS length limit from <= 48 to <= 51 to match the limit enforced by the AO process.[Release 21] - 2024-12-05 Added Added a ClickHouse auto-import service. When enabled, it calls the Parquet export API, imports the exported Parquet into ClickHouse, moves the Parquet files to an imported subdirectory, and deletes data items in SQLite up to where the Parquet export ended. To use it, run Docker Compose with the clickhouse profile, set the CLICKHOUSE_URL to http://clickhouse:8123, and ensure you have set an ADMIN_KEY. Using this configuration, the core service will also combine results from ClickHouse and SQLite when querying transaction data via GraphQL. Note: if you have a large number of data items in SQLite, the first export and subsequent delete may take an extended period. Also, this functionality is considered experimental. We expect there are still bugs to be found in it and we may make breaking changes to the ClickHouse schema in the future. If you choose to use it in production (not yet recommended), we suggest backing up copies of the Parquet files found in data/parquet/imported so that they can be reimported if anything goes wrong or future changes require it.Added a background data verification process that will attempt to recompute data roots for bundles and compare them to data roots indexed from Arweave nodes. When the data roots match, all descendant data items will be marked as verified. This enables verification of data initially retrieived from sources, like other gateways, that serve contiguous data instead of verifiable chunks. Data verification can be enabled by setting the ENABLE_BACKGROUND_DATA_VERIFICATION environment variable to true. The interval between attempts to verify batches of bundles is configurable using the BACKGROUND_DATA_VERIFICATION_INTERVAL_SECONDS environment variable.Added a CHUNK_POST_MIN_SUCCESS_COUNT environment variable to configure how many Arweave nodes must accept a chunk before a chunk broadcast is considered successful.Added arweave_chunk_post_total and arweave_chunk_broadcast_total Prometheus metrics to respectively track the number of successful chunk POSTs to Arweave nodes and the number of chunks successfully broadcast.When resolving ArNS names, the entire list of names is now cached instead of individually checking whether each name exists. This reduces the load on AO CUs since the entire list can be reused across multiple requests for different names. Note: due to the default 5 minute interval between name list refreshes, newly registered may now take longer to resolver after initial registration. We intend to make further caching refinements to address this in the future.Added support for multiple prioritized trusted gateways configurable by setting the TRUSTED_GATEWAYS_URLS environment variable to a JSON value containing a mapping of gateway hosts to priorities. Data requests are sent to other gateways in ascending priority order. If multiple gateways share the same priority, all the gateways with the same priority are tried in a random order before continuing on to the next priority.Added support for caching contiguous data in S3. It is enabled by default when the AWS_S3_CONTIGUOUS_DATA_BUCKET and AWS_S3_CONTIGUOUS_DATA_PREFIX environment variables are set.Changed trusted-gateway was changed to trusted-gateways in ON_DEMAND_RETRIEVAL_ORDER and BACKGROUND_RETRIEVAL_ORDER.Renamed the S3 contiguous environment variables - AWS_S3_BUCKET to AWS_S3_CONTIGUOUS_DATA_BUCKET and AWS_S3_PREFIX to AWS_S3_CONTIGUOUS_DATA_PREFIX.[Release 20] - 2024-11-15 Added Exposed the core service chunk POST endpoint via Envoy. It accepts a Arweave data chunk and broadcasts it to either the comma separated list of URLs specified by the CHUNK_POST_URLs environment variable or, if none are specified, the /chunk path on URL specified by the TRUST_GATEWAY_URL environment variable.Added a X-AR-IO-Root-Transaction-Id HTTP header to data responses containing the root base layer transaction ID for the ID in question if it's been indexed.Added a X-AR-IO-Data-Item-Data-Offset HTTP header containing the offset of the data item relative to the root bundle base layer transaction for it. In conjunction with X-AR-IO-Root-Transaction-Id, it enables retrieving data for data item IDs from base layer data using first a HEAD request to retrieve the root ID and data offset followed by a range request into the root bundle. This greatly increases the likelihood of retriving data item data by ID since only an index into the base layer and Arweave chunk availability is needed for this access method to succeed.Added an experimental ClickHouse service to docker-compose.yaml (available via the clickhouse profile). This will be used as a supplemental GraphQL DB in upcoming releases.Added a data item indexing healthcheck that can be enabled by setting the RUN_AUTOHEAL environment variable to true. When enabled, it will restart the core service if no data items have been indexed since the value specified by the MAX_EXPECTED_DATA_ITEM_INDEXING_INTERVAL_SECONDS environment variable.[Release 19] - 2024-10-21 Fixed Adjusted data item flushing to use the bundle DB worker instead of the core DB worker to prevent write contention and failed flushes under heavy unbundling load.Added Added X-AR-IO-Digest, X-AR-IO-Stable, X-AR-IO-Verified, and ETag headers. X-AR-IO-Digest contains a base64 URL encoded representation of the SHA-256 hash of the data item data. It may be empty if the gateway has not previously cached the data locally. X-AR-IO-Stable contains either true or false depending on whether the associated Arweave transaction is more than 18 blocks old or not. X-AR-IO-Verified contains either true if the gateway has verified the data root of the L1 transaction or the L1 root parent of the data item or false if it has not. ETag contains the same value a X-AR-IO-Digest and is used to improve HTTP caching efficiency.Added support for using a different data source for on-demand and background data retrieval. Background data retrieval is used when unbundling. The background retrieval data source order is configurable using the BACKGROUND_RETRIEVAL_ORDER environment variable and defaults to chunks,s3,trusted-gateway,tx-data. Priority is given to chunk retrieval since chunks are verifiable.Added an /ar-io/admin/export-parquet/status to support monitoring of in-progress Parquet export status.Added sqlite_in_flight_ops Prometheus metric with worker (core, bundles, data, or moderation) and role (read or write) labels to support monitoring the number of in-flight DB operations.Added experimental Grafana and Prometheus based observability stack. See the "Monitoring and Observability" section of the README for more details.Changed Bundle data is now retrieved as chunks from Arweave nodes by default so that data roots can be compared against the chain (see entry about background retrieval above).Changed observer configuration to use 8 instead of 5 chosen names. These are combined with 2 names prescribed from the contract for a total of 10 names observed each epoch to provide increased ArNS observation coverage.Verification status is set on data items when unbundling a parent that has already been verified.[Release 18] - 2024-10-01 Fixed Improved performance of data attributes query that was preventing data.db WAL flushing.Added Added WAL sqlite_wal_checkpoint_pages Prometheus metric to help monitor WAL flushing.Added a POST /ar-io/admin/export-parquet endpoint that can be used to export the contents of the SQLite3 core and bundle DBs as Parquet. To trigger an export, POST JSON containing outputDir, startHeight, endHeight, and maxFileRows keys. The resulting Parquet files can then be queried directly using DuckDB or loaded into another system (e.g. ClickHouse). Scripts will be provided to help automate the latter in a future release.Added ARNS_RESOLVER_OVERRIDE_TTL_SECONDS that can be used to force ArNS names to refresh before their TTLs expire.Added a GET /ar-io/resolver/:name endpoint that returns an ArNS resolution for the given name.Changed Removed ArNS resolver service in favor of integrated resolver. If a standalone resolver is still desired, the core service can be run with the START_WRITERS environment variable set to false. This will disable indexing while preserving resolver functionality.Deduplicated writes to data.db to improve performance and reduce WAL growth rate.[Release 17] - 2024-09-09 Notes This release includes a LONG RUNNING MIGRATION. Your node may appear unresponsive while it is running. It is best to wait for it to complete. If it fails or is interrupted, removing your SQLite DBs (in data/sqlite by default) should resolve the issue, provided you are willing to lose your GraphQL index and let your node rebuild it.Fixed Use the correct environment variable to populate WEBHOOK_BLOCK_FILTER in docker-compose.yaml.Don't cache data regions retrieved to satisfy range requests to avoid unnecessary storage overhead and prevent inserting invalid ID to hash mappings into the data DB.Added Added a new ClickHouse based DB backend. It can be used in combination with the SQLite DB backend to enable batch loading of historical data from Parquet. It also opens up the possibility of higher DB performance and scalability. In its current state it should be considered a technology preview. It won't be useful to most users until we either provide Parquet files to load into it or automate flushing of the SQLite DB to it (both are planned in future release). It is not intended to be standalone solution. It supports bulk loading and efficient GraphQL querying of transactions and data items, but it relies on SQLite (or potentially another OLTP in the future) to index recent data. These limitations allow greatly simplified schema and query construction. Querying the new ClickHouse DB for transaction and data items via GraphQL is enabled by setting the CLICKHOUSE_URL environment variable.Added the ability to skip storing transaction signatures in the DB by setting WRITE_TRANSACTION_DB_SIGNATURES to false. Missing signatures are fetched from the trusted Arweave node when needed for GraphQL results.Added a Redis backed signature cache to support retrieving optimistically indexed data item signatures in GraphQL queries when writing data items signatures to the DB has been disabled.Added on-demand and composite ArNS resolvers. The on-demand resolver fetches results directly from an AO CU. The composite resolver attempts resolution in the order specified by the ARNS_RESOLVER_PRIORITY_ORDER environment variable (defaults to on-demand,gateway).Added a queue_length Prometheus metric to fasciliate monitoring queues and inform future optimizations Added SQLite WAL cleanup worker to help manage the size of the data.db-wal file. Future improvements to data.db usage are also planned to further improve WAL management.Changed Handle data requests by ID on ArNS sites. This enables ArNS sites to use relative links to data by ID.Replaced ARNS_RESOLVER_TYPE with ARNS_RESOLVER_PRIORITY_ORDER (defaults to on-demand,gateway).Introduced unbundling back pressure. When either data item data or GraphQL indexing queue depths are more than the value specified by the MAX_DATA_ITEM_QUEUE_SIZE environment variable (defaults to 100000), unbundling is paused until the queues length falls bellow that threshold. This prevents the gateway from running out of memory when the unbundling rate exceeds the indexing rate while avoiding wasteful bundle reprocessing.Prioritized optimistic data item indexing by inserting optimistic data items at the front of the indexing queues.Prioritized nested bundle indexing by inserting nested bundles at the front of the unbundling queue.[Release 16] - 2024-08-09 Fixed Fixed promise leak caused by missing await when saving data items to the DB.Modified ArNS middleware to not attempt resolution when receiving requests for a different hostname than the one specified by ARNS_ROOT_HOST.Added Added support for returning Content-Encoding HTTP headers based on user specified Content-Encoding tags.Added isNestedBundle filter enables that matches any nested bundle when indexing. This enables composite unbundling filters that match a set of L1 tags and bundles nested under them.Added ability to skip writing ANS-104 signatures to the DB and load them based on offsets from the data instead. This significantly reduces the size of the bundles DB. It can be enabled by setting the WRITE_ANS104_DATA_ITEM_DB_SIGNATURES environment variable to false.Added data_item_data_indexed_total Prometheus counter to count data items with data attributes indexed.Changed Queue data attributes writes when serving data rather than writing them syncronously.Reduced the default data indexer count to 1 to lessen the load on the data DB.Switched a number of overly verbose info logs to debug level.Removed docker-compose on-failure restart limits to ensure that services restart no matter how many times they fail.Modified the data_items_indexed_total Prometheus counter to count data items indexed for GraphQL querying instead of data attributes.Increased aggressiveness of contiguous data cleanup. It now pauses 5 seconds instead of 10 seconds per batch and runs every 4 hours instead of every 24 hours.[Release 15] - 2024-07-19 Fixed Fixed query error that was preventing bundles from being marked as fully imported in the database.Added Adjusted data item indexing to record data item signature types in the DB. This helps distinguish between signatures using different key formats, and will enable querying by signature type in the future.Adjusted data item indexing to record offsets for data items within bundles and signatures and owners within data items. In the future this will allow us to avoid saving owners and signatures in the DB and thus considerably reduce the size of the bundles DB.Added ARNS_CACHE_TTL_MS environment variable to control the TTL of ARNS cache entries (defaults to 1 hour).Added support for multiple ranges in a single HTTP range request.Added experimental chunk POST endpoint that broadcasts chunks to the comma-separate list of URLS in the CHUNK_BROADCAST_URLS environment variable. It is available at /chunk on the internal gateway service port (4000 by default) but is not yet exposed through Envoy.Added support for running an AO CU adjacent to the gateway (see README.md for details).Added X-ArNS-Process-Id to ArNS resolved name headers.Added a set of AO_... environment variables for specifying which AO URLs should be used (see docker-compose.yaml for the complete list). The AO_CU_URL is of particular use since the core and resolver services only perform AO reads and only the CU is needed for reads.Changed Split the monolithic docker-compose.yaml into docker-compose.yaml, docker-compose.bundler.yaml, and docker-compose.ao.yaml (see README for details).Replaced references to 'docker-compose' with 'docker compose' in the docs since the former is mostly deprecated.Reduce max fork depth from 50 to 18 inline to reflect Arweave 2.7.2 protocol changes.Increased the aggressiveness of bundle reprocessing by reducing reprocessing interval from 10 minutes to 5 minutes and raising reprocessing batch size from 100 to 1000.Use a patched version of Litestream to work around insufficient S3 multipart upload size in the upstream version.[Release 14] - 2024-06-26 Fixed Correctly handle manifest index after paths.[Release 13] - 2024-06-24 Added Added support for optimistically reading data items uploaded using the integrated Turbo bundler via the LocalStack S3 interface.Added X-AR-IO-Origin-Node-Release header to outbound data requests.Added hops, origin, and originNodeRelease query params to outbound data requests.Added support for fallback in v0.2 manifests that is used if no path in the manifest is matched.Changed Updated Observer to read prescribed names from and write observations to the ar.io AO network process.Updated Resolver to read from the ar.io AO network process.Fixed Modified optimistic indexing of data items to use a null parent_id when inserting into the DB instead of a placeholder value. This prevents unexpected non-null bundledIn values in GraphQL results for optimistically indexed data items.Modified GraphQl query logic to require an ID for single block GraphQL queries. Previously queries missing an ID were returning an internal SQLite error. This represents a small departure from arweave.net's query logic which returns the latest block for these queries. We recommend querying blocks instead of block in cases where the latest block is desired.Adjusted Observer health check to reflect port change to 5050.Security Modified docker-compose.yaml to only expose Redis, PostgreSQL, and LocalStack ports internally. This protects gateways that neglect to deploy behind a firewall, reverse proxy, or load balancer.[Release 12] - 2024-06-05 Added Added /ar-io/admin/queue-data-item endpoint for queuing data item headers for indexing before the bundles containing them are processed. This allows trusted bundlers to make their data items quickly available to be queried via GraphQL without having to wait for bundle data submission or unbundling.Added experimental support for retrieving contiguous data from S3. See AWS_* environment variables documentation for configuration details. In conjuction with a local Turbo bundler this allows optimistic bundle (but not yet data item) retrieval.Add experimental support for fetching data from gateway peers. It can be enabled by adding ario-peer to ON_DEMAND_RETRIEVAL_ORDER. Note: do not expect this work reliably yet! This functionality is in active development and will be improved in future releases.Add import_attempt_count to bundle records to enable future bundle import retry optimizations.Changed Removed version from docker-compose.yaml to avoid warnings with recent versions of docker-compose.Switched default observer port from 5000 to 5050 to avoid conflict on OS X. Since Envoy is used to provide external access to the observer API this should have no user visible effect.[Release 11] - 2024-05-21 Added Added arweave_tx_fetch_total Prometheus metric to track counts of transaction headers fetched from the trusted node and Arweave network peers.Changed Revert to using unnamed bind mounts due to cross platform issues with named volumes.[Release 10] - 2024-05-20 Added Added experimental support for streaming SQLite backups to S3 (and compatible services) using Litestream. Start the service using the docker-compose "litestream" profile to use it, and see the AR_IO_SQLITE_BACKUP_* environment variables documentation for further details.Added /ar-io/admin/queue-bundle endpoint for queueing bundles for import for import before they're in the mempool. In the future this will enable optimistic indexing when combined with a local trusted bundler.Added support for triggering webhooks when blocks are imported matching the filter specified by the WEBHOOK_BLOCK_FILTER environment variable.Added experimental support for indexing transactions and related data items from the mempool. Enable it by setting ENABLE_MEMPOOL_WATCHER to 'true'.Made on-demand data caching circuit breakers configurable via the GET_DATA_CIRCUIT_BREAKER_TIMEOUT_MS environment variable. This allows gateway operators to decide how much latency they will tolerate when serving data in exchange for more complete data indexing and caching.Rename cache header from X-Cached to X-Cache to mimic typical CDN practices.Add X-AR-IO-Hops and X-AR-IO-Origin headers in preparation for future peer-to-peer functionality.Upgrade to Node.js v20 and switch to native test runner.[Release 9] - 2024-04-10 Added Added experimental Farcaster Frames support, enabling simple Arweave based Frames with button navigation. Transaction and data item data is now served under /local/farcaster/frame/<ID>. /local is used as a prefix to indicate this functionality is both experimental and local to a particular gateway rather than part of the global gateway API. Both GET and POST requests are supported.Added an experimental local ArNS resolver. When enabled it removes dependence on arweave.net for ArNS resolution! Enable it by setting RUN_RESOLVER=TRUE, TRUSTED_ARNS_RESOLVER_TYPE=resolver, and TRUSTED_ARNS_RESOLVER_URL=http://resolver:6000 in your .env file.Added an X-Cached header to data responses to indicate when data is served from the local cache rather than being retrieved from an external source. This is helpful for interfacing with external systems, debugging, and end-to-end testing.Save hashes for unbundled data items during indexing. This enables reduction in data storage via hash based deduplication as well as more efficient peer-to-peer data retrieval in the future.[Release 8] - 2024-03-14 Added Added GraphQL SQL query debug logging to support trouble-shooting and performance optimization.Added support for indexing data items (not GraphQL querying) based solely on tag name. (example use case: indexing all IPFS CID tagged data items).Changes Observer data sampling now uses randomized ranges to generate content hashes.Reference gateway ArNS resolutions are now cached to improve report generation performance.Contract interactions are now tested before posting using dryWrite to avoid submitting interactions that would fail./ar-io/observer/info now reports INVALID for wallets that fail to load.Fixed Fix data caching failure caused by incorrect method name in getData circuit breakers.Fix healthcheck when ARNS_ROOT_HOST includes a subdomain.[Release 7] - 2024 - 02 - 14 Added Add support for notifying other services of transactions and data items using webhooks (see README for details).Add support for filter negation (particularly useful for excluding large bundles from indexint).Improve unbundling throughput by decoupling data fetching from unbundling.Add Envoy and core service ARM builds.Changed Improve resouce cleanup and shutdown behavior.Don't save Redis data to disk by default to help prevent memory issues on startup for small gateways.Reduce the amount of data sampled from large files by the observer.Ensure block poa2 field is not chached to reduce memory consumption.[Release 6] - 2024-01-29 Fixed Update observer to improve reliability of contract state synchronization and evaluation.[Release 5] - 2024-01-25 Added Added transaction offset indexing to support future data retrieval capabilities.Enabled IPv6 support in Envoy config.Added ability to configure observer report generation interval via the REPORT_GENERATION_INTERVAL_MS environmental variable. (Intended primarily for development and testing) Changed Updated observer to properly handle FQDN conflicts.Renamed most created_at columns to index to indexed_at for consistency and clarity.Fixed Updated LMDB version to remove Buffer workaround and fix occasional block cache errors.[Release 4] - 2024-01-11 Added Added circuit breakers around data index access to reduce impact of DB access contention under heavy requests loads.Added support for configuring data source priority via the ON_DEMAND_RETRIEVAL_ORDER environment variable.Updated observer to a version that retrieves epoch start and duration from contract state.Changed Set the Redis max memory eviction policy to allkeys-lru.Reduced default Redis max memory from 2GB to 256MB.Improved predictability and performance of GraphQL queries.Eliminated unbundling worker threads when filters are configured to skip indexing ANS-104 bundles.Reduced the default number of ANS-104 worker threads from 2 to 1 when unbundling is enabled to conserve memory.Increased nodejs max old space size to 8GB when ANS-104 workers > 1.Fixed Adjusted paths for chunks indexed by data root to include the full data root.[Release 3] - 2023-12-05 Added Support range requests (PR 61, PR 64) Note: serving multiple ranges in a single request is not yet supported.Release number in /ar-io/info response.Redis header cache implementation (PR 62).New default header cache (replaces old FS cache).LMDB header cache implementation (PR 60).Intended for use in development only.Enable by setting CHAIN_CACHE_TYPE=lmdb.Filesystem header cache cleanup worker (PR 68).Enabled by default to cleanup old filesystem cache now that Redis is the new default.Support for parallel ANS-104 unbundling (PR 65).Changed Used pinned container images tags for releases.Default to Redis header cache when running via docker-compose.Default to LMDB header cache when running via yarn start.Fixed Correct GraphQL pagination for transactions with duplicate tags.

---

# 116. ARIO Docs

Document Number: 116
Source: https://docs.ar.io/gateways/upgrading
Words: 351
Extraction Method: html

Upgrading your Gateway To ensure the optimal performance and security of your AR.IO Gateway, it's essential to regularly upgrade to the latest version. Notably, indexed data resides separate from Docker. As a result, neither upgrading the Gateway nor pruning Docker will erase your data or progress. Here's how you can perform the upgrade:Prerequisites Your Gateway should have been cloned using git. If you haven't, follow the installation instructions for windows or linux.Checking your Release Number Effective with release 3, you can view the currently implemented release on any gateway by visiting https://<gateway>/ar-io/info in a browser. Be sure to replace <gateway> with the domain of the gateway you are checking.If the release number displayed includes -pre it means that your gateway is using the develop branch of the github repo for the gateway code. Follow steps in our troubleshooting guide to switch over to the more stable main branch.Announcements will be made in our discord server showing each new release.Upgrade Steps Pull the latest changes from the repository Navigate to your cloned repository directory and execute the following command:Shut down Docker Depending on your operating system, use the respective commands:Linux Windows Prune Docker (Optional) It's a good practice to clean up unused Docker resources. Again, use the command based on your OS:NOTE: This will erase all inactive docker containers on your machine. If you use docker for anything beyond running a gateway be extremely careful using this command.Linux Windows Check for New Environmental Variables Read the update release change logs and community announcements to see if the new version includes any new environmental variables that you should set before restarting your gateway.Restart the Docker container Finally, start the Docker container again to implement the changes:Linux Windows NOTE: Effective with Release #3, it is no longer required to include the --build flag when starting your gateway. Docker will automatically build using the image specified in the docker-commpose.yaml file.That's it! Your AR.IO Gateway is now upgraded to the latest version. Ensure to test and verify that everything is functioning as expected. If you encounter any issues, reach out to the AR.IO community for assistance.

---

# 117. Gateway Troubleshooting  FAQ - ARIO Docs

Document Number: 117
Source: https://docs.ar.io/gateways/troubleshooting
Words: 3811
Extraction Method: html

Welcome to the unified troubleshooting and FAQ resource for AR.IO Gateway operators. Use the quick lookup table below for fast answers, or browse the detailed sections for in-depth guidance.Quick Lookup Below is a quick summary of what you should check when troubleshooting your gateway. Find more detailed information in the sections below.← Swipe to see more → Issue What to Check My release number is wrong Pull the latest github updates and make sure you are on the main branch Gateway appears offline on Viewblock or ar://gateways Probably fine, but verify that your gateway is still running.'/ar-io/observer/reports/current' just says "report pending" Normal behavior, wait for the report to complete.Observer error "Cannot read properties of undefined" Normal behavior, Observer is checking for data not implemented yet.Observing my gateway shows failures Check AR_IO_WALLET and ARNS_ROOT_HOST settings.Updated.env settings not reflected on gateway Rebuild your gateway after editing.env file.Out of disk space error Check for inode exhaustion and delete files if necessary.Can't load ArNS names Check ARNS_ROOT_HOST setting in.env file, and DNS records."Your connection is not private" error Generate or renew SSL certificates.404/Nginx error when accessing domain Check Nginx settings and restart Nginx if necessary.502 error from Nginx Check for errors in your gateway.Trouble generating SSL certificates Ensure TXT records have propagated and follow certbot instructions.← Swipe to see more →  General Troubleshooting My Gateway Seems to be Running but...My release number doesn't match the latest version, or includes "-pre" If your release number when you go to <your-gateway>/ar-io/info is lower than the current release, you simply need to upgrade your gateway in order to reach the latest release.If your release number includes the suffix "-pre" it means you are running your gateway from the development branch of the github repository, instead of the main branch. The development branch is used for staging work that the engineering team is in the middle of. Because of this, it can be much less stable than the main branch used for production and can cause significant issues.Ensure that you are running the latest release, from the main branch, by running the below commands in your terminal:If this doesn't resolve the issue, you can also try a more extreme method of clearing out the incorrect docker images:It appears offline on Viewblock or ar://gateways Viewblock and ar://gateways use a very simple ping method for determining if a gateway is "up". There are plenty of reasons why this ping may fail while the gateway is running perfectly, so showing as down is not cause for concern. Just verify that your gateway is still running, and wait. Your gateway will show as up again soon.< gateway >/ar-io/observer/reports/current just says "report pending" This is normal. Your Observer is working to generate a report and that report will be displayed once it is complete.My Observer is showing me the error "error: Error reading interaction: Cannot read properties of undefined" This is not an issue with your observer. The short explanation is that your Observer is looking for tasks assigned to it by the AR.IO network contract, but there isnt anything there. You can safely ignore this error message.Observing my gateway shows failures When observing a gateway, there are two main pass/fail tests. "Ownership" and "ArNS Assessment" Ownership: This tests to see if the value set in your gateway AR_IO_WALLET value (in.env) matches the wallet used to join the AR.IO Network. If they don't match, update the value in your.env file and restart your gateway.ArNS Assessment: This tests to see if a gateway is able to resolve ArNS names correctly. The first thing you should check is if you have the ARNS_ROOT_HOST value set in your.env file. If not, set the value and restart your gateway. If this value is set, check to make sure you have current DNS records and SSL certificates for wildcard subdomains on your gateway.I updated my.env settings, but nothing changed on my gateway Once you edit your.env file, you need to "rebuild" your gateway for the changes to take effect. As of release 3, every time you start your gateway with docker-compose it is automatically rebuilt. So all you need to do is shut your gateway down and restart it.I am getting an out of disk space error, but I still have open storage space on my computer The most likely cause of this is inode exhaustion. Test this by running the command:If one of the lines in the output says 100%, you have run out of inodes and so your filesystem is not capable of creating new files, even if you have available space. The solution is to delete files from your data folder in order to free up inodes.This was a common issue prior to release #3, when Redis caching was introduced to reduce the number of small files created. If you are using an older version of the gateway, consider upgrading to mitigate the risk of inode exhaustion.I can't load ArNS names The first thing you should check if your gateway is not resolving ArNS names is that you have ARNS_ROOT_HOST set in your.env file. If not, set it to your domain name used for the gateway. For example, ARNS_ROOT_HOST=arweave.dev.Once this value is set, restart your gateway for the changes to take effect.If that doesn't resolve the issue, check your dns records. You need to have a wildcard subdomain ( *.< your-domain > ) set with your domain registrar so that ArNS names will actually point at your gateway. You can set this record, and generate an SSL certificate for it, in the same way you set the records for your primary domain.When I try to access my gateway in a browser I get a "Your connection is not private" error This error message means that your SSL certificates have expired. You need to renew your certificates by running the same certbot command you used when you initially started your gateway:Certbot SSL certificates expire after 90 days, and you will need to rerun this command to renew every time. If you provide an email address, you will receive an email letting you know when it is time to renew.I set my gateway up, but when I go to my domain I get a 404/Nginx error If you navigate to your domain and see a 404 error from Nginx (the reverse proxy server used in the setup guide) it means that your domain is correctly pointed at the machine running your gateway, but you have not properly configured your Nginx settings (or your gateway is not running).The Set up Networking section of the setup guide has detailed instructions on configuring your Nginx server. If all else fails, try restarting Nginx, that usually clears any issues with the server clinging to old configurations.When I visit my domain I see a 502 error from Nginx A 502 error from Nginx means that Nginx is working correctly, but it is receiving an error from your gateway when it tries to forward traffic.I am having trouble generating my SSL certificates When using the manual certbot command provided in the setup guide:You need to be sure that you are waiting after creating your TXT records for them to completely propagate. You can check propagation using a tool like dnschecker.org.If you continue to have issues, you can check the official certbot instructions guide.My gateway was working, but it just stopped Visit your gateway in a browser and see if your SSL certs are expired. This is the most common issue causing sudden stops in proper operation.I updated my SSL certs, but it still shows as bad in a browser Try restarting nginx, it sometimes has trouble looking at the new certs without a restart.My gateway won't resolve ArNS names Make sure ARNS_ROOT_HOST is properly set in your .env file. Updating this requires restarting your gateway.Make sure you have a DNS record set for *.<your-gateway-domain>. Since ArNS names are served as subdomains, you need to make sure all subdomains are pointed at your gateway.If your gateway is attempting to resolve the name, but times out, it's most likely a CU issue.I see an error in my logs, but everything appears to be working AR.IO gateways are very robust, they can handle temporary errors gracefully and not affect normal operation. You should only be concerned if the error is consistent or it is causing your gateway to not function properly.I was selected as an observer, but my logs say a report was not saved Observers generate and submit their reports at specific times throughout the epoch. This is to ensure a healthy network throughout the entire epoch, not just at the start.Your observer wallet must match the observer wallet associated with your gateway in the AR.IO contract. You can check this by navigating to your gateway in ar://gateways.I see an error in my logs that says <h"... is not valid JSON This happens when a request to a CU fails, and your gateway receives an html failure message instead of the expected JSON response. This will normally clear up on its own after congestion on that CU dies down, but if it is persistent try switching to a different CU.My gateway logs just changed, instead of importing blocks I see "polling for block" This is normal. It means you have reached the current Arweave block and need to wait for more before you can index them.Error resolving name with resolver Promise timed out This is normal. If a gateway fails to resolve an arns name within 3 seconds, it will fall back to a trusted gateway (arweave.net by default) to help resolve the name.My gateway failed an epoch There are many reasons a gateway could fail an epoch. Following these steps is usually enough to identify and correct the issue:Try to visit your gateway in a browser and see if your SSL certs are bad Try to resolve an ArNS name on your gateway. If it fails to resolve, check the console and your gateway logs for errors Look at the observation reports that failed your gateway, they will list the reason for failure  Troubleshooting Failed Epochs Overview The ARIO Network provides several tools to help troubleshoot problems with a gateway. The most powerful among these is the Observer.The Observer, which is a component of every gateway joined to the ARIO Network, checks all gateways in the network to ensure that they are functioning properly, and returning the correct data. The Observer then creates a report of the results of these checks, including the reasons why a gateway might have failed the checks.If a gateway fails the checks from more than half of the prescribed observers, the gateway is marked as failed for the epoch, and does not receive any rewards for that epoch.The first step in troubleshooting a failed gateway is always to attempt to resolve data on that gateway in a browser, but if that does not make the issue clear, the Observer report can be used to diagnose the problem.Manual Observation Manual observations may be run on a gateway at any time buy using the Network Portal   . This allows operators (or anyone with an interest in the gateway's performance) to check the gateway's performance at any time. To run a manual observation:Navigate to the Network Portal    Select the gateway you are interested in from the list of gateways Click on the "Observe" button in the top right corner of the page. Click on the "Run Observation" button in the bottom right corner of the page. Two randomly selected ArNS names will be entered automatically in the "ArNS names" field to the left of the "Run Observation" button. These can be changed, or additional ArNS names can be added to the list before running the observation.The Manual observation will run the same checks as the observer, and will display the results on the right side of the page. Accessing the Observer Report The simplest way to access an observer report is via the Network Portal   , following the steps below:Navigate to the Network Portal    Select the gateway you are interested in from the list of gateways In the Observation window, select the epoch you are interested in. This will display a list of the observers that failed the gateway for that epoch.Click on the "View Report" button to the right any observer on that list. This will display the entire report that observer generated. Locate the gateway you are interested in in the report, and click on that row. This will display the report for that gateway.Understanding the Observer Report The observer report will display a list of checked ArNS names, and a reason if the gateway failed to return the correct data for that name. There are several reasons why a gateway might fail to return the correct data for an ArNS name. Below is a list of the most common reasons, and how to resolve them.Timeout awaiting 'socket', or Timeout awaiting 'connect'   This failure means that the observer was unable to connect to the gateway when it tried to check the ArNS name. There are lots of reasons why this might happen, many of them unrelated to the gateway itself. If an observer report has a small number of these failures, among a larger number of successful checks, it is unlikely to be an issue with the gateway.If this failure occurs persistently for a large number, or all ArNS names checked, it likely means that the observer is having trouble connecting to the gateway at all. You can verify this by:Attempting to connect to the gateway in a browser Running manual observations on the gateway using the Network Portal    Using tools like curl or ping to check the gateway's connectivity If these methods consistently fail to connect to the gateway, it is likely that the gateway is not properly configured or powered on. If this is the case:Check Docker and the gateway's logs to see if the gateway is on.Ensure that the SSL certificates are valid for the gateway's domain.Check DNS records for the gateway's domain, misconfigured or conflicting DNS records can cause connectivity issues.Some gateway operators who run their gateways on their personal home networks have also reported issues with their ISP blocking, throttling, or otherwise delaying traffic to a gateway. If none of the above steps resolve the issue, it may be worth checking with your ISP to see if they are blocking or throttling traffic to the gateway.Using Grafana can also provide a visual representation of the gateway's ArNS resolution times. If this is consistently high (above 10 seconds), it is likely that the gateway is not properly configured to resolve ArNS names. Ensure that the gateway is operating on the latest Release.certificate has expired  This failure means that the gateway's SSL certificate has expired. Obtaining a new SSL certificate and updating the gateway's reverse proxy (nginx, etc) configuration to use the new certificate is the only solution to this issue.dataHashDigest mismatch  This failure means that the gateway did respond to a resolution request, but the data it returned did not match the data that was expected. This could be due to a number of reasons, including:Cached data was returned by the gateway that doesnt match the most current data on the network.The gateway is configured to operate on testnet or devnet. Gateways joined to the ARIO Network MUST operate on mainnet in order to pass observation checks.The gateway is intentionally returning fraudulent data.A gateway will not return fraudulent data unless that operator intentionally rewrote the gateway's code to do so, and a major purpose of the Observation and Incentive Protocol is to catch and prevent this behavior. A gateway may return mistaken data on occasion, usually due to a cache mismatch between the gateway and the observer's authority (usually arweavae.net). This is a relatively rare occurrence, and should only be considered an issue if it occurs persistently. If most or all of the ArNS names checked are failing for this reason, it is likely that the gateway is not operating on mainnet.Response code 502 (Bad Gateway)  This failure means that the observer was able to connect to the gateway's network, but the reverse proxy returned a 502 error. This is almost always a reverse proxy issue. Ensure that the gateway's reverse proxy is running, and that it is configured to forward requests to the gateway.Testing the validity of the reverse proxy's configuration file (sudo nginx -t on Nginx) may provide more information about the issue, and restarting the reverse proxy (sudo nginx -s reload) often resolves the issue if there are no problems with the configuration file.It is also possible that the gateway itself is not running at all. Check Docker and the gateway's logs to see if the gateway is on.Response code 503 (Service Unavailable)  This failure means that the observer was able to connect to the gateway's network, but the reverse proxy was unable to forward the request to the gateway. It differs from the 502 error in that the reverse proxy is likely able to see that the gateway is running, but is unable to communicate with it. This is often a temporary issue, caused by the gateway not being able to handle a heavy load of requests, or the gateway being in the process of restarting. If this failure occurs once or twice in a report, it is likely a temporary issue and should not be considered an issue with the gateway. However, when this failure occurs persistently, particularly for every ArNS name checked on the report, it is likely that the gateway may have crashed.Manually restarting the gateway can likely resolve the issue.connect EHOSTUNREACH  This failure means that the observer was unable to connect to the gateway at all. The connection was either refused, or the gateway was not able to find a target based on the domain name's DNS records.This is almost always an issue with DNS records or local network configuration. Ensure that the gateway domain has correct DNS records, and that the local network is set up to allow connections. Checking logs from the local network's reverse proxy (nginx, etc) may provide more information about the issue.getaddrinfo ENOTFOUND  This is another DNS related issue. Likely, the gateway does not have a valid DNS record either for the top level domain or the required wildcard subdomain. Having this failure occur once or twice in a report could mean that the DNS server being used by the observer is having temporary issues and should not be considered an issue with the gateway. However, when this failure occurs persistently, particularly for every ArNS name checked on the report, it is likely that the gateway's DNS records are not set, or are misconfigured.Hostname/IP does not match certificate's altnames: Host: <gateway-domain>. is not in the cert's altnames: DNS:<gateway-domain>  This failure means that the observer's SSL certificate does not match the gateway's domain name. This is almost always an issue with the gateway's SSL certificate. This most likely occurred because the gateway's operator did not update the gateway's SSL certificate when the gateway's domain name was changed. Obtaining a new SSL certificate and updating the gateway's reverse proxy configuration to use the new certificate is the only solution to this issue.write EPROTO <connection-id>:error:<error-code>:SSL routines:ssl3_read_bytes:tlsv1 unrecognized name:<path-to-openssl-source>:SSL alert number 112  This failure almost always means that the gateway operator did not properly obtain SSL certificates for the gateway's wildcard subdomain. Obtaining a new SSL certificate and updating the gateway's reverse proxy configuration to use the new certificate is the only solution to this issue. FAQ Why was my reward different this epoch?Show answer Gateway protocol rewards are calculated as 0.1% of the protocol balance (0.05% after August 2025) split between all gateways in the network. A change in the protocol balance or the number of gateways in the network between epochs will result in the reward for an individual gateway changing.The Observer rewards are separate from protocol rewards, and if your gateway is selected as an observer for an epoch, assuming it performs its duties well, it will receive additional rewards I have a high stake on my gateway, why am I not an observer?Show answer The observer selection process uses a weighted random selection method that considers multiple factors beyond just stake:Stake Weight (SW): Ratio of your total staked ARIO tokens (including delegated stake) to the network minimum Tenure Weight (TW): How long your gateway has been part of the network (capped at 4 after 2 years) Gateway Performance Ratio Weight (GPRW): Ratio of epochs where you correctly resolved names vs total participation Observer Performance Ratio Weight (OPRW): Ratio of epochs where you successfully submitted reports vs total observer periods A composite weight (CW) is calculated as: CW = SW × TW × GPRW × OPRW Up to 50 gateways are chosen as observers per epoch. If there are more than 50 gateways, selection is randomized based on these normalized weights. Even with a high stake, other factors like performance and tenure affect your chances of being selected.I withdrew my stake, but now I have less Show answer There is a 90 day locking period when withdrawing stake, either from delegated stake or operator stake on your gateway. This locking period can be skipped, for a fee. The fee starts at 50% of the withdrawal amount, and goes down over time. If you selected instant withdrawal, you paid the fee to skip the locking period.Why Can't I withdraw my stake?Show answer The minimum operator stake for gateways (10,000 ARIO) cannot be instantly withdrawn, it is subject to the full 90 day locking period, and withdrawal can only be started by removing your gateway from the network.I would like to move my node to a new server - how?Show answer If possible, leave your original server running while you prepare the new one Set up the new server following the same steps you used to set up the original server This includes setting up SSL certificates for the new server You must use the same gateway wallet when setting up the new server The observer wallet may be changed at any point, but requires extra steps. It is recommended you use the original observer wallet as well Once the new server is set up, change your DNS A records to point at the new server After your DNS records are set and you have verified your gateway is operating correctly, shut down the original server No changes need to be made in the network contract or on ar://gateways Can I change my nodes FQDN?Show answer Yes Configure your new domain to point at your gateway, including setting up SSL certificates Update your NGINX (or other reverse proxy) server to recognize the new domain. This usually requires a restart of NGINX Update the ARNS_ROOT_HOST variable in your .env and restart the gateway Using ar://gateways, update your gateway settings to change the FQDN in the contract Your gateway is now using the new domain name for normal operation.

---

# 118. ARIO Docs

Document Number: 118
Source: https://docs.ar.io/gateways/windows-setup
Words: 1304
Extraction Method: html

Windows Installation Instructions Overview This guide provides step-by-step instructions for setting up the AR.IO node on a Windows computer. It covers installing necessary software, cloning the repository, creating an environment file, starting the Docker container, setting up networking, and installing and configuring NGINX Docker. No prior coding experience is required.Prerequisites Before starting the installation process, ensure you have the following:A Windows computer Administrative privileges on the computer Install Required Packages Install Docker:Download Docker Desktop for Windows from here.Run the installer and follow the prompts.During installation, make sure to select the option to use WSL (Windows Subsystem for Linux) rather than Hyper-V.Restart your PC.Update Windows Subsystem for Linux (WSL):Open the command prompt as an administrator:Press Windows Key + R.Type cmd and press Enter.Right-click on the "Command Prompt" application in the search results.Select "Run as administrator" from the context menu.Run the following commands:Restart Docker Desktop.Install Git:Download Git for Windows from here.Run the installer and use the default settings.Clone the Repository Clone the main repository:Open the command prompt:Press Windows Key + R.Type cmd and press Enter.Navigate to the directory where you want to clone the repository:Use the cd command to change directories. For example, to navigate to the Documents directory:More detailed instructions on navigating with the cd command can be found here NOTE: Your database of Arweave Transaction Headers will be created in the project directory, not Docker. So, if you are using an external hard drive to turn an old machine into a node, install the node directly to that external drive.Run the following command:Create the Environment File Create an environmental variables file:Open a text editor (e.g., Notepad):Press Windows Key and search for "Notepad".Click on "Notepad" to open the text editor.Paste the following content into the new file, replacing <your-domain> with the domain address you are using to access the node, and <your-public-wallet-address> with the public address of your Arweave wallet:The GRAPHQL values set the proxy for GQL queries to arweave.net, You may use any available gateway that supports GQL queries. If omitted, your node can support GQL queries on locally indexed transactions, but only L1 transactions are indexed by default.START_HEIGHT is an optional line. It sets the block number where your node will start downloading and indexing transactions headers. Omitting this line will begin indexing at block 0.RUN_OBSERVER turns on the Observer to generate Network Compliance Reports. This is required for full participation in the AR.IO Network. Set to false to run your gateway without Observer.ARNS_ROOT_HOST sets the starting point for resolving ARNS names, which are accessed as a subdomain of a gateway. It should be set to the url you are pointing to your node, excluding any protocol prefix. For example, use node-ar.io and not https://node-ar.io. If you are using a subdomain to access your node and do not set this value, the node will not understand incoming requests.AR_IO_WALLET is optional, and sets the wallet you want associated with your Gateway. An associated wallet is required to join the AR.IO network.OBSERVER_WALLET is the public address of the wallet used to sign Observer transactions. This is required for Observer to run, but may be omitted if you are running a gateway outside of the AR.IO network and do not plan to run Observer. You will need to supply the keyfile to this wallet in the next step.Advanced configuration options can be found at docs.ar.io Save the file with the name ".env" and make sure to select "All Files" as the file type. This helps to ensure the file saves as ".env" and not ".env.txt" Note: The .env file should be saved inside the same directory where you cloned the repository (e.g., ar-io-node).Supply Your Observer Wallet Keyfile:If you are running Observer, you need to provide a wallet keyfile in order to sign report upload transactions. The keyfile must be saved in the wallets directory in the root of the repository. Name the file <Observer-Wallet-Address>.json, replacing "<Observer-Wallet-Address>" with the public address of the wallet. This should match your OBSERVER_WALLET environmental variable.Learn more about creating Arweave wallets and obtaining keyfiles here Start the Docker Containers Start the Docker container:Open the command prompt:Press Windows Key + R.Type cmd and press Enter.Navigate to the directory where you cloned the repository (e.g., ar-io-node):Use the cd command to change directories. For example, if the repository is located in the Documents directory, you would enter:If the directory path contains spaces, enclose it in double quotation marks. For example:Use the dir command to list the contents of the current directory and verify that you're in the correct location:dir Once you are in the correct directory, run the following command to start the Docker container:Explanation of flags:up: Start the Docker containers.-d: Run the containers as background processes (detached mode).NOTE: Effective with Release #3, it is no longer required to include the --build flag when starting your gateway. Docker will automatically build using the image specified in the docker-commpose.yaml file.The gateway can be shut down using the command:If prompted by the firewall, allow access for Docker when requested.Set Up Router Port Forwarding To expose your node to the internet and use a custom domain, follow these steps:Obtain a Domain Name:Choose a domain registrar (e.g., Namecheap) and purchase a domain name.Point the Domain at Your Home Network:In your browser, go to https://www.whatsmyip.org/ to display your public ip address. It can be found at the top of the screen. Note this number down.Access your domain registrar's settings (e.g., Namecheap's cPanel).Navigate to the DNS settings for your domain. In cPanel this is under the "Zone Editor" tab.Create an A record with your registrar for your domain and wildcard subdomains, using your public IP address. For example, if your domain is "ar.io," create a record for "ar.io" and "*.ar.io." Instructions may vary depending on the domain registrar and cPanel. Consult your registrar's documentation or support for detailed steps.Obtain the Local IP Address of Your Machine:Open the command prompt:Press Windows Key + R.Type cmd and press Enter.Run the following command:ipconfig Look for the network adapter that is currently connected to your network (e.g., Ethernet or Wi-Fi).Note down the IPv4 Address associated with the network adapter. It should be in the format of 192.168.X.X or 10.X.X.X.This IP address will be used for port forwarding.Set Up Router Port Forwarding:Access your home router settings:Open a web browser.Enter your router's IP address in the address bar (e.g., 192.168.0.1).If you're unsure of your router's IP address, consult your router's documentation or contact your Internet Service Provider (ISP).Navigate to the port forwarding settings in your router configuration.The exact steps may vary depending on your router model. Consult your router's documentation or support for detailed steps.Set up port forwarding rules to forward incoming traffic on ports 80 and 443 to the local IP address of your machine where the node is installed.Configure the ports to point to the local IP address noted in the previous step.Save the settings.Install and Configure NGINX Docker Clone the NGINX Docker repository:Open the command prompt:Press Windows Key + R.Type cmd and press Enter.Navigate to the directory where you want to clone the repository (This should not be done inside the directory for the node):Use the cd command to change directories. For example, to navigate to the Documents directory:Run the following command:Note: This NGINX container was designed to easily automate many of the more technical aspects of setting up NGNIX and obtaining an ssl certificate so your node can be accessed with https. However, wildcard domain certifications cannot be universally automated due to significant security concerns. Be sure to follow the instructions in this project for obtaining wildcard domain certificates in order for your node to function properly.Follow the instructions provided in the repository for setting up NGINX Docker.Congratulations! Your AR.IO node is now running and connected to the internet. Test it by entering https://<your-domain>/3lyxgbgEvqNSvJrTX2J7CfRychUD5KClFhhVLyTPNCQ in your browser.Note: If you encounter any issues during the installation process, please seek assistance from the AR.IO community.

---

# 119. Importing SQLite Database Snapshots - ARIO Docs

Document Number: 119
Source: https://docs.ar.io/gateways/snapshots
Words: 483
Extraction Method: html

Overview One of the challenges of running an AR.IO Gateway is the initial synchronization time as your gateway builds its local index of the Arweave network. This process can take days or even weeks, depending on your hardware and the amount of data you want to index. To accelerate this process, you can import a pre-synchronized SQLite database snapshot that contains transaction and data item records already indexed.This guide will walk you through the process of importing a database snapshot into your AR.IO Gateway.Note The below instructions are designed to be used in a linux environment. Windows and MacOS users must modify the instructions to use the appropriate package manager/ command syntax for their platform.Unless otherwise specified, all commands should be run from the root directory of the gateway.Obtaining a Database Snapshot SQLite database snapshots are very large and not easy to incrementally update. For these reasons, AR.IO is distributing them using BitTorrent. These snapshots can be downloaded using any preferred torrenting client, and below is instructions on doing so using transmission-cli from a terminal.This will download a snapshot, current to April 23, 2025, of an unbundled data set that includes all data items uploaded via an ArDrive product, including Turbo. The file will be named 2025-04-23-sqlite.tar.gz and be approximately 42.8Gb in size.Note While continuing to seed the torrent after download is not required, it is highly recommended to help ensure the continued availability of the snapshot for others, as well as the integrity of the data. Seeding this file should not cause any issues with your internet service provider.This is a compressed tarball, so it will need to be extracted before it can be used.Extracting the Database Snapshot Once the file has downloaded, you can extract it using the following command, be sure to replace the filename with the actual filename of the snapshot you are using, if not using the example above.This will extract the file into a directory matching the filename, minus the .tar.gz extension.Importing the Database Snapshot Once you have an extracted database snapshot, you can import it into your AR.IO gateway by replacing the existing SQLite database files. Follow the instructions below to do so.IMPORTANT Importing a database snapshot will delete your existing database and replace it with the snapshot you are importing.Stop your AR.IO gateway.(Optional) Backup your existing SQLite database files.Delete the existing SQLite database files.Move the snapshot files into the data/sqlite directory.Be sure to replace 2025-04-23-sqlite with the actual directory name of the extracted snapshot you are using.Start your AR.IO gateway.Verifying the Import The simplest way to verify the import is to check the gateway logs to see what block number is being imported. The 2025-04-23 snapshot was taken at block 1645229, so the gateway will start importing blocks after this height if the snapshot was imported successfully.You can also use the Grafana Sidecar to view the last block imported in a more human readable format.

---

# 120. Deploy a dApp with ArDrive web - ARIO Docs

Document Number: 120
Source: https://docs.ar.io/guides/ardrive-web
Words: 790
Extraction Method: html

ArDrive Web Deployment Guide Overview This guide will outline the simple steps needed to deploy your dApp or website onto the Arweave blockchain using the ArDrive web app and friendly UI.Simple apps and websites should work right out of the box. However, for advanced applications, this assumes you have already prepared your dApp to use hash routing and relative file paths, and built static files for any dApp in a language or framework that requires it (like React).Learn more about preparing your dApp for deployment here.Deploying Step 1: Log into ArDrive Go to the ArDrive web app and log in using the method of your choosing. If you don't already have an account, you will need to follow the instructions to set one up.Step 2: Select or Create a Drive Once logged in, navigate to the drive where you want your project to be hosted. If you haven't created a drive yet, or if you want a new one specifically for this project, click the big red "New" button at the top left and create a new drive. Remember, the drive needs to be set to public for your dApp to be accessible to others.Step 3: Upload your project With your drive selected, click the big red "New" button again, but this time, select "Upload Folder". Navigate to your project's root directory, or the built directory if required, and select it. This will upload the entire directory, maintaining your project's file structure.Step 4: Confirm Upload You'll be given a chance to review the upload and the associated cost. If everything looks right, click "Confirm". Remember, uploading to Arweave isnt free, but the cost is usually quite small and the benefits of having your dApp or website hosted on the permaweb are significant.Step 5: Create the Manifest While ArDrive displays your uploaded files as a traditional file structure, with files and folders inside other folders, thats not how they actually exist on Arweave. The manifest acts as a map to all the files your dApp needs to function. After you confirm your upload, navigate into your newly created folder by double clicking on it. Click the big red "New" button again and select "New Manifest" in the "Advanced" section. You'll be prompted to name the manifest and choose where to save it. Be sure to save it inside the folder you just created.Step 6: Get the Data TX ID Once the manifest is created, click on it to expand its details. In the "details" tab, on the bottom right, there's a line labeled "Data TX ID". This is the unique identifier for your uploaded dApp on Arweave. Copy this value.Step 7: View and Share your dApp Your dApp or website is now available on the permaweb forever! Append the Data TX ID you just copied to the end of an Arweave gateway URL, like https://arweave.net/. It might take a few minutes for all of your files to finish propagating through the network, but once they do your dApp or website will be accessible to anyone, anywhere, at any time.Step 8: Assign a Friendly Name The Data TX ID you copied in Step 6 is long and difficult to remember. To make it easier to access your dApp or website, you can assign a friendly name to it using ArNS. If you already own an ArNS name, you will be prompted during the creation of
your manifest if you want to assign one. If you do not, you can purchase one from arns.app.You can also assign an ArNS name to an existing manifest (or any other file) by clicking on the three dots on the right side of the file and selecting "Assign ArNS name".Updating your dApp Files uploaded to Arweave are permanent and immutable. They cannot be changed. However, the Arweave File System (ArFS) protocol used (and created) by ArDrive lets you "replace" them with new versions while still being able to access the old ones. You can do this with entire dApps as well. The old files won't be displayed in the ArDrive web app unless you click on a file to view its history.Once you have made changes to your dApp or website, and built the static directory for it, you can upload the entire folder again to the same location where you uploaded the original. Follow all the same steps listed above for uploading your dApp. You will need to create a new manifest to correctly point to the updated files. Give it the same name as the old manifest in order to "replace" it. Creating the new manifest will generate a new TX ID used to view the updated dApp.The old version of the dApp will always be available to anyone who has the correct TX ID.

---

# 121. ANTs on Bazar - ARIO Docs

Document Number: 121
Source: https://docs.ar.io/guides/ants-on-bazar
Words: 529
Extraction Method: html

Trading ANTs on Bazar Overview Arweave Name Tokens are Atomic Asset Spec compliant AO tokens that manage records and permission for ArNS names. Because the ANT spec is compliant with the Atomic Asset Spec, they are tradable on Bazar, which is a decentralized market place for Atomic Assets on AO. There are a few simple steps that are required in order to make an ANT available on Bazar to be traded.Bazar relies on profiles for displaying user information and tradable assets. Profiles are AO processes that contain user specified information like a name, a nickname, and images associated with the profile. Profiles also track assets held by the profile in order to provide their information to bazar.Create a Profile If you do not already have a profile associated with your wallet, you can easily create one on using the "Create your profile" button on bazar after connecting your wallet: You will be prompted to add, at a minimum, a name and handle (nickname) to associate with the profile. These values can be changed later. Click "Save" at the bottom to finish creation of your profile.Once your profile is created, you can get its ao process Id at any time by clicking on the user icon in Bazar, and then the "Copy profile address" button from the menu. Bazar profiles only track assets that are held in the profile process, not in a user wallet. In order for an ANT to be displayed and transferred on Bazar, it must first be transferred into the Bazar profile. This can be done easily using arns.app    in your manage page for a given name.    Once an ANT is transferred into the profile process, it will automatically be detected and displayed by Bazar. It can be transferred or sold just like any other atomic asset on the marketplace, with no additional steps required.Restore Controllers Optional This is an optional step that will enable updating an ANT's Target Id without transferring it back into your wallet. This step may be safely skipped without affecting the ANT's functionality or tradability on Bazar.Transferring an ANT to a new wallet or AO process resets all authorized controllers, or non-owner entities that are allowed to update some settings on the ArNS name. It does not reset the Target Id that the ArNS name is pointing to. If you want to be able to update the Target ID and undernames from your wallet using arns.app, you will need to set your wallet address as a controller for the ANT while it is in your profile. The easiest way to do this is using aos.If you have not used aos before, you can find installation instructions here    Using aos, you can log directly into your profile process with the command:Be sure to replace <profile-address> with the process Id for your profile process, and /path/to/your/keyfile with the path to the keyfile for the wallet you created the profile with.Once you are logged in with aos, you can send a message to the ANT in your profile to set your wallet as a controller:Replace <Ant-Process-ID> with the process Id of the ANT you transferred into your profile, and <Wallet-Address> with your wallet address.

---

# 122. Arlink Deploy - ARIO Docs

Document Number: 122
Source: https://docs.ar.io/guides/arlink
Words: 462
Extraction Method: html

Overview Arlink is a third party tool that allows you to permanently deploy and manage web apps on the permaweb with ease.How it works Users can link their Github or Protocol.land repositories to their Arlink account through the Arlink dashboard. When a new project or build is deployed,
Arlink will take the repository, build it, and upload the build folder to Arweave.Arlink also allows users to connect their project to an ArNS name they own, or an undername of the ArNS name ar://arlink.Dashboard After connecting your wallet to the Arlink web app using the button at the top right, you will be taken to your dashboard. This page will display any deployments associated with your wallet, and includes a "+ New Deployment" button
in order to start the process of deploying a new project. New Deployment After clicking on the new deployment button, you will be prompted to import a repository from either Github or Protocol.land. Authorize Github If this is your first time importing from Github, you will be prompted to authorize Arlink to access your Github repositories. You can authorize all repositories, or limit authorization to any number of specific ones. Select Repository Once authorization is approved, select which repository and branch you want to deploy. Define Build and Output Steps Once you select what you want to deploy, you need to specify how the project needs to be built to get it ready. Arlink prompts for five inputs:Project Name: This is the name of your project.Install Command: The command for installing dependencies for your project. Usually npm install or yarn install Build Command: This is the command to run your build script. Usually npm run build or yarn build Sub Directory: If the front end for your project lives in a sub directory of your selected repository, you can specify that here.Output Directory: This is the path to the build folder being deployed. This will be different depending on the framework your project uses. Select ArNS The last thing to do is select an ArNS name to deploy your project to. If you own your own name, you can connect to it here with the "Use existing ArNS" toggle. Otherwise, you can select an undername of the ArNS name arlink to deploy to.
Duplicate undernames cannot exist, so you can only select an undername that is not already being used. Logs Once you select your ArNS name and click "Deploy", your project will be deployed. Logs from the build and deploy process will be displayed so you can monitor for errors. Updates To deploy a new build of your project, select it from the dashboard. The project page gives you the option to update any settings or configurations, and has a "Deploy Latest" button which will redeploy your project.

---

# 123. Glossary - ARIO Docs

Document Number: 123
Source: https://docs.ar.io/glossary
Words: 1405
Extraction Method: html

Many novel terms and acronyms are used by the Arweave ecosystem as well as some new ones introduced by AR.IO. The list below is intended to serve as a non-exhaustive reference of those terms. For a comprehensive glossary of permaweb-specific terminology, check out the permaweb glossary section:AO Computer (AO):The AO Computer is an actor-oriented machine on the Arweave network, creating a unified computing environment across diverse nodes. It supports many parallel processes through an open message-passing layer, linking independent processes into a cohesive system, similar to how websites are interconnected via hyperlinks.Arweave Name System (ArNS):a decentralized and censorship-resistant naming system enabled by AR.IO gateways which connects friendly names to permaweb applications, pages, data or identities.Arweave Name Token (ANT), "Name Token":A an AO Computer based token, that is connected to each registered ArNS Name. Each ANT gives the owner the ability to update the subdomains and Arweave Transaction IDs used by the registered name as well as transfer ownership and other functions.Arweave Network Standards (ANS):Drafts and finalized standards for data formats, tag formats, data protocols, custom gateway features and anything that is built on top the Arweave Network. Specific standards are denoted by an associated number, e.g., ANS-###.Base Layer Transaction:refers to one of up to 1,000 transactions that make up a single Arweave block. A base layer transaction may contain bundled data items.Bundle, bundling:an Arweave concept introduced in ANS-104 that allows for a way of writing multiple independent data transactions into one base layer transaction. Bundled transactions contain multiple independent transactions, called data items, wrapped into one larger transaction. This offers two major network benefits:A scaling solution for increasing the throughput of uploads to the Arweave network,Allows delegation of payment for an upload to a third party, while maintaining the identity and signature of the person who created the upload, without them needing to have a wallet with funds.Bundled Data Item (BDI):A data item / transaction nested within an ANS-104 bundled transaction.Bundler:A third-party service and gateway feature that bundles data files on a user's behalf.Chunk:A chunk is a unit of data that is stored on the Arweave network. It represents a piece of a larger file that has been split into smaller, manageable segments for efficient storage and retrieval.Decentralized, decentralization, etc:A nonbinary, many axis scale enabling a system or platform to be: permissionless, trustless, verifiable, transparent, open-source, composable, resilient, and censorship resistant. Ultimately, something that is decentralized is not prone to single points of failure or influence.Epoch:a specific duration (e.g., one day) during which network activities and evaluations are conducted. It serves as a key time frame for processes such as observation duties, performance assessments, and reward distributions within the network's protocols.Gateway:A node operating on the Arweave network that provides services for reading from, writing to, and indexing the data stored on the permaweb. Sometimes referred to as "permaweb nodes".Gateway Address Registry (GAR):a decentralized directory maintained in the AR.IO smart contract. It serves as the authoritative list of all registered gateways on the AR.IO Network. The registry provides detailed metadata about each gateway to facilitate discovery, health monitoring, and data sharing among apps, users and other infrastructure. The GAR is designed to be easily queryable, sortable, and filterable by end users and clients, allowing for tailored selections based on various criteria to meet specific use cases.Indexing:The act of organizing transaction data tags into queryable databases.Layer 2 Infrastructure:Layer 2 refers to the technology / infrastructure stack built "above" a base layer. In this use, the AR.IO Network would be considered Layer 2 infrastructure to the base Arweave protocol.Manifest (aka Path Manifest, Arweave Manifest):Special "aggregate" files uploaded to Arweave that map user-definable sub-paths with other Arweave transaction IDs. This allows users to create logical groups of content, for example a directory of related files, or the files and assets that make up a web page or application. Instead of having to manually collate these assets, manifests group them together so that an entire website or app can be launched from a single manifest file. Gateways can interpret this structure, so that users can then reference individual transactions by their file name and/or path.Mempool:Short for "memory pool," is a component of Arweave mining nodes that temporarily stores valid transactions that have been broadcasted to the network but have not yet been added to a block.Message:An interaction with an AO Process, including action and tags. Every interaction with AO takes the form of a message.Miner (aka Arweave Node):A node operating on the Arweave network responsible for data storage and recall.Native Address:The way public addresses are commonly (or by spec) represented in their native blockchain. Arweave keys are 43 character base64url representations of the public key, while Ethereum keys use a different hashing algorithm and start with 0x etc.Normalized Address:43 character base64url representation of the sha256 hash of a public key. Public keys for other chains can be normalized by this representation.Observer:A gateway selected to evaluate the performance of peer gateways in resolving ArNS names. Observers assess and report on the operational efficacy of other gateways.Optimistic Indexing:Indexing transaction or data item headers before the associated L1 transaction has been accepted and confirmed in a chain block.Owner:Generally, the public key of the signer.Owner Address:The normalized address of the owner Period:Refers to a predefined time span (e.g., a day) that serves as a cycle for network activities such as dynamic pricing. It is a fundamental unit of time for operational and protocol processes within the network.Permanent Cloud Network:A decentralized network that securely stores, distributes, and serves data and applications in a timeless, tamper-proof, and universally accessible way. Unlike traditional clouds, it ensures data permanence and user sovereignty by eliminating reliance on centralized providers and creating a resilient, censorship-resistant infrastructure.Permaweb:The permaweb is the permanent and decentralized web of files and applications built on top of Arweave.Process:Process: A decentralized computation unit in the AO framework, enabling scalable, parallel execution via message-passing. Each process maintains its own state, interacts asynchronously, and is permanently stored on Arweave for transparency and immutability.Process ID (PID):Every process in AO is assigned a unique immutable identifier code.Protocol Balance:The primary sink and source of ARIO tokens circulating through the AR.IO Network. This balance is akin to a central vault or wallet programmatically encoded into the network's smart contract from which ArNS revenue is accumulated and incentive rewards are distributed.Protocol Rewards:ARIO Token incentive rewards distributed by the protocol to the network's eligible users and gateway operators.Public Key:The publicly known keys for a signer (wallet). Public keys are different byte lengths depending on the signer type (e.g. Arweave vs. Ethereum (ECDSA), vs Solana, etc.) Seeding:Refers to the act of propagating new data throughout the network. Miner nodes seed Arweave base layer transaction data to other miners, while gateways ensure that the transactions they receive reach the Arweave nodes. Both gateways and Arweave nodes seed base layer transactions and data chunks.Staking (of tokens):Refers to the process of locking ARIO tokens into a protocol-facilitated vault, temporarily removing them from circulation until unlocked. This action represents an opportunity cost for the gateway operator and serves as a motivator to prioritize the network's collective interests.Stake Redelegation:The process by which stakers move their delegated tokens from one gateway to another.Stake Redemption:A feature allowing stakers to use their staked tokens for ArNS-related activities, such as purchasing names, extending leases, or increasing undername capacity.Transaction ID (txID):Every transaction and data file uploaded to Arweave is assigned a unique identifier code known as the Transaction ID. These txID's can be referenced by users to easily locate and retrieve files.Trust-minimization:Relates to enacting network security by minimizing the number of entities and the degree to which they must be trusted to achieve reliable network interactions. A network with trust-minimizing mechanisms means that it has reduced exposure to undesirable third-party actions and built-in incentives to reward good behavior while punishing bad behavior.Vault:Token vaults are protocol level mechanisms used to contain staked tokens over time. Each vault contains a starting timestamp, ending timestamp (if applicable), along with a balance of tokens.Wayfinder Protocol:The Wayfinder protocol provides applications with a pattern for dynamically switching / routing between network gateways. It also allows for abstraction of top level domain names from Arweave data and verifies the responses from AR.IO Gateways. It forms the basis of the ar:// schema, so users can seamlessly access ArNS names, Arweave base layer transactions, and bundled data items without the user providing a top-level domain.Permaweb Glossary For a more comprehensive glossary of terms used in the permaweb, try the Permaweb Glossary. Or use it below:

---

# 124. ARIO Docs

Document Number: 124
Source: https://docs.ar.io/guides/gql
Words: 1295
Extraction Method: html

GraphQL Overview GraphQL is a powerful query language designed for modern web applications to efficiently fetch data. It enables precise queries, allowing users to specify exactly which data they need and in what format, significantly reducing the amount of unnecessary data transferred. This approach is ideal for dealing with complex systems and large datasets, as it minimizes bandwidth and improves performance. GraphQL operates through a single endpoint, streamlining the way applications communicate with databases.The integration of GraphQL with Arweave introduces a refined method for interacting with decentralized data storage. Arweave allows for the tagging of uploaded data, facilitating enhanced searchability and retrievability within its blockchain network. Utilizing GraphQL, users can perform targeted queries that leverage these tags, ensuring the retrieval of specific data swiftly and efficiently. This capability is particularly beneficial for the development of decentralized applications (dApps), the archival of content in a permanent and unalterable form, and the establishment of data marketplaces where precision and efficiency in data access are paramount.Together, GraphQL and Arweave form a compelling combination, offering developers and users a robust framework for managing and querying data in a decentralized environment. This integration not only promotes the efficient and scalable retrieval of data but also supports the creation of more sophisticated and data-intensive applications on the decentralized web, maintaining a balance between technical depth and accessibility.Constructing a Query Basic Syntax In GraphQL, you start with a root field and use braces to outline the fields you want to retrieve, allowing for precise, hierarchical data requests. For instance:This query demonstrates fetching transactions and their tags, illustrating the hierarchical nature of GraphQL queries.Customizing Searches with Tags Arweave utilizes a tagging system for transactions, enabling intricate search capabilities. You can filter queries using these tags:This example filters transactions by a specific application name, and returns the id, size, and type of the transaction, showcasing how to customize queries for targeted data retrieval.NOTE: Tags are not the only option for filtering results, but are extremely useful due to the ability to add custom tags during the upload process.Understanding Edges and Nodes In the realm of GraphQL queries, especially when interfacing with Arweave, grasping the concept of edges and nodes is pivotal for constructing efficient and effective queries. This structure is not unique to Arweave but is particularly relevant due to the decentralized and interconnected nature of the data stored on its blockchain.Nodes: At the heart of GraphQL's query structure, nodes represent individual data points or entities. In the context of Arweave, a node could be a transaction, a block, or any piece of data stored within the network. Nodes are the primary targets of your query, containing the data you wish to retrieve, such as transaction IDs, tags, or the content of data transactions.Edges: Serving as the glue between nodes, edges are constructs that outline the relationship between different nodes. They can contain metadata about the connection, such as the nature of the relationship or additional attributes that describe how nodes are linked. In many GraphQL implementations, including those that interact with Arweave, edges are used to navigate through collections of related data, making them crucial for understanding the data's structure and lineage.This hierarchical model is especially useful for querying complex and relational data sets, allowing for detailed navigation and efficient data retrieval within Arweave's decentralized storage system. By effectively utilizing the edges and nodes structure, you can precisely target the data you need, whether it's filtering transactions by tags, fetching related transactions, or exploring the blockchain's structure.Pagination To add pagination to your GraphQL queries, you can use the first, last, before, and after parameters. These parameters control the slice of data you're querying, making data retrieval more efficient and manageable.first: Specify the number of items to retrieve from the start of the list or dataset.last: Specify the number of items to retrieve from the end of the list or dataset.This query fetches the first 10 transactions.To navigate through your dataset, you can use after and before in conjunction with first or last. These parameters accept cursors, which are typically provided in the response of your initial query.after: Fetch items after the specified cursor, used with first.before: Fetch items before the specified cursor, used with last.This query fetches the next 10 transactions following the transaction with the cursor "cursorOfLastItem".If no pagination terms are set, GraphQL servers may apply default limits to prevent excessively large datasets from being returned in a single query, potentially impacting performance. The default behavior can vary based on the server's configuration but often involves returning a predefined maximum number of items.For instance, without specifying first or last, a query to the transactions field might return the first 5-10 transactions by default, depending on the server settings.This behavior ensures that server resources are not overwhelmed by large requests and that client applications receive data in manageable chunks.General Tips for Optimizing Queries To optimize your GraphQL queries in Arweave, follow these general guidelines:Specificity: Query with the most precise tags possible to narrow the search scope and enhance performance.Minimalism: Limit your query to the essential set of tags to reduce processing time and data transfer.Schema Design: Design your app's schema to reflect query patterns, possibly introducing tags that encapsulate frequent combinations of criteria.Include Non-tag Fields: Adding fields like owner can refine your search, making your queries more efficient.Order Your Tags: Arrange tags from most specific to most general to leverage Arweave's indexing more effectively.By incorporating these strategies, developers can achieve faster and more precise data access from Arweave, enhancing the performance and responsiveness of decentralized applications. This balanced approach to query construction and optimization is key to navigating the expansive and decentralized storage landscape Arweave provides.Making a Query Executing GraphQL queries within the Arweave ecosystem offers flexibility and multiple avenues for developers and users alike. Whether you prefer a hands-on, manual approach to constructing and testing queries, or you aim for automation and integration within your applications, Arweave provides the tools necessary to interact with its decentralized data storage seamlessly.GraphQL Playground For those new to GraphQL or seeking to fine-tune their queries before implementation, the GraphQL playground offers an invaluable resource. This interactive interface allows users to manually construct queries, explore the schema, and immediately see the results of their queries. Accessible via web browsers, the playground can be found at the /graphql endpoint of most Arweave indexing services, such as https://arweave.dev/graphql. Here, you can experiment with different queries, understand the structure of the data, and refine your approach without writing a single line of code in your application.Steps for Accessing the GraphQL Playground:Navigate to https://arweave.dev/graphql, or the graphql endpoint of any AR.IO gateway, in your web browser.Enter your GraphQL query in the provided interface.Press the "play" button to execute the query to see real-time results and debug as needed.Using an API For application development and automation, making GraphQL queries programmatically is essential. You can send POST requests directly to the GraphQL endpoint of any indexing service that supports it, such as arweave.net or any AR.IO gateway. These requests should contain your query in the body, allowing for dynamic and automated data retrieval within your application.When selecting an indexing service, consider the data coverage and reliability of the gateway to ensure it meets your application's needs. Different gateways might have varying degrees of indexed data available, so choosing one that is consistently up-to-date and comprehensive is key.Example of making a programmatic query:Using an SDK For an even more integrated experience, some Software Development Kits (SDKs) offer direct methods for executing GraphQL queries. The Arweave SDK, for example, provides built-in functionalities to interact with the blockchain, simplifying the process of making queries. By leveraging these SDKs, developers can bypass the intricacies of manual HTTP request construction, focusing instead on the logic and design of their applications.Example of using the Arweave SDK for GraphQL queries:

---

# 125. ARIO Docs

Document Number: 125
Source: https://docs.ar.io/guides/permaweb-deploy
Words: 1060
Extraction Method: html

Deploy a Website or Application Overview With the growing popularity of permanently deployed apps, hosted on Arweave, along with the growing list of tools offered by AR.IO, several methods have been developed to automate the process of deploying a website and updating the ArNS name pointed at it. A particularly useful tool for this is permaweb-deploy from Forward Research.permaweb-deploy is a cli tool that handles uploading a build folder to Arweave using Turbo, creating a manifest, and then updating an ArNS name to point at the new manifest. It being a cli tool makes it very easy to incorporate into a github actions flow. Setting up an automated deployment with permaweb-deploy is simple, but does require a few steps.ENV Security Before automating your deployments, be sure to build your app and check for exposed environmental secrets. Some app frameworks or build flows will build your app with the secrets exposed, and if you are using a tool like permaweb-deploy, those secrets will be uploaded to Arweave. Since the permaweb is permanent, this could pose a security risk, especially with a copy of your wallet keyfile required for the deployment automation.Getting Started Installing package permaweb-deploy is an npm package, and must be installed in any project before it can be used. If you are using npm, you can install the package with the below command:If you prefer yarn for your package installations, the process is slightly more involved. permaweb-deploy is not designed for installation with yarn, so you must provide the additional argument ignore-engines in order to skip over the yarn version error you would normally get with installation. There are two methods for doing so:Directly in the install command In a .yarnc file You can provide a file, named .yarnc in the same directory as your package.json in order to assign specific instructions to all of your yarn commands. Creating a .yarnc file with the line will have the same effect as providing the flag directly in your yarn command Adding a Deploy Script The simplest way to utilize the permaweb-deploy tool is to build it into a script in your package.json. Here you will provide all of the variables that permaweb-deploy needs in order to function properly, as well as ensure that your app is statically built before being uploaded.Be sure to replace <YOUR_ARNS_NAME> with the name of the ArNS name you want to deploy to.The above example shows a build script for a vuepress app, which will build the app into a static folder for deployment, and a deploy script which runs build and then permaweb-deploy. Your build script will look different depending on the framework you are using, but most will provide that for you when you create your app.The permaweb-deploy command has two required arguments:--deploy-folder This is the relative path (from your package.json) to the build folder you want to upload. In a vuepress app, that will be ./src/.vuepress/dist unless you manually specify otherwise in your vuepress configuration. It will be different depending on your chosen framework and if you have modified the default location.--arns-name This is the ArNS name you want to deploy to. It must be an ArNS name that the wallet used to authenticate has ownership or controller privileges over, otherwise the deployment will fail at authentication in the ao process that controls the ArNS name.Undernames The --arns-name flag MUST be the top level name, not and undername. That is, if you want to deploy to undername_arnsname you must set --arns-name arnsname and not --arns-name undername_arnsname.There is the additional, optional flag --undername. If you want to deploy your app to an undername on an ArNS name, provide that name with this flag.--arns-name arnsname --undername undername Testnet Permaweb-deploy supports both Mainnet and Testnet deployments. By default, it will deploy to Mainnet. To deploy to Testnet, you can provide the --ario-process flag as "testnet". If not provided, deployments will default to Mainnet.Providing Arweave Wallet Keys While using permaweb-deploy, you will be uploading data to Arweave using Turbo, as well as performing protected actions on an Arweave Name Token. Because of this, you will need to provide the keys to an Arweave wallet in order for the actions to be successful. The wallet must contain Turbo Credits to pay for the upload, and it must either be a controller or the owner of the ArNS name you are trying to update.permaweb-deploy requires your wallet keyfile be encoded in base64 format. You can convert a local keyfile to base64, and copy the new value to your clipboard by using one of the below commands, depending on your operating system:Linux Mac Windows (CMD) Be sure to replace wallet.json with the path to your chosen wallet keyfile. Once you have this value saved to your clipboard, you can move on to the next step.Create Github Secrets Anyone who has your wallet keyfile (including the base64 formatted keyfile) has full control over your wallet and any of its assets. Because of this, you do not want to include it directly in your package.json script. Instead, keep the value safe by storing it in a github secret. You will create the secrets in the settings tab on your github repo, and the secrets will act as environmental variables in the github actions workflow.You will need to create 1 secret DEPLOY_KEY: This is the base64 encoded version of your Arweave wallet keyfile.Create Action Workflow Github Actions allow you to perform specific actions whenever you push code to github. They are handled by using .yaml files provided in <root-of-project>/.github/workflows.To get started, create a new file named deploy.yaml in the workflows directory, then paste the below inside of it:The above tells github to perform these actions when you push new code to the branch main It then sets up a vps with nodejs v 20. When that is complete, it installs dependencies for your project using npm (You will need to add a step to install yarn if that is your preferred package manager), and runs your deploy script, which builds your static folder and then runs permaweb-deploy. It also loads your github secrets into environmental variables that can be used by your deploy script.Deploying App With the above setup complete, the only thing you need to do to deploy a new version of a permasite app to Arweave is push the updated code to branch main on github. Everything else is fully automated.

---

# 126. Managing Undernames - ARIO Docs

Document Number: 126
Source: https://docs.ar.io/guides/managing-undernames
Words: 302
Extraction Method: html

Overview ArNS undernames are subdomains of top level ArNS domains. They are separated from the main ArNS domain using an underscore "_" in place of the more typically used dot ".".Records for undernames can be set using the setRecord method on the AR.IO SDK, or removed by using the removeRecord method.
The process for setting/removing a record for an undername vs. a top level ArNS domain is nearly identical, the only difference being the undername parameter. When managing a record on a top level ArNS domain, this must be set to @, while updates to an undername should provide the undername being updated.Chaining Undernames Undernames can be created on other undernames, for example ar://og_logo_ardrive. In this example the undername og exists under the undername logo on the ArNS name ardrive.For the purpose of the undername parameter in the AR.IO SDK, this should be written as a single undername, including the separating underscores:og_logo Creating an Undername There are no special steps required to create an undername (provided the selected ArNS name has available undername space). Simply setting a record for an undername that does not exist will create the undername.Updating an Undername If an undername already exists, its record can easily be updated using the same setRecord method.Removing an Undername An existing undername can be removed by using the removeRecord method on the AR.IO SDK.
The undername parameter should be set to the undername being removed.Increasing Undername Support By default, ArNS names support up to 10 undernames. This number can be increased, for a fee. This is done using the increaseUndernameLimit method on the ARIO class of the AR.IO SDK, rather than the ANT class.
The quantity (qty) parameter specifies the number of ADDITIONAL undernames to be supported. i.e. increasing from 10 undernames to 15 would require the qty parameter set to 5.

---

# 127. Managing Primary Names - ARIO Docs

Document Number: 127
Source: https://docs.ar.io/guides/primary-names
Words: 716
Extraction Method: html

Overview Primary names allow users to set a user-friendly alias for their Arweave wallet address, simplifying how addresses are displayed across applications. This process involves interaction between two separate smart contracts:The AR.IO Contract - which manages the primary name registry and requests The ANT Contract - which controls the specific ArNS name and must approve any primary name requests The process requires two steps because these are separate contracts:First, a request must be submitted to the AR.IO contract to set a specific ArNS name as the primary name for a wallet Then, the ANT owner must approve this request, confirming that this wallet can use the name as its primary identifier This two-step verification ensures that both the wallet owner and the ANT owner have authorized the connection.Think of this like setting a username on a social platform - where the
platform (AR.IO contract) maintains the registry of usernames, and the name
owner (ANT) must approve who can claim their name as an identifier. Setting a Primary Name with arns.app arns.app is the official ArNS portal from AR.IO. It allows you to manage your ArNS names and set primary names for your wallet addresses.To set a primary name using arns.app, connect your wallet and navigate to the ArNS name management page. Simply locate the ArNS name you want to set as primary and click the star icon at the right of the entry. You will then be prompted to accept the cost of setting the name, and the location of the funds to pay for the transaction. Once the transaction is confirmed, you will be prompted to sign the transaction with your connected wallet. When this is completed, the name will be set as primary for your wallet address, and apps that support primary names will display the name instead of the wallet address. Setting a Primary Name With the AR.IO SDK The process of setting a primary name using the AR.IO SDK involves two steps: requesting and approval. This two-step process ensures proper authorization from both the wallet owner and the ANT owner.Requesting a Primary Name When requesting a primary name, you're asking to use an ArNS name as the identifier for your wallet address. This requires:The ArNS name to exist Your wallet to submit the request using the requestPrimaryName method The ANT owner's approval Check Primary Name Requests The getPrimaryNameRequest method allows you to verify if a primary name request exists and its status. Use this to:Verify if your request is pending Check if someone has requested to use your ANT's name Build UI flows around the request/approval process Approving a Primary Name Request The ANT owner must approve any requests to use their name as a primary name using the approvePrimaryNameRequest method. This gives ANT owners control over how their names are used as identifiers.Querying Primary Names The AR.IO SDK provides several methods to query primary names, each serving different use cases:Get a Single Primary Name Use getPrimaryName when you need to find the primary name for a specific wallet address. This is particularly useful in applications where you want to display a user-friendly identifier instead of their wallet address.Common use cases:Displaying a user's primary name in a profile or dashboard Showing who authored a piece of content Making transaction histories more readable List All Primary Names Use getPrimaryNames when fetching all primary names. This is useful when you need to:Build a directory of users Create search functionality Display multiple users in a more readable format Map multiple wallet addresses to their friendly names at once The method supports pagination through a cursor-based system, where the cursor is the last name from your previous request.The response includes:items: Array of primary names for the current page cursor: The last name from the current request, used for getting the next page hasMore: Boolean indicating if there are more results available totalItems: Total number of primary names matching your query Best Practices Always verify ownership of both the ArNS name and ANT before attempting to set a primary name Check if a primary name request already exists before submitting a new one Consider implementing error handling for cases where the name or ANT doesn't exist When displaying primary names in your application, always have a fallback to show the wallet address if no primary name exists

---

# 128. ANTs on Bazar - ARIO Docs

Document Number: 128
Source: https://docs.ar.io/learn/guides/ants-on-bazar
Words: 529
Extraction Method: html

Trading ANTs on Bazar Overview Arweave Name Tokens are Atomic Asset Spec compliant AO tokens that manage records and permission for ArNS names. Because the ANT spec is compliant with the Atomic Asset Spec, they are tradable on Bazar, which is a decentralized market place for Atomic Assets on AO. There are a few simple steps that are required in order to make an ANT available on Bazar to be traded.Bazar relies on profiles for displaying user information and tradable assets. Profiles are AO processes that contain user specified information like a name, a nickname, and images associated with the profile. Profiles also track assets held by the profile in order to provide their information to bazar.Create a Profile If you do not already have a profile associated with your wallet, you can easily create one on using the "Create your profile" button on bazar after connecting your wallet: You will be prompted to add, at a minimum, a name and handle (nickname) to associate with the profile. These values can be changed later. Click "Save" at the bottom to finish creation of your profile.Once your profile is created, you can get its ao process Id at any time by clicking on the user icon in Bazar, and then the "Copy profile address" button from the menu. Bazar profiles only track assets that are held in the profile process, not in a user wallet. In order for an ANT to be displayed and transferred on Bazar, it must first be transferred into the Bazar profile. This can be done easily using arns.app    in your manage page for a given name.    Once an ANT is transferred into the profile process, it will automatically be detected and displayed by Bazar. It can be transferred or sold just like any other atomic asset on the marketplace, with no additional steps required.Restore Controllers Optional This is an optional step that will enable updating an ANT's Target Id without transferring it back into your wallet. This step may be safely skipped without affecting the ANT's functionality or tradability on Bazar.Transferring an ANT to a new wallet or AO process resets all authorized controllers, or non-owner entities that are allowed to update some settings on the ArNS name. It does not reset the Target Id that the ArNS name is pointing to. If you want to be able to update the Target ID and undernames from your wallet using arns.app, you will need to set your wallet address as a controller for the ANT while it is in your profile. The easiest way to do this is using aos.If you have not used aos before, you can find installation instructions here    Using aos, you can log directly into your profile process with the command:Be sure to replace <profile-address> with the process Id for your profile process, and /path/to/your/keyfile with the path to the keyfile for the wallet you created the profile with.Once you are logged in with aos, you can send a message to the ANT in your profile to set your wallet as a controller:Replace <Ant-Process-ID> with the process Id of the ANT you transferred into your profile, and <Wallet-Address> with your wallet address.

---

# 129. Uploading to Arweave - ARIO Docs

Document Number: 129
Source: https://docs.ar.io/guides/uploading-to-arweave
Words: 341
Extraction Method: html

Uploading to Arweave Overview While AR.IO provides powerful tools for accessing and interacting with data on Arweave, that data must first be uploaded to the network. This guide will walk you through the process of uploading data to Arweave using the Turbo SDK, which provides a streamlined experience for data uploads.Installing Turbo SDK Authentication Node.js Environment Browser Environment Purchasing Turbo Credits Turbo Credits are the payment medium used by the Turbo Upload Service. Each Credit represents a 1:1 conversion from the upload power of the Arweave native token (AR). Turbo Credits can be purchased with fiat currency via the Turbo Top Up App, or with supported cryptocurrencies via the Turbo SDK. Learn more about Turbo Credits and available methods for purchasing them here.Node.js Environment Browser Environment In a browser environment, the topUpWithTokens method is not available. Instead, you'll need to manually send tokens to the Turbo wallet address and then submit the transaction for processing. Here are detailed examples for each supported chain:Browser Top-Up Examples Note: The wait times for chain settlement are approximate and may need adjustment based on network conditions:Ethereum: ~15 minutes Solana: ~400-600 milliseconds Arweave: ~30-36 minutes Polygon: ~2-3 seconds Base: ~2-5 seconds KYVE: ~5 minutes Once you have purchased Turbo credits, you can upload files and folders to Arweave. The process is the same regardless of which token type you used for authentication, but differs between Node.js and browser environments.Node.js Environment Node.js Upload Examples Browser Environment Browser Upload Examples Important Notes:For single file uploads, always include a Content-Type tag to ensure proper file viewing The fileStreamFactory must return a NEW stream each time it's called Folder uploads automatically detect and set Content-Type tags for all files You can specify additional tags in dataItemOpts for both file and folder uploads The maxConcurrentUploads option controls how many files are uploaded simultaneously Use throwOnFailure: true to ensure all files are uploaded successfully Complete Examples Here are complete examples showing how to authenticate, check balances, and handle lazy funding for uploads. These examples demonstrate the full workflow from start to finish.

---

# 130. The ARIO Token - ARIO Docs

Document Number: 130
Source: https://docs.ar.io/token
Words: 251
Extraction Method: html

Overview ARIO is the multifunction AO Computer based token that powers the AR.IO Network and its suite of permanent cloud applications. The ARIO Token uses include:Gateway Participation: Gateway operators must stake ARIO tokens to join and actively participate in the network.Eligibility for Protocol Rewards: Both individuals who stake tokens as gateway operators and those who delegate tokens to a gateway are positioned to receive protocol rewards.ArNS Name Purchases: Acquiring friendly names through the Arweave Name System (ArNS) requires ARIO tokens. These transactions directly contribute to the protocol, with the proceeds being redistributed through the Observation and Incentive Protocol.Universal Currency: Within the AR.IO ecosystem, ARIO tokens serve as a versatile currency, enabling network participants to make purchases and exchange value.Moreover, ARIO tokens play a crucial role in driving ecosystem growth, fueling incentive programs, investments, bounties, and grants designed for active participants.Adding ARIO Token to Wander To view your ARIO token balance in Wander, formerly ArConnect, follow these steps to add the token to your wallet:Open your Wander wallet (available on both desktop and mobile) Access Settings:Mobile: Click the 3 vertical dots in the top right, then select "Settings" Desktop: Click the hamburger menu icon in the top left Select "Tokens" Click "Import Token" For Desktop users: Ensure "Token Type" is set to "ao Token" Enter the AO process ID:qNvAoz0TgcH7DMg8BCVn8jF32QH5L6T29VjHxhHqqGE The token ticker "ARIO" and name "AR.IO Network" will appear automatically Click "Import Asset" to complete the process Once imported, you'll be able to view your total ARIO balance in your Wander wallet.

---

# 131. ARIO Docs

Document Number: 131
Source: https://docs.ar.io/wayfinder
Words: 351
Extraction Method: html

Wayfinder Wayfinder is a client-side routing and verification protocol that provides decentralized, cryptographically verified access to data stored on Arweave via the AR.IO Network. It automatically selects optimal gateways and ensures data integrity for seamless permaweb experiences.What is Wayfinder?Wayfinder solves the challenge of reliable data access on the permaweb by:Intelligent Routing: Automatically selects the best gateway for each request based on performance, availability, and user preferences Data Verification: Cryptographically verifies data integrity to ensure you're getting authentic, unmodified content Decentralized Access: Eliminates single points of failure by distributing requests across multiple AR.IO gateways Seamless Integration: Works behind the scenes to provide fast, reliable access without requiring users to understand the underlying infrastructure Who is Wayfinder For?Builders People who build dApps on the permaweb Wayfinder enables developers to build robust decentralized applications with:Reliable Data Access: Never worry about gateway downtime or slow responses Built-in Verification: Ensure data integrity without implementing complex verification logic Developer-Friendly APIs: Simple JavaScript/TypeScript libraries and React components Performance Monitoring: Built-in telemetry to track and optimize application performance Flexible Configuration: Choose routing strategies and verification methods that fit your use case Browsers People who browse the permaweb Wayfinder provides end users with:Fast Loading: Automatically routes to the fastest available gateway for optimal performance Reliable Access: Seamlessly switches between gateways if one becomes unavailable Data Integrity: Verifies that content hasn't been tampered with or corrupted Transparent Operation: Works invisibly in the background without requiring user interaction No Tokens Required: Access permaweb content without needing AR tokens or wallet connections Operators People who operate AR.IO gateways Wayfinder helps gateway operators by:Performance Insights: Provides telemetry data to help optimize gateway performance Network Participation: Enables gateways to participate in the decentralized routing ecosystem Load Distribution: Intelligently distributes traffic based on gateway capabilities and performance Quality Monitoring: Tracks gateway reliability and performance metrics Network Health: Contributes to overall AR.IO network resilience and performance Available Packages @ar.io/wayfinder-core: Core JavaScript/TypeScript library for any web application @ar.io/wayfinder-react: React components, hooks, and providers for React applications Getting Started Ready to integrate Wayfinder into your project? Check out our Getting Started Guide for installation instructions and basic configuration examples.

---

# 132. ARIO Docs

Document Number: 132
Source: https://docs.ar.io/wayfinder/core/gateway-providers/local-storage
Words: 142
Extraction Method: html

LocalStorageGatewaysProvider The LocalStorageGatewaysProvider is a gateway provider that caches gateway lists in the browser's localStorage. This allows gateway data to persist across page reloads and browser sessions, making it ideal for web applications that require fast access to gateway information without repeated network requests. The provider automatically manages cache expiration based on a configurable TTL (time-to-live), ensuring that gateway data remains fresh while minimizing network usage. Use this provider when you want persistent, client-side caching of gateway lists in browser environments.Note: If you are building a React-based application, consider using @ar.io/wayfinder-react for seamless integration with React components, hooks, and context providers. This package is designed to work hand-in-hand with gateway providers like LocalStorageGatewaysProvider for optimal developer experience.Basic Usage Configuration Options Related Documentation Gateway Providers Overview: Compare all gateway providers NetworkGatewaysProvider: Dynamic network discovery StaticGatewaysProvider: Static gateway configuration Wayfinder Configuration: Main wayfinder setup

---

# 133. ARIO Docs

Document Number: 133
Source: https://docs.ar.io/wayfinder/core/telemetry
Words: 178
Extraction Method: html

Telemetry Configuration Wayfinder includes optional telemetry support built on the OpenTelemetry standard. Telemetry is completely opt-in and disabled by default.Overview Wayfinder's telemetry system:100% Opt-in: Disabled by default, only enabled when you explicitly configure it OpenTelemetry Standard: Built on the industry-standard OpenTelemetry framework Your Choice of Destination: Send data to your own servers Configurable: Full control over what data is collected and where it goes All telemetry is disabled by default. It only activates when you
explicitly enable it in your configuration.Basic Configuration Minimal Setup Send to Your Own Infrastructure Configuration Options Client Identification You can identify your client application and version in telemetry data:Frequently Asked Questions Q: Is telemetry enabled by default?A: No, telemetry is completely disabled by default and only activates when you explicitly configure it.Q: Can I use my own telemetry backend?A: Yes, specify your own exporterUrl to send data to any OpenTelemetry-compatible backend.Q: Does this affect performance?A: Minimal impact when using appropriate sampling rates (1-10% for production).Q: Can I disable telemetry after enabling it?A: Yes, simply set enabled: false or remove the telemetrySettings configuration entirely.

---

# 134. Signature Verification Strategy - ARIO Docs

Document Number: 134
Source: https://docs.ar.io/wayfinder/core/verification-strategies/signature-verification
Words: 148
Extraction Method: html

SignatureVerificationStrategy Overview The SignatureVerificationStrategy validates Arweave transaction signatures to ensure data authenticity and ownership. This strategy provides cryptographic proof that the data was created by the claimed wallet address and hasn't been tampered with since signing.Important SignatureVerificationStrategy requires that the trusted gateway has the relevant
transaction data indexed locally. Gateways cannot proxy out verification
requests to other sources, as this would compromise the security and
reliability of the verification process. If a gateway doesn't have the
required data indexed, verification will fail.How It Works Fetch Metadata: Retrieve transaction metadata from trusted gateways Reconstruct Signature Data: Build the signature data using the received content Verify Signature: Validate the signature matches the claimed owner's public key Check Ownership: Confirm the transaction was signed by the claimed wallet Result: Pass or fail based on signature validation Basic Usage Related Hash Verification: Learn about fast integrity checking Signature Verification: Understand authenticity validation

---

# 135. Privacy  Cooking with the Permaweb

Document Number: 135
Source: https://cookbook.arweave.net/concepts/arfs/privacy.html
Words: 700
Extraction Method: html

Privacy The Arweave blockweave is inherently public. But with apps that use ArFS, like ArDrive, your private data never leaves your computer without using military grade (and quantum resistant) encryption. This privacy layer is applied at the Drive level, and users determine whether a Drive is public or private when they first create it. Private drives must follow the ArFS privacy model.Every file within a Private Drive is symmetrically encrypted using AES-256-GCM. Every Private drive has a master "Drive Key" which uses a combination of the user's Arweave wallet signature, a user defined drive password, and a unique drive identifier (uuidv4). Each file has its own "File Key" derived from the "Drive Key". This allows for single files to be shared without exposing access to the other files within the Drive.Once a file is encrypted and stored on Arweave, it is locked forever and can only be decrypted using its file key.Deriving Keys Private drives have a global drive key, D, and multiple file keys F, for encryption. This enables a drive to have as many uniquely encrypted files as needed. One key is used for all versions of a single file (since new file versions use the same File-Id) D is used for encrypting both Drive and Folder metadata, while F is used for encrypting File metadata and the actual stored data. Having these different keys, D and F, allows a user to share specific files without revealing the contents of their entire drive.D is derived using HKDF-SHA256 with an unsalted RSA-PSS signature of the drive's id and a user provided password.F is also derived using HKDF-SHA256 with the drive key and the file's id. Other wallets (like Wander) integrate with this Key Derivation protocol just exposing an API to collect a signature from a given Arweave Wallet in order to get the SHA-256 signature needed for the HKDF to derive the Drive Key.An example implementation, using Dart, is available here, with a Typescript implementation here.Private Drives Drives can store either public or private data. This is indicated by the Drive-Privacy tag in the Drive entity metadata.If a Drive entity is private, an additional tag Drive-Auth-Mode must also be used to indicate how the Drive Key is derived. ArDrive clients currently leverage a secure password along with the Arweave Wallet private key signature to derive the global Drive Key.Drive-Auth-Mode?: 'password' On every encrypted Drive Entity, a Cipher tag must be specified, along with the public parameters for decrypting the data. This is done by specifying the parameter with a Cipher-* tag. eg. Cipher-IV. If the parameter is byte data, it must be encoded as Base64 in the tag.ArDrive clients currently leverage AES256-GCM for all symmetric encryption, which requires a Cipher Initialization Vector consisting of 12 random bytes.Cipher?: "AES256-GCM"
Cipher-IV?: "<12 byte initialization vector as Base64>" Additionally, all encrypted transactions must have the Content-Type tag application/octet-stream as opposed to application/json Private Drive Entities and their corresponding Root Folder Entities will both use these keys and ciphers generated to symmetrically encrypt the JSON files that are included in the transaction. This ensures that only the Drive Owner (and whomever the keys have been shared with) can open the drive, discover the root folder, and continue to load the rest of the children in the drive.Private Files When a file is uploaded to a private drive, it by default also becomes private and leverages the same drive keys used for its parent drive. Each unique file in a drive will get its own set of file keys based off of that file's unique FileId. If a single file gets a new version, its File-Id will be reused, effectively leveraging the same File Key for all versions in that file's history.These file keys can be shared by the drive's owner as needed.Private File entities have both its metadata and data transactions encrypted using the same File Key, ensuring all facets of the data is truly private. As such, both the file's metadata and data transactions must both have a unique Cipher-IV and Cipher tag:Cipher?: "AES256-GCM"
Cipher-IV?: "<12 byte initialization vector as Base64>" Just like drives, private files must have the Content-Type tag set as application/octet-stream in both its metadata and data transactions:Content-Type: "application/octet-stream"

---

# 136. ArFS Protocol A Decentralized File System on Arweave  Cooking with the Permaweb

Document Number: 136
Source: https://cookbook.arweave.net/concepts/arfs/arfs.html
Words: 404
Extraction Method: html

ArFS Protocol: A Decentralized File System on Arweave Arweave File System, or “ArFS” is a data modeling, storage, and retrieval protocol designed to emulate common file system operations and to provide aspects of mutability to your data hierarchy on Arweave 's otherwise permanent, immutable data storage blockweave.Due to Arweave's permanent, immutable and public nature traditional file system operations such as permissions, file/folder renaming and moving, and file updates cannot be done by simply updating the on-chain data model.ArFS works around this by implementing a privacy and encryption pattern and defining an append-only transaction data model using tags within Arweave Transaction headers.Key Features File Structure ArFS organizes files and folders using a hierarchical structure. Files are stored as individual transactions on the Arweave blockchain, while folders are metadata that reference these file transactions.Each file and folder has associated metadata, such as the name, type, size, and modification timestamp. ArFS leverages Arweave's tagging system to store this metadata in a standardized format, which allows for easy querying and organization.File Permissions ArFS supports public and private file permissions. Public files can be accessed by anyone on the network, while private files are encrypted using the owner's private key, ensuring only they can decrypt and access the content.File Versioning ArFS supports versioning of files, allowing users to store multiple versions of a file and access previous versions at any time. This is achieved by linking new file transactions to previous versions through the use of metadata tags.Data Deduplication To minimize storage redundancy and costs, ArFS employs data deduplication techniques. If a user tries to store a file that already exists on the network, the protocol will simply create a new reference to the existing file instead of storing a duplicate copy.Search and Discovery ArFS enables users to search and discover files based on their metadata, such as file names, types, and tags. This is made possible by indexing the metadata stored within the Arweave blockchain.Interoperability ArFS is designed to be interoperable with other decentralized applications and services built on the Arweave network. This allows for seamless integration and collaboration between different applications and users.Getting Started To start using ArFS, you'll need to familiarize yourself with the Arweave ecosystem, acquire AR tokens to cover storage costs, and choose a compatible client or library to interact with the ArFS protocol.Resources For more information, documentation, and community support, refer to the following resources:Arweave Official Website Arweave Developer Documentation Arweave Community Forums

---

# 137. Entity Types  Cooking with the Permaweb

Document Number: 137
Source: https://cookbook.arweave.net/concepts/arfs/entity-types.html
Words: 1325
Extraction Method: html

Entity Types Overview Arweave transactions are composed of transaction headers and data payloads.ArFS entities, therefore, have their data split between being stored as tags on their transaction header and encoded as JSON and stored as the data of a transaction. In the case of private entities, JSON data and file data payloads are always encrypted according to the protocol processes defined below.Drive entities require a single metadata transaction, with standard Drive tags and encoded JSON with secondary metadata.Folder entities require a single metadata transaction, with standard Folder tags and an encoded JSON with secondary metadata.File entities require a metadata transaction, with standard File tags and an encoded Data JSON with secondary metadata relating to the file.File entities also require a second data transaction, which includes a limited set of File tags and the actual file data itself.Snapshot entities require a single transaction. which contains a Data JSON with all of the Drive’s rolled up ArFS metadata and standard Snapshot GQL tags that identify the Snapshot.Drive A drive is the highest level logical grouping of folders and files. All folders and files must be part of a drive, and reference the Drive ID of that drive.When creating a Drive, a corresponding folder must be created as well. This will act as the root folder of the drive. This separation of drive and folder entity enables features such as folder view queries, renaming, and linking.Drive Entity Transaction Example Folder A folder is a logical grouping of other folders and files. Folder entity metadata transactions without a parent folder id are considered the Drive Root Folder of their corresponding Drives. All other Folder entities must have a parent folder id. Since folders do not have underlying data, there is no Folder data transaction required.ArFS: "0.13"
Cipher?: "AES256-GCM"
Cipher-IV?: "<12 byte initialization vector as Base64>"
Content-Type: "<application/json | application/octet-stream>"
Drive-Id: "<drive uuid>"
Entity-Type: "folder"
Folder-Id: "<uuid>"
Parent-Folder-Id?: "<parent folder uuid>"
Unix-Time: "<seconds since unix epoch>"

Data JSON {
    "name": "<user defined folder name>"
} Folder Entity Transaction Example File A File contains uploaded data, like a photo, document, or movie.In the Arweave File System, a single file is broken into 2 parts - its metadata and its data.In the Arweave File System, a single file is broken into 2 parts - its metadata and its data.A File entity metadata transaction does not include the actual File data. Instead, the File data must be uploaded as a separate transaction, called the File Data Transaction. The File JSON metadata transaction contains a reference to the File Data Transaction ID so that it can retrieve the actual data. This separation allows for file metadata to be updated without requiring the file itself to be reuploaded. It also ensures that private files can have their JSON Metadata Transaction encrypted as well, ensuring that no one without authorization can see either the file or its metadata.ArFS: "0.13"
Cipher?: "AES256-GCM"
Cipher-IV?: "<12 byte initialization vector as Base64>"
Content-Type: "<application/json | application/octet-stream>"
Drive-Id: "<drive uuid>"
Entity-Type: "file"
File-Id: "<uuid>"
Parent-Folder-Id: "<parent folder uuid>"
Unix-Time: "<seconds since unix epoch>"

Data JSON {
    "name": "<user defined file name with extension eg. happyBirthday.jpg>",
    "size": "<computed file size - int>",
    "lastModifiedDate": "<timestamp for OS reported time of file's last modified date represented as milliseconds since unix epoch - int>",
    "dataTxId": "<transaction id of stored data>",
    "dataContentType": "<the mime type of the data associated with this file entity>",
    "pinnedDataOwner": "<the address of the original owner of the data where the file is pointing to>" # Optional
}  Pin Files  Since the version v0.13, ArFS suports Pins. Pins are files whose data may be any transaction uploaded to Arweave, that may or may not be owned by the wallet that created the pin.When a new File Pin is created, the only created transaction is the Metadata Transaction. The dataTxId field will point it to any transaction in Arweave, and the optional pinnedDataOwner field is gonna hold the address of the wallet that owns the original copy of the data transaction.File Data Transaction Example The File Data Transaction contains limited information about the file, such as the information required to decrypt it, or the Content-Type (mime-type) needed to view in the browser.Cipher?: "AES256-GCM"
Cipher-IV?: "<12 byte initialization vector as Base64>"
Content-Type: "<file mime-type | application/octet-stream>"
 { File Data - Encrypted if private } File Metadata Transaction Example The the File Metadata Transaction contains the GQL Tags necessary to identify the file within a drive and folder.Its data contains the JSON metadata for the file. This includes the file name, size, last modified date, data transaction id, and data content type.ArFS: "0.13"
Cipher?: "AES256-GCM"
Cipher-IV?: "<12 byte initialization vector as Base64>"
Content-Type: "<application/json | application/octet-stream>"
Drive-Id: "<drive uuid>"
Entity-Type: "file"
File-Id: "<uuid>"
Parent-Folder-Id: "<parent folder uuid>"
Unix-Time: "<seconds since unix epoch>"
 { File JSON Metadata - Encrypted if private } Snapshot ArFS applications generate the latest state of a drive by querying for all ArFS transactions made relating to a user's particular Drive-Id. This includes both paged queries for indexed ArFS data via GQL, as well as the ArFS JSON metadata entries for each ArFS transaction.For small drives (less than 1000 files), a few thousand requests for very small volumes of data can be achieved relatively quickly and reliably. For larger drives, however, this results in long sync times to pull every piece of ArFS metadata when the local database cache is empty. This can also potentially trigger rate-limiting related ArWeave Gateway delays.Once a drive state has been completely, and accurately generated, in can be rolled up into a single snapshot and uploaded as an Arweave transaction. ArFS clients can use GQL to find and retrieve this snapshot in order to rapidly reconstitute the total state of the drive, or a large portion of it. They can then query individual transactions performed after the snapshot.This optional method offers convenience and resource efficiency when building the drive state, at the cost of paying for uploading the snapshot data. Using this method means a client will only have to iterate through a few snapshots instead of every transaction performed on the drive.Snapshot Entity Tags Snapshot entities require the following tags. These are queried by ArFS clients to find drive snapshots, organize them together with any other transactions not included within them, and build the latest state of the drive.ArFS: "0.13"
Drive-Id: "<drive uuid that this snapshot is associated with>"
Entity-Type: "snapshot"
Snapshot-Id: "<uuid of this snapshot entity>"
Content-Type: "<application/json>"
Block-Start: "<the minimum block height from which transactions were searched for in this snapshot, eg. 0>"
Block-End: "<the maximum block height from which transactions were searched for in this snapshot, eg 1007568>"
Data-Start: "<the first block in which transaction data was found in this snapshot, eg 854300"
Data-End: "<the last block in which transaction was found in this snapshot, eg 1001671"
Unix-Time: "<seconds since unix epoch>" Snapshot Transaction GQL tags example Snapshot Entity Data A JSON data object must also be uploaded with every ArFS Snapshot entity. THis data contains all ArFS Drive, Folder, and File metadata changes within the associated drive, as well as any previous Snapshots. The Snapshot Data contains an array txSnapshots. Each item includes both the GQL and ArFS metadata details of each transaction made for the associated drive, within the snapshot's start and end period.A tsSnapshot contains a gqlNode object which uses the same GQL tags interface returned by the Arweave Gateway. It includes all of the important block, owner, tags, and bundledIn information needed by ArFS clients. It also contains a dataJson object which stores the correlated Data JSON for that ArFS entity.For private drives, the dataJson object contains the JSON-string-escaped encrypted text of the associated file or folder. This encrypted text uses the file's existing Cipher and Cipher-IV. This ensures clients can decrypt this information quickly using the existing ArFS privacy protocols.Snapshot Transaction JSON data example Schema Diagrams The following diagrams show complete examples of Drive, Folder, and File entity Schemas.Public Drive  Public Drive Schema Private Drive  Private Drive Schema

---

# 138. Data Model  Cooking with the Permaweb

Document Number: 138
Source: https://cookbook.arweave.net/concepts/arfs/data-model.html
Words: 344
Extraction Method: html

Data Model Because of Arweave's permanent and immutable nature, traditional file structure operations such as renaming and moving files or folders cannot be accomplished by simply updating on-chain data. ArFS works around this by defining an append-only transaction data model based on the metadata tags found in the Arweave Transaction Headers.This model uses a bottom-up reference method, which avoids race conditions in file system updates. Each file contains metadata that refers to the parent folder, and each folder contains metadata that refers to its parent drive. A top-down data model would require the parent model (i.e. a folder) to store references to its children.These defined entities allow the state of the drive to be constructed by a client to look and feel like a file system Drive Entities contain folders and files Folder Entities contain other folders or files File Entities contain both the file data and metadata Snapshot entities contain a state rollups of all files and folder metadata within a drive Entity relationships The following diagram shows the high level relationships between drive, folder, and file entities, and their associated data. More detailed information about each Entity Type can be found here. Entity Relationship Diagram As you can see, each file and folder contains metadata which points to both the parent folder and the parent drive. The drive entity contains metadata about itself, but not the child contents. So clients must build drive states from the lowest level and work their way up.Metadata stored in any Arweave transaction tag will be defined in the following manner:{ "name": "Example-Tag", "value": "example-data" } Metadata stored in the Transaction Data Payload will follow JSON formatting like below:{
    "exampleField": "exampleData"
} fields with a ? suffix are optional.{
  "name": "My Project",
  "description": "This is a sample project.",
  "version?": "1.0.0",
  "author?": "John Doe"
} Enumerated field values (those which must adhere to certain values) are defined in the format "value 1 | value 2".All UUIDs used for Entity-Ids are based on the Universally Unique Identifier standard.There are no requirements to list ArFS tags in any specific order.

---

# 139. Content Types  Cooking with the Permaweb

Document Number: 139
Source: https://cookbook.arweave.net/concepts/arfs/content-types.html
Words: 197
Extraction Method: html

Content Types All transaction types in ArFS leverage a specific metadata tag for the Content-Type (also known as mime-type) of the data that is included in the transaction. ArFS clients must determine what the mime-type of the data is, in order for Arweave gateways and browswers to render this content appropriately.All public drive, folder, and file (metadata only) entity transactions all use a JSON standard, therefore they must have the following content type tag:Content-Type: '<application/json>' However, a file's data transaction must have its mime-type determined. This is stored in the file's corresponding metadata transaction JSON's dataContentType as well as the content type tag in the data transaction itself.Content-Type: "<file's mime-type>" All private drive, folder, and file entity transactions must have the following content type, since they are encrypted:Content-Type: '<application/octet-stream>' ArDrive-Core open in new window includes methods to determine a file's content type.Other Tags ArFS enabled clients should include the following tags on their transactions to identify their application App-Name: "<defined application name eg. ArDrive"
App-Version: "<defined version of the app eg. 0.5.0"
Client?: "<if the application has multiple clients, they should be specified here eg. Web" Built with  ❤️  by the Arweave community. Learn more at  Arweave.org

---

# 140. ArNS - Arweave Name System  Cooking with the Permaweb

Document Number: 140
Source: https://cookbook.arweave.net/concepts/arns.html
Words: 523
Extraction Method: html

ArNS - Arweave Name System Overview The Arweave Name System (ArNS) is the phonebook of the Permaweb.It is a decentralized and censorship-resistant naming system that is enabled by AR.IO Gateways and used to connect friendly names to Permaweb apps, pages and data.This system works similarly to traditional DNS, where a user can purchase a name in a registry and DNS Name servers resolve these names to IP addresses.With ArNS, the registry is decentralized, permanent and stored on Arweave (via AO), and each AR.IO gateway acts as both cache and name resolver. Users can register a name within the ArNS Registry, like "my-name" and set a pointer to any Arweave Transaction ID.AR.IO Gateways will resolve that name as one of their own subdomains, eg. https://laserilla.arweave.net and proxy all requests to the associated Arweave Transaction ID. Each registered name can also have under names associated with it that each point to an Arweave Transaction ID, like https://v1_laserilla.arweave.net, giving even more flexibility and control to its owner.The ArNS Registry ArNS uses AO to manage its name records. Each record, or name, is leased by a user or bought permanently and tied to an ANT token. You can register multiple ArNS names to a single ANT, but you cannot register multiple ANTs to a single ArNS name - the gateways wouldn't know where to point the routing ID.ArNS names can be up to 32 characters, including numbers [0-9], letters [a-z], and dashes [-]. The dashes cannot be trailing dashes, e.g. -myname.ANTs (Arweave Name Tokens) ANTs are a crucial part of the ArNS ecosystem - they are the actual key to owning an ArNS name. When you register an ArNS name to an ANT, the ANT then becomes the transfer method for that name. The ArNS registry does not care who owns the ANT, it simply knows what name ANT it belongs to.Within ANTs you can build out whatever functionality you wish, within the scope ArNS registry approved source code transaction list.Under_Names Undernames are records held and managed by your ANT (Arweave Name Token). These records can be created and managed without even owning an ARNS name, and will be transferred along with the ant when sent to a new owner. Likewise if your ArNS name expires, and you register your ANT to a new ArNS name, all your undername will remain intact.Example: you own oldName.arweave.net.then: You create the undername "my" - my_oldName.arweave.net.then: oldName.arweave.net expires, and you register newName.arweave.net to your ANT.now: my_ undername is accessable on newName - my_newName.arweave.net.Below is an example of an ANT contract State:{
  balances:{ QGWqtJdLLgm2ehFWiiPzMaoFLD50CnGuzZIPEdoDRGQ : 1 },
  controller: "QGWqtJdLLgm2ehFWiiPzMaoFLD50CnGuzZIPEdoDRGQ",
  evolve: null,
  name: "ArDrive OG Logo",
  owner: "QGWqtJdLLgm2ehFWiiPzMaoFLD50CnGuzZIPEdoDRGQ",
  records:{
    @:{ transactionId: "xWQ7UmbP0ZHDY7OLCxJsuPCN3wSUk0jCTJvOG1etCRo" },
    undername1:{ transactionId: "usOLUmbP0ZHDY7OLCxJsuPCN3wSUk0jkdlvOG1etCRo" }
  },
  ticker:"ANT-ARDRIVE-OG-LOGO"
} the base "@" record is the initial routing id for the ANT. if you registered 'my-name' to this ANT, and tried to access it via my-name.arweave.net, you would be redirected to the @ record's transactionId.if you tried to access undername1_my-name.arweave.net, you would get 'undername1's transactionId.ANT's, in theory, have an UNLIMITED number of undernames. However, how many will be served depends on which tier is used with your ArNS name.Resources ArNS App ArNS Docs

---

# 141. Wallets and Keys  Cooking with the Permaweb

Document Number: 141
Source: https://cookbook.arweave.net/concepts/keyfiles-and-wallets.html
Words: 530
Extraction Method: html

Wallets and Keys  Arweave Wallets On Arweave a wallet secures a unique address on the blockchain. The address is used to keep track of your $AR balance, and interact with the Arweave network - such as sending transactions, or interacting with AO Processes.Like most blockchains, the concept of a wallet on Arweave is slightly misleading.A Wallet does not "hold" any tokens itself; token balances are stored on the blockchain and linked to the wallets address. Instead a wallet holds the cryptographic public-private key pair that can be used to sign transactions to post data or transfer tokens. The wallet owner (the person with access to the wallet's private key) is the only one who can sign transactions for the address and access its funds.Keypair and Wallet Format Arweave uses 4096bit RSA-PSS key-pairs stored using the JWK (JSON Web Keys) format. The JWK format can be used to store many types of cryptographic keys, not just RSA key-pairs.Shown here is the contents of a JWK file which describes an RSA-PSS key-pair. The values are abbreviated so they are not accidentally used as the sender or recipient of an on-chain transaction. When storing RSA-PSS key-pairs the value associated with n in the JWK is your wallets public key and can be shared safely without compromising the security of your wallet.{
    "d": "cgeeu66FlfX9wVgZr5AXKlw4MxTlxSuSwMtTR7mqcnoE...",
    "dp": "DezP9yvB13s9edjhYz6Dl...",
    "dq": "SzAT5DbV7eYOZbBkkh20D...",
    "e": "AQAB",
    "ext": true,
    "kty": "RSA",
    "n": "o4FU6y61V1cBLChYgF9O37S4ftUy4newYWLApz4CXlK8...",
    "p": "5ht9nFGnpfW76CPW9IEFlw...",
    "q": "tedJwzjrsrvk7o1-KELQxw...",
    "qi": "zhL9fXSPljaVZ0WYhFGPU..."
} Your private key is also stored in the JWK, primarily under the value associated with d but it is also partially derived from some of the other values in the JWK. The private key is like the password for your wallet - which can be used to create digital signatures (such as for signing transactions), or decrypting data.These JWKs are actual json files created and exported from a wallet app such as Arweave.app or generated through code using arweave-js.When using a wallet app to generate your key-pair your private key can also be represented as a mnemonic seed phrase, which in some cases can be used as an alternative to sign transactions and/or recover your wallet.Wallet Safety Your private key must be kept confidential at all times as it has the ability to transfer tokens from your address to someone elses. As a developer, make sure not to include your keyfile in any public GitHub repositories or host it anywhere else publicly.Wallet Addresses Interestingly the address of your wallet is derived from its public key. While it's safe to share your public key with others, a 4096bit public key is a bit large to pass around conveniently. To reduce that overhead and keep wallet addresses a little more human readable, the SHA-256 hash of the public key is Base64URL encoded and used as the wallet address. This security and deterministically links a unique 43 character wallet address to the wallets public-key and provides a convenient shorthand that anyone with the public-key can verify.Wallets Arweave.app - Arweave web wallet to deploy permanent data, connect your accounts securely to decentralized applications, and navigate the weave.Wander - Browser extension and mobile wallet for Arweave and AO Arweave Docs JSON Web Key Format (RFC 7517)

---

# 142. Permaweb Applications  Cooking with the Permaweb

Document Number: 142
Source: https://cookbook.arweave.net/concepts/permawebApplications.html
Words: 434
Extraction Method: html

Permaweb Applications A Permaweb application refers to a web page or app built on top of Arweave. Applications built on Arweave have the properties of immutability and long-term availability, which can go for not only data, but also backend processes (smart contracts) and the frontend of websites as well.What is the permaweb?INFORMATION For a deeper dive into the permaweb check out this article on The Permaweb The permaweb is a collection of sites, apps, and SmartContracts built on top of the Arweave's Permaweb Services.The core parts of the Permaweb are the following:Gateways Bundlers Compute Networks (AO) Indexers Gateway Services Gateways are often referred to as the "front door" to the Permaweb.Gateway services are the bridge between data on Arweave and displaying data in the browser. They serve transaction data, expose GraphQL endpoints for querying Arweave, and often provide indexing and caching services alongside their gateway responsibilities.AR.IO is one of the largest gateway networks in the ecosystem, and provide education and open source software for anyone to spin up their own gateway node, as well as running gateways of their own.Bundling Services Bundling services aggregate transactions into transaction bundles and make sure those bundles are posted directly to Arweave. By using a bundling service like ArDrive Turbo you can post hundreds of thousands of transactions in a single Arweave block.Compute Services AO Computer is a decentralized compute network built on top of Arweave to provide the ability to create general-purpose smart contracts (Processes).Every interaction with a process on AO is stored as an Arweave transaction.AO is built for large-scale parallel computation, and includes integrations to use Arweave data in Processes on AO.Indexing Services Indexing services listen to all the transactions on Arweave and import them into an indexed database suitable for fast querying. They then expose a GraphQL endpoint so Permaweb apps can make optimized queries for Arweave data.These services work together to form the Permaweb Services Layer and gives developers the power to build fully decentralized applications on the Permaweb.Application Development Approaching application development with the Permaweb is similar to Single Page Application development.The application consists of frontend functionality that is executed in a web browser, and uses GraphQL (Read/Query), Arweave/ArDrive Turbo (Write), and AO (decentralized computation) to make up the business logic and persistence layer of the application.By leveraging modern web application frameworks and the Path Manifest specification, developers can deploy web sites and applications to the permaweb.To learn more about creating and deploying Permaweb Apps, check out our starter kits in your favorite framework:React Svelte Vue Missing my framework?Can't find your framework, why don't you contribute? How to contribute to the cookbook

---

# 143. Path Manifests  Cooking with the Permaweb

Document Number: 143
Source: https://cookbook.arweave.net/concepts/manifests.html
Words: 370
Extraction Method: html

Path Manifests Overview When uploading files to Arweave each file is assigned its own unique transaction ID. By default these ID's aren't grouped or organized in any particular manner.One picture of your cat might be stored with a transaction ID of bVLEkL1SOPFCzIYi8T_QNnh17VlDp4RylU6YTwCMVRw, while another with FguFk5eSth0wO8SKfziYshkSxeIYe7oK9zoPN2PhSc0 as its transaction ID.Cat1 Cat2   bVLEkL1SOPFCzIYi8T_QNnh17VlDp4...FguFk5eSth0wO8SKfziYshkSxeIYe7oK9zoPN2PhSc0 These transaction ID's are a bit unwieldy and make it difficult to find all of your relevant files. Without a path manifest, if you uploaded 100 pictures of your cat you would need to keep track of 100 different IDs and links!Path Manifests are a way to link multiple transactions together under a single base transaction ID and give them human readable file names. In relation to the cat example, you could have one base transaction ID to remember and use it like a folder - accessing your cat pictures with more memorable filenames like {base id}/cat1.jpg, {base id}/cat2.jpg, etc.Creating grouped sets of readable file names is essential for creating practical applications on Arweave, and unlocks the ability to host websites or other file collections as explored in the examples below.What Can You Use Manifests For? Any time you need to group files in a hierarchical way, manifests can be useful. For example:Storing NFT collections:https://arweave.net/X8Qm…AOhA/0.png https://arweave.net/X8Qm…AOhA/1.png This mirrors the common base path approach used by NFT collections when linking to NFT images and metadata on a storage API or IPFS.Hosting websites:https://arweave.net/X8Qm…AOhA/index.html https://arweave.net/X8Qm…AOhA/styles.css https://arweave.net/X8Qm…AOhA/public/favicon.png Manifest Structure  Path Manifests are a special format of transaction created and posted to Arweave using the Tags:{ name: "Content-type", value: "application/x.arweave-manifest+json" } and having JSON formatted transaction data that matches the example below.{
  "manifest": "arweave/paths",
  "version": "0.2.0",
  "index": {
    "path": "index.html"
  },
  "fallback": {
    "id": "cG7Hdi_iTQPoEYgQJFqJ8NMpN4KoZ-vH_j7pG4iP7NI"
  },
  "paths": {
    "index.html": {
      "id": "cG7Hdi_iTQPoEYgQJFqJ8NMpN4KoZ-vH_j7pG4iP7NI"
    },
    "js/style.css": {
      "id": "fZ4d7bkCAUiXSfo3zFsPiQvpLVKVtXUKB6kiLNt2XVQ"
    },
    "css/style.css": {
      "id": "fZ4d7bkCAUiXSfo3zFsPiQvpLVKVtXUKB6kiLNt2XVQ"
    },
    "css/mobile.css": {
      "id": "fZ4d7bkCAUiXSfo3zFsPiQvpLVKVtXUKB6kiLNt2XVQ"
    },
    "assets/img/logo.png": {
      "id": "QYWh-QsozsYu2wor0ZygI5Zoa_fRYFc8_X1RkYmw_fU"
    },
    "assets/img/icon.png": {
      "id": "0543SMRGYuGKTaqLzmpOyK4AxAB96Fra2guHzYxjRGo"
    }
  }
} fallback:Manifest version 0.2.0 introduced the fallback attribute. fallback is an object that accepts the sub attribute id, which defines an Arweave data item transaction id for the resolver to fall back to if it fails to correctly resolve a requested path.Source and Further Reading in the official Arweave Path Manifest docs: Arweave Docs

---

# 144. Welcome to the Permaweb  Cooking with the Permaweb

Document Number: 144
Source: https://cookbook.arweave.net/concepts/permaweb.html
Words: 139
Extraction Method: html

Welcome to the Permaweb The permaweb is like the web, but permanent. Developers build on top of permaweb services to create apps and sites that will exist forever on Arweave. Benefits of the permaweb.Sites and apps are permanent, you never have to worry about them going away (even if the team supporting them moves on) App developers have to make sure every new version of the app actually adds value, otherwise, why would you switch off the old one.Because all Permaweb apps share a common storage layer, Arweave, they can all compose with one anther's data.Your data is owned by your wallet and can follow you from app to app.Compare Traditional Web vs Permaweb  For more information about the permaweb check out the medium post open in new window.Built with  ❤️  by the Arweave community. Learn more at  Arweave.org

---

# 145. Posting Transactions  Cooking with the Permaweb

Document Number: 145
Source: https://cookbook.arweave.net/concepts/post-transactions.html
Words: 634
Extraction Method: html

Posting Transactions😍 There are several ways to post transactions to Arweave. Each has its own unique affordances and constraints. The diagram below illustrates the four main approaches to posting transactions.Direct to Peer,Direct to Gateway, Bundled, and Dispatched.  Guaranteed Transactions When posting a large quantity of transactions or when fast settlement time is desireable consider using a bundling service. Bundlers settle large volumes of transactions immediately and make the transaction data available within milliseconds. The bundling service holds onto posted transactions until they are confirmed on-chain. If the transactions are not included in the most recent block the bundling service re-posts them with each new block until they are recorded on chain with a sufficient number of confirmations.Direct Transactions Transactions posted directly to Arweave come in two varieties wallet-to-wallet transactions and data transactions. The first transfers AR tokens between wallet addresses. The second posts data to Arweave and pays the associated storage costs.Interestingly, data transactions may also transfer AR tokens to a wallet address while paying storage costs at the same time.All transactions allow the user to specify up to 2KB worth of metadata in the form of custom tags.Direct to Peer Transactions may be posted directly to an Arweave peer (mining node). This is perhaps the most decentralized means of posting a transaction as clients can choose what peer they wish to post to.This approach is not without drawbacks. Peers may come and go making it difficult to reliably post transactions from an app. While it's possible to query a list of active peers and choose one before posting it adds overhead and friction to the process. Additionally, transactions posted to peers are only queryable at the gateway after being mined in a block. This introduces a 1-2 minute delay between posting the transaction to a peer and it being available to read in a browser from a gateway.For the above reasons, developers tend to configure arweave-js to point to a gateway when posting direct transactions as the optimistic cache at the gateway makes the transaction available almost immediately.Direct to Gateway Gateways sit between clients and Arweave's network of peers. One of the primary functions of the gateway is to index transactions and optimistically cache the data posted to the network while waiting for it to be included in a block. This makes the transaction queryable in a "Pending" state almost instantly which allows applications built on top of a gateway to be more responsive. There is still a risk of transactions dropping out of the optimistic cache if they are not mined in a block by the peers.An example of how to post a direct transaction using arweave-js can be found in this guide.Bundled Transactions Services built on top of Arweave that provide additional utility for Permaweb builders are sometimes called Permaweb Services. A bundler is one such service. Bundlers take multiple individual transactions and bundle them together into a single transaction that is posted directly to Arweave. In this way a single transaction at the protocol level can contain tens of thousands of bundled transactions. There is one restriction, however, only data transactions can be included in a bundle. Wallet-to-wallet transactions (that transfer AR tokens between wallet addresses) must be done as individual transactions posted directly to Arweave.Dispatched Transactions Another way to post bundled transactions is from the browser. While browsers enforce some constraints around the size of data that can be uploaded, browser based wallets are able to post transactions to bundlers. Arweave browser wallets implement a dispatch() API method. If you are posting small transactions (100KB or less) you can use the wallets dispatch() method to take advantage of bundled transactions.An example of how to post a 100KB or less bundled transaction with an Arweave wallets dispatch() method can be found in this guide.Resources arweave-js example dispatch example arseeding-js example

---

# 146. SmartWeave  Cooking with the Permaweb

Document Number: 146
Source: https://cookbook.arweave.net/concepts/smartweave.html
Words: 891
Extraction Method: html

SmartWeave ⚠️ Deprecation Notice This document is deprecated and may contain outdated information.What is SmartWeave?SmartWeave is the name given to the dominant SmartContract paradigm on Arweave. A unique property of SmartWeave contracts is that the current state of the contract is provided by a process of "lazy evaluation". This means that instead of Arweave mining nodes constantly evaluating the current state of all contracts, a client reading a contract evaluates the state for themselves.Why is SmartWeave important?The state and logic of decentralized applications need to be as censorship-resistant, permanent, and verifiable as the rest of their data. SmartWeave enables developers to write smart contracts that encapsulate their apps state and logic on-chain and execute it in a trustless verifiable way. This is no small feat as the Arweave protocol does not include incentives for nodes to evaluate smart contract state for the network.SmartWeave provides an immutable append-only pattern for contract interactions that leverage permanent storage to hold onto their state. The result is a fully decentralized on-chain state machine that can give protocols and applications dynamic functionality in a permissionless and trustless way. By using SmartWeave, developers can create smart contracts that are stored on Arweave and are guaranteed not to change over time. This allows them to build Permaweb applications with dynamic functionality that can be used in a permissionless and trustless manner.There are several reasons why developers might choose to use SmartWeave to implement the logic for their permaweb applications:Decentralized storage: SmartWeave is built on Arweave, which means that applications created using SmartWeave will be stored on a distributed network of nodes rather than on a centralized server. This can make them more resistant to censorship, tampering, and other forms of interference.Lazy evaluation: The lazy evaluation feature of SmartWeave contracts allows for efficient and scaleable execution. Instead of Arweave nodes constantly evaluating the state of a contract, the client reading the contract is responsible for evaluating the state, leveraging the users processing power instead of the networks nodes.Language support: SmartWeave supports a range of programming languages, including JavaScript, TypeScript, Rust, Go, AssemblyScript, and WASM (WebAssembly). This allows developers to use the language they are most familiar with when creating SmartWeave applications.Data durability: Arweave is designed to store data in a way that makes it highly durable and long-lasting. This can be useful for applications that need to store data over a long period of time, such as historical records or scientific data.Economic model: Arweave uses a unique economic model based on the concept of permanent storage that incentivizes miners to store data indefinitely. This can help ensure the long-term viability and durability of permaweb applications created using SmartWeave.How does SmartWeave Work?SmartWeave contracts, at their core, are built from an initial contract state, with edits, additions, and subtractions using transaction tags.SmartWeave SDK's such as Warp (previously RedStone), are used to query for these transactions to build contract state locally, modifying the contract state with each transaction. The Evaluator (Warp) uses tags to query for a contracts transactions; It knows a transaction is part of the contract by way of the App-Name tag, and the Contract tag.Here is an example of a contract interaction.The App-Name says its a Smartweave ACTION.The Contract tag gives the specific transaction ID of the initial contract state.The Input tag gives the contract its function to execute and any other data it needs:[
    {
        name:"App-Name"
        value:"SmartWeaveAction"
    },
    {
        name:"App-Version"
        value:"0.3.0"
    },
    {
        name:"Contract"
        value:"pyM5amizQRN2VlcVBVaC7QzlguUB0p3O3xx9JmbNW48"
    },
    {
        name:"Input"
        value:"{
            "function":"setRecord",
            "subDomain":"@",
            "transactionId":"lfaFgcoBT8auBrFJepLV1hyiUjtlKwVwn5MTjPnTDcs"
        }"
    }
] And here is an example of a contract.The App-Name says its a Smartweave CONTRACT The Contract-Src tag points to the source code of the contract:[
    {
        key:"App-Name"
        value:"SmartWeaveContract"
    },
    {
        key:"App-Version"
        value:"0.3.0"
    },
    {
        key:"Contract-Src"
        value:"JIIB01pRbNK2-UyNxwQK-6eknrjENMTpTvQmB8ZDzQg"
    },
    {
        key:"SDK"
        value:"RedStone"
    },
    {
        key:"Content-Type"
        value:"application/json"
    }
] The resulting state is the current contract state, which the SDK on the client side can use to calculate user balances, contract owners, and other contract specific details. Once the caller has a validated contract state they can build an interaction for the user to deploy to the chain, which upon mining or indexing on a Gateway will be included the next time someone builds the contract state.For a comprehensive overview of the SmartWeave Protocol, its leading implementation Warp Contracts, and more, head to Warp Academy. Dive into step-by-step tutorials, explore advanced concepts, and uncover how SmartWeave power up the permaweb!Smartweave ecosystem projects There's quite a few ecosystem projects leveraging SmartWeave SmartContracts, but here are some of notes:Implementations Warp | Main provider of SmartWeave SDK's, tutorials, and helps maintain the SmartWeave protocol.MEM | Molecular Execution Machine (MEM) is a developer platform that powers the creation and usage of highly available and highly performant applications within a decentralized environment.Tools SonAr | SmartWeave contract explorer, created and hosted by Warp.Resources Warp Academy | A one-stop shop for all things SmartWeave Apps Permapages | Permanent webpage creation tool, ArNS purchase portal, and ANT creation portal. Your profile on the permaweb.ArNS | Arweave Name System  WeaveDB | NoSQL Database as a Smart Contract.KwilDB | SQL Database as a Smart Contract.ArDrive Inferno | Get PST's for uploading thru Ardrive.FirstBatch | FirstBatch aids developers and enterprises in creating personalized, private, and distortion-free AI applications.Othent | Web3 transactions with existing traditional social logins.BazAR | Digital content marketplace with real-world rights.Alex the Archieve | A decentralized archival platform utilizing Arweave's immutable storage.and so much more.

---

# 147. Transaction Metadata (Tags)  Cooking with the Permaweb

Document Number: 147
Source: https://cookbook.arweave.net/concepts/tags.html
Words: 559
Extraction Method: html

Arweave can be thought of as a permanent append-only hard drive where each entry on the drive is its own unique transaction. Transactions have a unique ID, signature, and owner address for the address that signed and paid for the transaction to be posted. Along with those header values, the Arweave protocol allows users to tag transactions with custom tags. These are specified as a collection name value pairs appended to the transaction. These tags make it possible to query Arweave and find all the Transactions that include a particular tag or tags. The ability to query and filter transactions is critical to supporting apps built on Arweave.What are Transaction Tags?Transaction tags are key-value pairs, where the combination of base64URL keys and values must be less than the maximum of 2048 bytes for an arweave native transaction.Some common examples of transaction tags include:Content-Type: Used to specify the MIME type of content for render on the permaweb.App-Name: This tag describes the app that is writing the data App-Version: This tag is the version of the app, paired with App-Name Unix-Time: This tag is the a unix timestamp, seconds since epoch.Title: Used to give a name or brief description of the content stored in the transaction.Description: Used to provide a longer description of the content.Transaction tags can be used for a variety of purposes, such as indexing transactions for search, organizing transactions into categories, or providing metadata about the content stored in a transaction.Some good things to know about Transaction Tags Transaction tags are encoded as Base64URL encoded strings for both the key and value. This makes it possible to post arrays of bytes as keys or values and transfer them safely over http. While it's not human readable without decoding, it shouldn't be considered encryption.The max total size of Transaction tags for transaction posted directly to Arweave is 2048 bytes. This size is determined by the concatenation of all keys and all values of the transaction tags.Transaction tags can be used in GraphQL queries to return a filtered set of transaction items.Common Tags used in the community Tag Name Description Use Cases App-Name Most commonly used to identify applications using Arweave Common uses are the project's name, sometimes also used in specific ANS transactions App-Version The version of this data, it may represent the app consuming this information E.g. 0.3.0 Content-Type MIME Type to identify the data contained in the transaction text/html, application/json, image/png Unix-Time This tag is the a unix timestamp, seconds since epoch The time the transaction is submitted Title ANS-110 Standard for describing content Providing a name for an Atomic Asset Type ANS-110 Standard for categorization of data a type can classify a permaweb asset Examples const tx = await arweave.createTransaction({ data: mydata });
tx.addTag("Content-Type", "text/html");
tx.addTag("Title", "My incredible post about Transaction Tags");
tx.addTag("Description", "This is one post you do not want to miss!");
tx.addTag("Topic:Amazing", "Amazing");
tx.addTag("Type", "blog-post");

await arweave.transactions.sign(tx, jwk);
await arweave.transactions.post(tx);Summary Understanding how Transaction Tags factor into the Arweave tech stack can provide context on how to solve problems using the Permaweb as an application platform. Tags provide a tool to consume and create common data standards and patterns to encourage a non-rivalous data experience on the Permaweb. The result gives users of the ecosystem the choice of applications to consume and create content as their data is always with the user not the application.

---

# 148. Cooking with the Permaweb

Document Number: 148
Source: https://cookbook.arweave.net/guides/deploying-manifests/arseeding-js.html
Words: 371
Extraction Method: html

Arseeding JS SDK  The guide of how to easily use the Manifest feature in Arseeding.Getting Started Installing the SDK npm i arseeding-js Create a demo.js, and copy the following code into it.import {uploadFolderAndPay} from "arseeding-js/cjs/uploadFolder";

const run = async () => {
  const path = 'Your Folder path'
  const priv = 'YOUR PRIVATE KEY'
  const arseedUrl = 'https://arseed.web3infra.dev'
  const tag = '<chaintype-symbol-id>' // everpay supported all token tag (chainType-symbol-id)
  const indexFile = ''

  const res = await uploadFolderAndPay(path,priv,arseedUrl,tag, indexFile)
  console.log(res)
}

// review manifest Data
curl --location --request GET 'https://arseed.web3infra.dev/{res.maniId}' Configuration Notes:Populate your ECC key with YOUR PRIVATE KEY. Make sure that the wallet corresponding to the private key has assets in everPay.arseedUrl is the URL of the Arseeding backend service, here we use the public Arseeding service provided by permadao: https://arseed.web3infra.dev.payUrl is the URL of the everPay service that needs to be configured: https://api.everpay.io path is the path to the folder you want to upload, for example, to deploy a static website, the front-end project will generate a build or dist folder after the project is compiled, just choose the path to that folder.tag is the payment token tag you need to select, if your MetaMask address held in everPay is usdc, you can get the usdc tag via getTokenTagByEver('usdc'),If you want to pay with another token, just fill in the token name to get the specified tag.indexFile is optional,if you don't pass it, index.html (if exist) or null will be default value, if the folder is a front-end project build folder you don't need pass indexFile.After preparing the configuration, call uploadFolderAndPay (path,priv,url,payCurrency) to upload all the files under your folder to web3infra's Arseeding node by means of manifest.node demo.js return:{
  fee: '0.004218',
  maniId: 'EHeDa8b428L38b972qyHI87YELuZKue1jDI_JWC-aGE',
    everHash:[
            '0x46744320be6529c48bf18c348fa181facef3d9d6d920a24687dc9964ba3ead0a'
    ]
} Download data-Access page The maniId can be found in the returned result, the maniId above is EHeDa8b428L38b972qyHI87YELuZKue1jDI_JWC-aGE In this tutorial, we are uploading a Docusaurus front-end project, and running yarn build under that project will generate a build folder, which is the one we uploaded. Now, we can access the site via maniId!In your browser, enter:https://arseed.web3infra.dev/EHeDa8b428L38b972qyHI87YELuZKue1jDI_JWC-aGE You can now access this website, and it will be permanently available! References and Further Reading:Arseeding Documentation Follow the Arseeding Upload Manifest tutorial here 。

---

# 149. Developing on the Permaweb  Cooking with the Permaweb

Document Number: 149
Source: https://cookbook.arweave.net/getting-started/welcome.html
Words: 203
Extraction Method: html

Developing on the Permaweb Welcome to the Permaweb Creating applications on the permaweb, which is built on the Arweave protocol, is similar to building traditional web applications but with some key differences.One major difference is that data is stored on the permaweb permanently, as the name suggests, rather than on a centralized server. This means that once data is uploaded to the permaweb, it cannot be deleted or altered. This can be beneficial for applications that require tamper-proof data storage, such as supply chain management or voting systems.Another difference is that the permaweb is decentralized, meaning there is no central point of control or failure. This can provide increased security and reliability for applications.Additionally, the permaweb uses a unique token, called AR, to pay for the storage of data on the network. This can add a new layer of complexity to application development, as developers need to consider how to integrate AR into their applications and handle payments.Overall, the experience of creating applications on the permaweb can be challenging, but it can also be rewarding as it offers unique benefits over traditional web development.Hello Worlds Hello World (No Code) Hello World (CLI) Built with  ❤️  by the Arweave community. Learn more at  Arweave.org

---

# 150. arkb  Cooking with the Permaweb

Document Number: 150
Source: https://cookbook.arweave.net/guides/deployment/arkb.html
Words: 243
Extraction Method: html

arkb Requirements An Arweave wallet is required to deploy using arkb for covering the data transaction costs.Installation To install arkb run npm install -g arkb yarn add ar-gql Deploying When uploading a directory of files or a Permaweb application, by default arkb deploys each file separately as an L1 transaction, with the option to bundle the transactions using Bundlr.Static Build Permaweb applications are statically generated, meaning that the code and content are generated ahead of time and stored on the network.Below is an example of a static site. To deploy this to the Permaweb, the build directory will be passed in as the argument for the deploy flag.|- build
    |- index.html
    |- styles.css
    |- index.js Default Deployment Deploying as an L1 transaction can take longer to confirm as it is directly uploaded to the Arweave network.arkb deploy [folder] --wallet [path to wallet]   Bundled Deployment To deploy using Bundlr you will need to fund a Bundlr node.Bundlr node2 allows free transactions under 100kb.You can add custom identifiable tags to the deployment using tag-name/tag-value syntax.arkb deploy [folder] --use-bundler [bundlr node] --wallet [path to wallet] --tag-name [tag name] --tag-value [tag value]   Other Commands Fund Bundlr arkb fund-bundler [amount] --use-bundler [bundlr node] * Funding a Bundlr instance can take up to 30 minutes to process Saving Keyfile arkb wallet-save [path to wallet] After saving your key you can now run commands without the --wallet-file option, like this arkb deploy [path to directory] Check Wallet Balance arkb balance

---

# 151. ServerSide DNS Integration  Cooking with the Permaweb

Document Number: 151
Source: https://cookbook.arweave.net/guides/dns-integration/server-side.html
Words: 400
Extraction Method: html

ServerSide DNS Integration So you have a permaweb application and it is on the permaweb, but you also have a specific domain that you want users to use to access this app. mydomain.com, to connect your domain to a permaweb app, you have several options, this option we will show here is a called a server-side redirect. The redirect occurs as a reverse proxy so that the user remains on mydomain.com in their browser, while behind the scenes the application is being served from the permaweb.TIP You can use any reverse proxy to setup a server-side redirect, in this guide we will be using deno and deno.com a lightweight edge hosting service.What you will need to setup a reverse proxy using deno.com A deno.com account, which at the time of this writting is free.A domain with access to the DNS Settings A permaweb application identifier and is deployed on the permaweb Create proxy on Deno.com Deno Deploy is a distributed system that runs at the edge. 35 regions worldwide. Open your browser to https://deno.com and click sign in or sign up if you do not have an account.Click on New Project and Click Play The deno playground will allow us to create a proxy without having to leave the browser.Copy the following code:This proxy server will receive requests from mydomain.com and proxy the request to arweave.net/APP_ID and then return the response as mydomain.com. Your APP_ID is the TX_ID identifier for you permaweb application.Click Save and Deploy Connecting to DNS In Project Settings go to the domains section and click to add a domain.Enter mydomain.com domain and follow the instructions to modify your DNS settings to point to the deno deploy edge network.It may take a few minutes to resolve to the dns, but once resolved your app will now be rendering from mydomain.com.🎉 Congrats you have published a server-side redirect to your permaweb application.WARNING Note that any changes to your application will generate a new TX_ID and you will need to modify that TX_ID to publish the new changes to your domain.Automating the Deploy If you would like to automate new deploys of your permaweb app, look into github actions and using the deno deploy github action: https://github.com/denoland/deployctl/blob/main/action/README.md Summary Server Side redirects are great for providing your users a Domain Name System URL to access your permaweb application. We hope you found this guide useful in your permaweb development journey!

---

# 152. Posting a Transaction using Dispatch  Cooking with the Permaweb

Document Number: 152
Source: https://cookbook.arweave.net/guides/posting-transactions/dispatch.html
Words: 151
Extraction Method: html

Posting a Transaction using Dispatch Arweave Browser wallets have the concept of dispatching transactions. If the transaction is under 100KB in size it can be posted for free!Dispatching a Transaction This can be done without any package dependencies for the client app. As long as the user has a browser wallet active and the data is less than 100KB, dispatched transactions are free and guaranteed to be confirmed on the network.// use arweave-js to create a transaction
let tx = await arweave.createTransaction({ data:"Hello World!" })

// add some custom tags to the transaction
tx.addTag('App-Name', 'PublicSquare')
tx.addTag('Content-Type', 'text/plain')
tx.addTag('Version', '1.0.1')
tx.addTag('Type', 'post')

// use the browser wallet to dispatch() the transaction
let result = await window.arweaveWallet.dispatch(tx);

// log out the transactino id
console.log(result.id);Resources For an overview of all the ways you can post transactions, see the Posting Transactions section of the cookbook.Built with  ❤️  by the Arweave community. Learn more at  Arweave.org

---

# 153. Posting Transactions using arseedingjs  Cooking with the Permaweb

Document Number: 153
Source: https://cookbook.arweave.net/guides/posting-transactions/arseeding-js.html
Words: 223
Extraction Method: html

Posting Transactions using arseeding.js You can use the arseeding-js JavaScript SDK package to publish transactions on the Arweave network. Arseeding automatically broadcasts the transaction to all Arweave nodes in the network, ensuring that the transaction is promptly received in the pending pool of all Arweave nodes, thus increasing the transaction's packaging speed.Installing arseeding.js To install arseeding.js run:npm install arseeding-js yarn add arseeding-js Transaction for Uploading Data When using Arseeding, you must pre-fund your account on everpay open in new window. This balance can be funded with $AR tokens or other cryptocurrencies. Another distinction is that the Arseeding service ensures that your data will make it onto the blockchain.const { genNodeAPI } = require('arseeding-js')

const run = async () => {
  const instance = genNodeAPI('YOUR PRIVATE KEY')
  const arseedUrl = 'https://arseed.web3infra.dev'
  const data = Buffer.from('........')
  const payCurrencyTag = 'ethereum-usdc-0xa0b86991c6218b36c1d19d4a2e9eb0ce3606eb48' // everPay supported token tag (chainType-symbol-id)
  const options = {
    tags: [{ name: 'Content-Type', value: 'image/png' }]
  }
  const res = await instance.sendAndPay(arseedUrl, data, payCurrencyTag, options)
  console.log('res', res)
}
run() Resources For an overview of all methods for publishing transactions, please refer to the Publishing Transactions section in the operation manual.You can find the complete Arseeding documentation on the Arseeding website open in new window.Follow the Arseeding Upload Manifest tutorial here open in new window.Built with  ❤️  by the Arweave community. Learn more at  Arweave.org

---

# 154. Posting Transactions using Ardrive Turbo  Cooking with the Permaweb

Document Number: 154
Source: https://cookbook.arweave.net/guides/posting-transactions/turbo.html
Words: 377
Extraction Method: html

Posting Transactions using Ardrive Turbo Posting transactions using Turbo can be accomplished using the @ardrive/turbo-sdk JavaScript package.Installing the @ardrive/turbo-sdk To install @ardrive/turbo-sdk run npm install @ardrive/turbo-sdk yarn add @ardrive/turbo-sdk Initializing Turbo Client There are multiple ways to upload data using the turbo sdk. You can:upload simple data upload a file upload a data-item ArDrive Turbo's documentation provides a quick start for any of these methods.The recommended way of posting transactions and uploading data to Arweave is with DataItems.Quick Start import { ArweaveSigner, TurboFactory } from '@ardrive/turbo-sdk';
import Arweave from 'arweave';
import fs from 'fs';
import open from 'open';
import path from 'path';

async function uploadWithTurbo() {
  const jwk = JSON.parse(fs.readFileSync('./my-jwk.json', 'utf-8'));
  const signer = new ArweaveSigner(jwk);
  const turbo = TurboFactory.authenticated({ signer });

  try {
    // upload some simple data - log upload progress events
    const { id, owner, dataCaches, fastFinalityIndexes } = await turbo.upload({
      data: 'Hello, world!',
      events: {
        // overall events (includes signing and upload events)
        onProgress: ({ totalBytes, processedBytes, step }) => {
          console.log('Overall progress:', { totalBytes, processedBytes, step });
        },
        onError: ({ error, step }) => {
          console.log('Overall error:', { error, step });
        },
      },
    });

    // upload a file - log signing and upload progress events
    const filePath = path.join(__dirname, './my-image.png');
    const fileSize = fs.statSync(filePath).size;
    const { id, owner, dataCaches, fastFinalityIndexes } =
      await turbo.uploadFile({
        fileStreamFactory: () => fs.createReadStream(filePath),
        fileSizeFactory: () => fileSize,
        events: {
          // overall events (includes signing and upload events)
          onProgress: ({ totalBytes, processedBytes, step }) => {
            console.log('Overall progress:', { totalBytes, processedBytes, step });
          },
          onError: ({ error, step }) => {
            console.log('Overall error:', { error, step });
          },
          // signing events
          onSigningProgress: ({ totalBytes, processedBytes }) => {
            console.log('Signing progress:', { totalBytes, processedBytes });
          },
          onSigningError: (error) => {
            console.log('Signing error:', { error });
          },
          onSigningSuccess: () => {
            console.log('Signing success!');
          },
          // upload events
          onUploadProgress: ({ totalBytes, processedBytes }) => {
            console.log('Upload progress:', { totalBytes, processedBytes });
          },
          onUploadError: (error) => {
            console.log('Upload error:', { error });
          },
          onUploadSuccess: () => {
            console.log('Upload success!');
          },
        },
      });
    // upload complete!
    console.log('Successfully upload data item!', {
      id,
      owner,
      dataCaches,
      fastFinalityIndexes,
    });
  } catch (error) {
    // upload failed
    console.error('Failed to upload data item!', error);
  }
} Resources Dive into the Code Join the discussion in the ArDrive Discord

---

# 155. Warp (SmartWeave) SDK - Deploying Contracts  Cooking with the Permaweb

Document Number: 155
Source: https://cookbook.arweave.net/guides/smartweave/warp/deploying-contracts.html
Words: 732
Extraction Method: html

Warp (SmartWeave) SDK - Deploying Contracts ⚠️ Deprecation Notice This document is deprecated and may contain outdated information.SmartWeave Contracts are created by posting two transactions to the network, a Source Transaction and a Initial State Transaction, the source transaction contains the source code the contract will use to determine the current state. The initial state transaction provides a contract identifer to reference as well as the initial seed data the contract should use as the starting point to evaluate the current state. The current state is calculated by accessing actions that are transactions written to the network that contain input parameters to execute using the evaluated and instantiated source code. Warp Contracts can be created using many different languages and can be evaluated using the Warp SDK. This guide will show the many different ways you can deploy a Warp Contract.TIP If you would like to learn more about authoring Warp SmartWeaveContracts, checkout the Warp Academy! https://academy.warp.cc/ As of Warp version 1.3.0 you willl need a plugin to deploy contracts with Warp. This plugin will enable you to add different wallet signatures.import { DeployPlugin, InjectedArweaveSigner } from 'warp-contracts-plugin-deploy'
import { WarpFactory } from 'warp-contracts'

const warp = WarpFactory.forMainnet().use(new DeployPlugin())

...

function deploy(initState, src) {
  if (window.arweaveWallet) {
    await window.arweaveWallet.connect(['ACCESS_ADDRESS', 'SIGN_TRANSACTION', 'ACCESS_PUBLIC_KEY', 'SIGNATURE']);
  }
  const userSigner = new InjectedArweaveSigner(window.arweaveWallet);
  await userSigner.setPublicKey();

  return warp.deploy({
    wallet: userSigner,
    src,
    initState: JSON.stringify(initState)
  })
} The Four ways to deploy a Warp SmartWeave Contract There are 4 ways you can deploy a SmartWeaveContract via the Warp SDK, these options handle different use cases that a developer may encounter.Need to deploy the contract with the source at the same time Need to deploy a contract where the source is already on the permaweb Need to deploy a contract through the sequencer and point it to some data using a path manifest Need to deploy a contract via Irys and register that contract on the sequencer TIP For more information about Warp deployments check out the github Readme for the project. https://github.com/warp-contracts/warp#deployment.WARNING This project is in rapid development, so the documentation here could be out of data quickly, if you discover it is out of date, please let us know on the Permaweb Cookbook Discord Channel.Examples TIP By default all deploy functions are published to Arweave via Irys, each option has a flag that can be set to not use Irys, but it can take many confirmations for the network to fully confirm the transaction.deploy Deploys contract plus source code to Warp Sequencer, to Irys (L2), to Arweave.const { contractTxId, srcTxId } = await warp.deploy({
    wallet,
    initState,
    data: { "Content-Type": "text/html", body: "<h1>Hello World</h1>" },
    src: contractSrc,
    tags: [{ name: "AppName", value: "HelloWorld" }],
});wallet - should be Arweave keyfile (wallet.json) parsed as a JSON object implementing the JWK Interface or the string 'use_wallet' initState - is a stringified JSON object data - is optional if you want to write data as part of your deployment src - is the string or Uint8Array value of the source code for the contract tags - is an array of name/value objects {name: string, value: string}[], Learn more about tags deployFromSourceTx Already have the source on the permaweb? Then deployFromSourceTx is your tool of choice! With the permaweb you never have to worry about data changing so re-using source code for contracts is a no brainer.const { contractTxId, srcTxId } = await warp.deployFromSourceTx({
    wallet,
    initState,
    srcTxId: "SRC_TX_ID",
});deployBundled Uses Warp Gateway Sequencer's endpoint to upload a raw data item to Irys and index it.import { createData } from "arbundles";

const dataItem = createData(
    JSON.stringify({
        manifest: "arweave/paths",
        version: "0.1.0",
        index: {
            path: "index.html",
        },
        paths: {
            "index.html": {
                id: "cG7Hdi_iTQPoEYgQJFqJ8NMpN4KoZ-vH_j7pG4iP7NI",
            },
        },
    }),
    { tags: [{ "Content-Type": "application/x.arweave-manifest+json" }] },
);
const { contractTxId } = await warp.deployBundled(dataItem.getRaw());register Uses Warp Gateway Sequencer's endpoint to index a contract that has been uploaded with Irys.Summary Why are there so many options to deploy contracts? These methods exist to reduce duplication, enable advanced contract interactions, and allow for flexibility for testing and usage of the smartweave protocol. The permaweb is very unique in its architecture, it provides a feature where you can deploy both digital data and the contract to manage that data generating the same transaction identifier. The result is dynamic data paired with an immutable set of data. Deploying contracts is just one piece of the Warp SDK, to learn more keep reading this guide!

---

# 156. Warp (SmartWeave) SDK - Evolve  Cooking with the Permaweb

Document Number: 156
Source: https://cookbook.arweave.net/guides/smartweave/warp/evolve.html
Words: 419
Extraction Method: html

Warp (SmartWeave) SDK - Evolve ⚠️ Deprecation Notice This document is deprecated and may contain outdated information.Evolve is a feature that allows developers to update the source code of a smart contract without deploying a new contract. To use this feature, you must first submit the new source code using the save function. Once the updated code has been confirmed on the Permaweb, you can use the evolve function to point the contract to the new source code ID. This allows you to update the contract's behavior without creating a new contract instance.Why?Writing SmartWeave contracts can be difficult and sometimes requires updates or new features to be added over time. Evolve allows you to make changes to your contract without having to create a new contract instance from scratch. To use this feature, your contract state object must include an evolve property that is set to the new contract source transaction identifier. This enables you to modify and improve your existing contract without starting from scratch.{
  ...
  "evolve": "YOUR SOURCE CODE TX_ID"
} Post your new source to the permaweb Before you can evolve your existing contract, you need to post the new source code to the permaweb, you can do this with the save function.import { WarpFactory } from 'warp-contracts'
import fs from 'fs'

const src = fs.readFileSync('./dist/contract.js', 'utf-8')
const jwk = JSON.parse(fs.readFileSync('./wallet.json', 'utf-8'))
const TX_ID = 'VFr3Bk-uM-motpNNkkFg4lNW1BMmSfzqsVO551Ho4hA'
const warp = WarpFactory.forMainnet()

async function main() {
  const newSrcTxId = await warp.contract(TX_ID).connect(jwk).save({src })
  console.log('NEW SRC ID', newSrcTxId)
}

main() Evolve your contract WARNING Verify your new Source TX_ID is confirmed, go to Sonar to make sure the TX_ID is confirmed.import { WarpFactory } from 'warp-contracts'
import fs from 'fs'

const src = fs.readFileSync('./dist/contract.js', 'utf-8')
const jwk = JSON.parse(fs.readFileSync('./wallet.json', 'utf-8'))
const TX_ID = 'VFr3Bk-uM-motpNNkkFg4lNW1BMmSfzqsVO551Ho4hA'
const warp = WarpFactory.forMainnet()

async function main() {
  const newSrcTxId = await warp.contract(TX_ID).connect(jwk).evolve('SRC TX ID')
  console.log(result)
}

main() TIP It's worth noting that the evolve feature is only applicable to future actions, meaning you cannot use it to apply new source code to actions that occurred before the contract was evolved.Summary Evolve is a powerful feature and can provide extensibility for your contracts, it can also be an attack vector, so make sure you fully understand what you are doing when using it. Below is a common snippet of what an evolve function may look like in your contract.export async function handle(state, action) {
  ...
  if (action.input.function === 'evolve') {
    if (action.caller === state.creator) {
      state.evolve = action.input.value 
    }
    return { state }
  }
  ...
}

---

# 157. Warp (SmartWeave) SDK - ReadState  Cooking with the Permaweb

Document Number: 157
Source: https://cookbook.arweave.net/guides/smartweave/warp/readstate.html
Words: 275
Extraction Method: html

Warp (SmartWeave) SDK - ReadState ⚠️ Deprecation Notice This document is deprecated and may contain outdated information.SmartWeave Contract state is calculated via lazy evaluation, which means, the state evaluation occurs on reads not writes. When reading contracts, the SDK gathers all state interactions, sorts them, and executes them against the source contract using a reduce or fold pattern.Basic Readstate const warp = WarpFactory.forMainnet()
const CONTRACT_ID = '_z0ch80z_daDUFqC9jHjfOL8nekJcok4ZRkE_UesYsk'

const result = await warp.contract(CONTRACT_ID).readState()

// log current state
console.log(result.cachedValue.state) Advanced Readstate Some contracts either read the state of other contracts, or invoke or write to other contracts, when requesting the state of these contracts it is necessary to set evaluation options.const warp = WarpFactory.forMainnet()
const CONTRACT_ID = 'FMRHYgSijiUNBrFy-XqyNNXenHsCV0ThR4lGAPO4chA'

const result = await warp.contract(CONTRACT_ID)
  .setEvaluationOptions({
    internalWrites: true,
    allowBigInt: true
  })
  .readState()

// log current state
console.log(result.cachedValue.state) Common Evaluation Options Name Description internalWrites Evaluates contracts that contain internal writes to other contracts allowBigInt Evaluates contracts that use the BigInt primitive you can find out more about bigInt MDN Docs unsafeClient This value could be allow or skip or throw. You should avoid using unsafeClient in your contracts it can lead to underministic results.Readstate from specific BlockHeight or Sortkey You may want to look at a previous state, not the current state, by supplying a blockHeight you can read the state of a contract at a specific block height const { sortKey, cachedValue } = await contract.readState(1090111) Summary Reading the current state of SmartWeave Contracts performs state evaluation by pulling all interactions and processing each interaction via a fold method. This approach is unique to the permaweb and requires a unique understanding of how your SmartWeave Contract code is executed.

---

# 158. Warp WriteInteractions  Cooking with the Permaweb

Document Number: 158
Source: https://cookbook.arweave.net/guides/smartweave/warp/write-interactions.html
Words: 353
Extraction Method: html

⚠️ Deprecation Notice This document is deprecated and may contain outdated information.To call a function on a SmartWeave contract, you can create a transaction known as a SmartWeave action. This action includes the function name and the necessary input parameters for the function on the SmartWeave contract. You can create a SmartWeave action using the contract.writeInteraction function.Code import { WarpFactory } from 'warp-contracts'

const warp = WarpFactory.forMainnet()
const STAMP_PROTOCOL = 'FMRHYgSijiUNBrFy-XqyNNXenHsCV0ThR4lGAPO4chA'

async function doStamp() {
  const result = await warp.contract(STAMP_PROTOCOL)
    .connect('use_wallet')
    .writeInteraction({
      function: 'stamp',
      timestamp: Date.now(),
      transactionId: 'zQhANphTO0DOsaWXhExylUD5cBN3a6xWvfn5ZCpmCVY'
    })
  console.log(result)
} When calling writeInteraction, you need to pass your input parameters, these are the parameters the contract is expecting to receive.WARNING Since SmartWeave contracts are evaluated in a lazy flow, you do not know if your interaction ran successfully until you evaluate the contract to the current state. Use Warp readState to access the contract and determine if the interaction was applied successfully.Dry Write DryWrite allows you to test and verify an interaction on the current state without actually executing it on the permaweb. This feature allows you to simulate the interaction locally and ensure that it will be successful before applying it.import { WarpFactory } from 'warp-contracts'

const warp = WarpFactory.forMainnet()
const STAMP_PROTOCOL = 'FMRHYgSijiUNBrFy-XqyNNXenHsCV0ThR4lGAPO4chA'

async function doStamp() {
  const result = await warp.contract(STAMP_PROTOCOL)
    .connect('use_wallet')
    .dryWrite({
      function: 'stamp',
      timestamp: Date.now(),
      transactionId: 'zQhANphTO0DOsaWXhExylUD5cBN3a6xWvfn5ZCpmCVY'
    })
  console.log(result)
} WARNING One thing to note when using dry writes, is that the entire state needs to be evaluated locally for contacts that use readState or internalWrites. This can result in a slow performing process.Optimized for speed By default, writeInteractions are submitted to the Warp Sequencer and bundled and posted to Arweave. You can post directly to Arweave by disabling bundling.const result = await contract.writeInteraction({
  function: 'NAME_OF_YOUR_FUNCTION',
  ...
}, { disableBundling: true }) Summary The SmartWeave Protocol allows for the modification of dynamic data on an immutable, append-only storage system using writeInteractions. These interactions enable trustless and permissionless communication with SmartWeave contracts. The Warp SDK provides developers with a user-friendly API for interacting with the SmartWeave Protocol and its writeInteractions feature.For additional resources:Warp SDK https://github.com/warp-contracts/warp Warp Docs https://warp.cc

---

# 159. Vouch  Cooking with the Permaweb

Document Number: 159
Source: https://cookbook.arweave.net/guides/vouch.html
Words: 224
Extraction Method: html

Vouch There are a few ways to query an Arweave address to verify if it has been vouched by a service. Below is two of those approaches.VouchDAO Package The isVouched function is made available to use in your applications in a straight-forward way.Installation Add the package:npm i vouchdao yarn add vouchdao Usage Inside of an async function you can use the isVouched function which will return true if a user is vouched.import { isVouched } from 'vouchdao'
(async () => {
  const res = await isVouched("ARWEAVE_ADDRESS") // true || undefined
  // ...
})();Using GraphQL You can query the Arweave network using GraphQL to find out if a given Arweave address has been vouched.query {
  transactions(
    tags:{name:"Vouch-For", values:["ARWEAVE_ADDRESS"]}
  ) {
    edges {
      node {
        id
        tags {
          name 
          value 
        }
      }
    }
  }
} If the address has been vouched, an array of nodes will be returned with tags pertaining to the service that issues the ANS-109. You can cross reference the owner address value with the passed community votes to ensure the service has been verified through community vote via VouchDAO."owner": {
 "address": "Ax_uXyLQBPZSQ15movzv9-O1mDo30khslqN64qD27Z8"
},
"tags": [
  {
    "name": "Content-Type",
    "value": "application/json"
  },
  {
    "name": "App-Name",
    "value": "Vouch"
  },
  {
    "name": "App-Version",
    "value": "0.1"
  },
  {
    "name": "Verification-Method",
    "value": "Twitter"
  },
  {
    "name": "Vouch-For",
    "value": "ARWEAVE_ADDRESS"
  }
] Resources VouchDAO VouchDAO Contract Arweave/GraphQL Playground

---

# 160. Warp (SmartWeave) SDK Intro  Cooking with the Permaweb

Document Number: 160
Source: https://cookbook.arweave.net/guides/smartweave/warp/intro.html
Words: 276
Extraction Method: html

Warp (SmartWeave) SDK Intro ⚠️ Deprecation Notice This document is deprecated and may contain outdated information.Warp is a popular SmartWeave Protocol SDK. With Warp and Irys your SmartWeave deployments and interactions can be extremely fast.Introduction This guide is a short introduction to the Warp SDK and some of its API methods, if you want to learn more about SmartWeave Contracts in general visit Core Concepts: SmartWeave.TIP You can find the Warp SDK on github. For a deeper dive on Warp SmartWeave visit Warp Website To use the SDK on the server, you will need access to a wallet.json file, to use the SDK in the browser you will need to connect to an arweave supported wallet.Install To install warp in your project you can use npm or yarn or other npm clients.npm install warp-contracts yarn add warp-contracts Import When using Warp with your project there are several ways to import the sdk depending on your project setup.import { WarpFactory } from "warp-contracts";import { WarpFactory } from "warp-contracts/mjs";const { WarpFactory } = require("warp-contracts");Connecting to an environment There are several environments that you may want to interact with, you can connect to those environments using the forXXXX helpers.const warp = WarpFactory.forMainnet();const warp = WarpFactory.forTestnet();const warp = WarpFactory.forLocal();const warp = WarpFactory.custom(
    arweave, // arweave-js
    cacheOptions, // { ...defaultCacheOptions, inMemory: true}
    environment, // 'local', 'testnet', 'mainnet'
);WARNING When using local environment, you will need to have arLocal running on port 1984.Summary This intro guide is to help you get setup with Warp, the following guides will show you how to deploy SmartWeave contracts using the Warp SDK, how to interact with those contracts and finally, how to evolve SmartWeave contracts.

---

# 161. React Starter Kits  Cooking with the Permaweb

Document Number: 161
Source: https://cookbook.arweave.net/kits/react/index.html
Words: 100
Extraction Method: html

React Starter Kits React is a popular library used for building user interfaces. Alongside other popular tools such as create-react-app, a React project can be compiled into a bundle. This bundle can be uploaded as a transaction to the permaweb where it will serve as a single page application.React Starter Kit Guides:Vite - React + Vite, publish with permaweb-deploy Create React App - utilize Create React App to build a React permaweb app Permaweb Application Constraints 100% Front-end application (No Server-Side Backend) Applications are served from a sub-path (https://[gateway]/[TX_ID]) Built with  ❤️  by the Arweave community. Learn more at  Arweave.org

---

# 162. SvelteVite Starter Kit  Cooking with the Permaweb

Document Number: 162
Source: https://cookbook.arweave.net/kits/svelte/vite.html
Words: 625
Extraction Method: html

Svelte/Vite Starter Kit Svelte is the framework that compiles out of the way, that results is small packages, which is perfect for the permaweb. As developers, we value Dev Experience as much as we value User Experience. This kit uses the vite bundle system to give developers a great DX experience.Installing vite with svelte and typescript npm create vite@latest my-perma-app --template svelte-ts npm create vite@latest my-perma-app -- --template svelte-ts yarn create vite my-perma-app --template svelte-ts pnpm create vite my-perma-app --template svelte-ts Project Info The vite build system places your index.html file in the root directory, this is where you would include any css or global script dependencies if needed. For more information about the vite project layout check out the vite documentation Setup hash-router To setup the hash-router we will use tinro. tinro is a tiny declarative routing library, that is similar to React Router.npm install --save-dev tinro yarn add -D tinro Telling Svelte to use hash routing In the src/App.svelte file, you want to configure the router to use the hash routing mode.The router.mode.hash function turns on hash router mode. The router.subscribe callback is nice to reset the page to the top on page transfers Adding some transition components These component will manage the transition between one page to another page when routing.Create a directory under the src directory called components and add these two files:announcer.svelte This component is for screen readers announcing when a page changes transition.svelte <script>
  import { router } from "tinro";
  import { fade } from "svelte/transition";
</script>

{#key $router.path}
  <div in:fade={{ duration: 700 }}>
    <slot />
  </div>
{/key} This component adds a fade to the page transition Adding Routes to the app <script lang="ts">
    ...
    import Announcer from "./components/announcer.svelte";
    import Transition from "./components/transition.svelte";
    ...
</script>
<Announcer />
<Transition>
    <Route path="/">
        <Home />
    </Route>
    <Route path="/about">
        <About />
    </Route>
</Transition> Adding the Announcer and Transition components to our routing system will handle announcing page transitions as well as animating the transition.Create some pages home.svelte <script lang="ts">
    let count = 0;

    function inc() {
        count += 1;
    }
</script>
<h1>Hello Permaweb</h1>
<button on:click="{inc}">Inc</button>
<p>Count: {count}</p>
<a href="/about">About</a> about.svelte <h1>About Page</h1>
<p>Svelte/Vite About Page</p>
<a href="/">Home</a> Modify App.svelte <script lang="ts">
    ...
    import Home from './home.svelte'
    import About from './about.svelte'
</script>
...Deploy Permanently Generate Wallet We need the arweave package to generate a wallet npm install --save arweave yarn add arweave -D then run this command in the terminal node -e "require('arweave').init({}).wallets.generate().then(JSON.stringify).then(console.log.bind(console))" > wallet.json Fund Wallet You will need to fund your wallet with ArDrive Turbo credits. To do this, enter ArDrive and import your wallet. Then, you can purchase turbo credits for your wallet.Setup Permaweb-Deploy npm install --global permaweb-deploy yarn global add permaweb-deploy Update vite.config.ts import { defineConfig } from 'vite'
import { svelte } from '@sveltejs/vite-plugin-svelte'

export default defineConfig({
  plugins: [svelte()],
  base: './'
}) Update package.json Update package.json {
  ...
  "scripts": {
    ...
    "deploy": "DEPLOY_KEY=$(base64 -i wallet.json) permaweb-deploy --ant-process << ANT-PROCESS >> --deploy-folder build"
  }
  ...
} Replace << ANT-PROCESS >> with your ANT process id.Run build Now it is time to generate a build, run npm run build yarn build Run deploy Finally we are good to deploy our first Permaweb Application npm run deploy yarn deploy ERROR If you receive an error Insufficient funds, make sure you remembered to fund your deployment wallet with ArDrive Turbo credits.Response You should see a response similar to the following:Deployed TxId [<<tx-id>>] to ANT [<<ant-process>>] using undername [<<undername>>] Your Svelte app can be found at https://arweave.net/<< tx-id >>.SUCCESS You should now have a Svelte Application on the Permaweb! Great Job!Summary This is a minimal version of publishing a Svelte application on the permaweb, but you may want more features, like hot-reloading and tailwind, etc. Check out hypar for a turnkey starter kit. HypAR

---

# 163. Vue Starter Kits  Cooking with the Permaweb

Document Number: 163
Source: https://cookbook.arweave.net/kits/vue/index.html
Words: 117
Extraction Method: html

Vue Starter Kits Vue.js is a progressive JavaScript framework that allows building user interfaces. Unlike other frameworks, it compiles the template into JavaScript during runtime, resulting in a smaller file size and faster performance. Vue is ideal for building performant and scalable single-page applications, making it a popular choice among front-end developers.Vue Starter Kit Guides:Note: - Since npm init vue@latest alredy uses vite, we have not included a vite guide for Vue.Create Vue App - Use Create Vue to efficiently build a Vue.js-based with TypeScript and Vite modern permaweb application Permaweb Application Constraints 100% Front-end application (No Server-Side Backend) Applications are served from a sub-path (https://[gateway]/[TX_ID]) Built with  ❤️  by the Arweave community. Learn more at  Arweave.org

---

# 164. Svelte Starter Kits  Cooking with the Permaweb

Document Number: 164
Source: https://cookbook.arweave.net/kits/svelte/index.html
Words: 115
Extraction Method: html

Svelte Starter Kits Svelte is a framework that compiles to a JavaScript bundle and in the process removes the framework from the distribution of the app. This results in a much smaller footprint than other frameworks. Svelte is the perfect framework for Permaweb Applications. A Permaweb Application is built on the principles of a Single Page Application, but lives on the Arweave network and is distributed by Permaweb gateways.Svelte Starter Kit Guides:Minimal - the minimum required to build a svelte permaweb app Vite - Svelte, Typescript and Vite Permaweb Application Constraints 100% Front-end application (No Server-Side Backend) Applications are served from a sub-path (https://[gateway]/[TX_ID]) Built with  ❤️  by the Arweave community. Learn more at  Arweave.org

---

# 165. ANT Configuration - ARIO Docs

Document Number: 165
Source: https://docs.ar.io/ar-io-sdk/ants/configuration
Words: 98
Extraction Method: html

init init is a factory function that creates a read-only or writable client. By providing a signer, additional write APIs that require signing (like setRecord and transfer) become available. By default, a read-only client is returned and no write APIs are available.Parameters ← Swipe to see more → Parameter Type Description Optional processId String The AO process ID of the ANT to connect to.false process AOProcess A pre-configured AOProcess instance used to initialize the ANT class true signer ContractSigner An optional signer instance, used to enable write operations on the
blockchain true ← Swipe to see more →

---

# 166. ARIO Configuration - ARIO Docs

Document Number: 166
Source: https://docs.ar.io/ar-io-sdk/ario/configuration
Words: 96
Extraction Method: html

init init is a factory function that creates a read-only or writable client. By providing a signer, additional write APIs that require signing (like buyRecord and transfer) become available. By default, a read-only client is returned and no write APIs are available.Parameters ← Swipe to see more → Parameter Type Description Optional process AOProcess A pre-configured AOProcess instance used to initialize the ARIO class true processId string The process ID of the AO process true signer ContractSigner An optional signer instance, used to enable write operations on the
blockchain true ← Swipe to see more →

---

# 167. getArNSReservedNames - ARIO Docs

Document Number: 167
Source: https://docs.ar.io/ar-io-sdk/ario/arns/get-arns-reserved-names
Words: 94
Extraction Method: html

getArNSReservedNames is a method on the ARIO class that retrieves all reserved ArNS names, with support for pagination and custom sorting. Reserved names are names that are protected and cannot be registered by users.getArNSReservedNames does not require authentication.Parameters Parameter Type Description Optional Default cursor string The name to use as the starting point for the next page of results true None limit number The maximum number of records to return (max: 1000) true 100 sortBy string The property to sort results by true name sortOrder string The sort direction ('desc' or 'asc') true asc

---

# 168. getArNSRecords - ARIO Docs

Document Number: 168
Source: https://docs.ar.io/ar-io-sdk/ario/arns/get-arns-records
Words: 92
Extraction Method: html

getArNSRecords is a method on the ARIO class that retrieves all ArNS records with optional pagination and filtering support.getArNSRecords does not require authentication.Parameters ← Swipe to see more → Parameter Type Description Optional Default cursor string The ArNS name to use as the starting point for the next page of results true None limit number The maximum number of records to return (max: 1000) true 100 sortBy string The property to sort results by true startTimestamp sortOrder string The sort direction ('desc' or 'asc') true desc ← Swipe to see more →

---

# 169. Hello World (No Code)  Cooking with the Permaweb

Document Number: 169
Source: https://cookbook.arweave.net/getting-started/quick-starts/hw-no-code.html
Words: 90
Extraction Method: html

Hello World (No Code) In this quick start we are going to upload an image to the Permaweb with no code.Requirements Computer Internet Modern web browser Create a wallet https://arweave.app/add open in new window or https://wander.app open in new window Send some data to Arweave Go to https://hello_cookbook.arweave.dev open in new window Enter some data and click publish, connect your wallet and "BAM" Congratulations!You just published some data on Arweave using zero code.To check out the project -> https://github.com/twilson63/pw-no-code-hello Built with  ❤️  by the Arweave community. Learn more at  Arweave.org

---

# 170. getGateways - ARIO Docs

Document Number: 170
Source: https://docs.ar.io/ar-io-sdk/ario/gateways/get-gateways
Words: 87
Extraction Method: html

getGateways is a method on the ARIO class that retrieves all gateways with optional pagination and filtering support.getGateways does not require authentication.Parameters ← Swipe to see more → Parameter Type Description Optional Default cursor string The gateway address to use as the starting point for pagination true None limit number The maximum number of gateways to return (max: 1000) true 100 sortBy string The property to sort gateways by true startTimestamp sortOrder string The sort direction ('desc' or 'asc') true desc ← Swipe to see more →

---

# 171. process10 - HyperBEAM - Documentation

Document Number: 171
Source: https://hyperbeam.arweave.net/build/devices/process-at-1-0.html
Words: 549
Extraction Method: html

Device: ~process@1.0 Overview The ~process@1.0 device represents a persistent, shared execution environment within HyperBEAM, analogous to a process or actor in other systems. It allows for stateful computation and interaction over time.Core Concept: Orchestration A message tagged with Device: process@1.0 (the "Process Definition Message") doesn't typically perform computation itself. Instead, it defines which other devices should be used for key aspects of its lifecycle:Scheduler Device: Determines the order of incoming messages (assignments) to be processed. (Defaults to ~scheduler@1.0).Execution Device: Executes the actual computation based on the current state and the scheduled message. Often configured as dev_stack to allow multiple computational steps (e.g., running WASM, applying cron jobs, handling proofs).Push Device: Handles the injection of new messages into the process's schedule. (Defaults to ~push@1.0).The ~process@1.0 device acts as a router, intercepting requests and delegating them to the appropriate configured device (scheduler, executor, etc.) by temporarily swapping the device tag on the message before resolving.Key Functions (Keys) These keys are accessed via an HTTP path relative to the Process Definition Message ID (<ProcessID>).GET /<ProcessID>~process@1.0/schedule Action: Delegates to the configured Scheduler Device (via the process's schedule/3 function) to retrieve the current schedule or state.Response: Depends on the Scheduler Device implementation (e.g., list of message IDs).POST /<ProcessID>~process@1.0/schedule Action: Delegates to the configured Push Device (via the process's push/3 function) to add a new message to the process's schedule.Request Body: The message to be added.Response: Confirmation or result from the Push Device.GET /<ProcessID>~process@1.0/compute/<TargetSlotOrMsgID> Action: Computes the process state up to a specific point identified by <TargetSlotOrMsgID> (either a slot number or a message ID within the schedule). It retrieves assignments from the Scheduler Device and applies them sequentially using the configured Execution Device.Response: The process state message after executing up to the target slot/message.Caching: Results are cached aggressively (see dev_process_cache) to avoid recomputation.GET /<ProcessID>~process@1.0/now Action: Computes and returns the Results key from the latest known state of the process. This typically involves computing all pending assignments.Response: The value of the Results key from the final state.GET /<ProcessID>~process@1.0/slot Action: Delegates to the configured Scheduler Device to query information about a specific slot or the current slot number.Response: Depends on the Scheduler Device implementation.GET /<ProcessID>~process@1.0/snapshot Action: Delegates to the configured Execution Device to generate a snapshot of the current process state. This often involves running the execution stack in a specific "map" mode to gather state from different components.Response: A message representing the process snapshot, often marked for caching.Process Definition Example A typical process definition message might look like this (represented conceptually):Device: process@1.0
Scheduler-Device: [`scheduler@1.0`](./source-code/dev_scheduler.md)
Execution-Device: [`stack@1.0`](./source-code/dev_stack.md)
Execution-Stack: "[`scheduler@1.0`](./source-code/dev_scheduler.md)", "[`cron@1.0`](./source-code/dev_cron.md)", "[`wasm64@1.0`](./source-code/dev_wasm.md)", "[`PoDA@1.0`](./source-code/dev_poda.md)"
Cron-Frequency: 10-Minutes
WASM-Image: <WASMImageTxID>
PoDA:
    Device: [`PoDA/1.0`](./source-code/dev_poda.md)
    Authority: <AddressA>
    Authority: <AddressB>
    Quorum: 2 This defines a process that uses:
* The standard scheduler.
* A stack executor that runs scheduling logic, cron jobs, a WASM module, and a Proof-of-Data-Availability check.State Management & Caching ~process@1.0 relies heavily on caching (dev_process_cache) to optimize performance. Full state snapshots and intermediate results are cached periodically (configurable via Cache-Frequency and Cache-Keys options) to avoid recomputing the entire history for every request.Initialization (init) Processes often require an initialization step before they can process messages. This is typically triggered by calling the init key on the configured Execution Device via the process path (/<ProcessID>~process@1.0/init). This allows components within the execution stack (like WASM modules) to set up their initial state.process module

---

# 172. approvePrimaryNameRequest - ARIO Docs

Document Number: 172
Source: https://docs.ar.io/ar-io-sdk/ants/approve-primary-name-request
Words: 85
Extraction Method: html

approvePrimaryNameRequest is a method on the ANT class that approves a primary name request for a given name or address.approvePrimaryNameRequest requires authentication.Parameters ← Swipe to see more → Parameter Type Description Optional name string ArNS name to approve as primary name.false address string - WalletAddress Public wallet address that made the primary name request being approved.false ioProcessId string Process Id of the ARIO contract.false tags array An array of GQL tag objects to attach to the transfer AO message.true ← Swipe to see more →

---

# 173. BetterIDEa  Cookbook

Document Number: 173
Source: https://cookbook_ao.arweave.net/references/betteridea/index.html
Words: 84
Extraction Method: html

Skip to content  BetterIDEa BetterIDEa is a custom web based IDE for developing on ao.It offers a built in Lua language server with ao definitions, so you don't need to install anything. Just open the IDE and start coding!Features include:Code completion Cell based notebook ui for rapid development Easy process management Markdown and Latex cell support Share projects with anyone through ao processes Tight integration with ao package manager Read detailed information about the various features and integrations of the IDE in the documentation.

---

# 174. addController - ARIO Docs

Document Number: 174
Source: https://docs.ar.io/ar-io-sdk/ants/add-controller
Words: 83
Extraction Method: html

addController is a method on the ANT class that adds a new controller to the ANT's list of approved controllers. Controllers have permissions to set records and modify the ANT process's ticker and name.addController requires authentication.Parameters ← Swipe to see more → Parameter Type Description Optional controller string - WalletAddress The public wallet address of the controller to be added false tags array An array of GQL tag objects to attach to the transfer AO message true ← Swipe to see more →

---

# 175. cancelWithdrawal - ARIO Docs

Document Number: 175
Source: https://docs.ar.io/ar-io-sdk/ario/gateways/cancel-withdrawal
Words: 82
Extraction Method: html

cancelWithdrawal is a method on the ARIO class that cancels a pending withdrawal for a gateway, returning the stake back to the delegated amount.cancelWithdrawal requires authentication.Parameters ← Swipe to see more → Parameter Type Description Optional gatewayAddress string - WalletAddress The wallet address of the gateway false vaultId string The ID of the vault containing the withdrawal to cancel false tags array An array of GQL tag objects to attach to the transfer AO message true ← Swipe to see more →

---

# 176. upgradeRecord - ARIO Docs

Document Number: 176
Source: https://docs.ar.io/ar-io-sdk/ario/arns/upgrade-record
Words: 76
Extraction Method: html

upgradeRecord is a method on the ARIO class that upgrades an existing ArNS record from a lease to a permanent ownership (permabuy). This allows converting a leased name to permanent ownership.upgradeRecord requires authentication.Parameters Parameter Type Description Optional name string The ArNS name to upgrade to permanent ownership false fundFrom string The source of funds: 'balance', 'stakes', 'any', or 'turbo' true tags array An array of GQL tag objects to attach to the transfer AO message true

---

# 177. ARIO Docs

Document Number: 177
Source: https://docs.ar.io/ar-io-sdk/ants/release-name
Words: 71
Extraction Method: html

releaseName releaseName is a method on the ANT class that releases an ArNS name from the ANT, making it available for auction on the ARIO contract. The name must be permanently owned by the releasing wallet. Upon successful auction, 50% of the winning bid will be distributed to the ANT owner at the time of release. If there are no bids, the name becomes available for anyone to register.releaseName requires authentication.

---

# 178. getTokenCost - ARIO Docs

Document Number: 178
Source: https://docs.ar.io/ar-io-sdk/ario/arns/get-token-cost
Words: 71
Extraction Method: html

getTokenCost is a method on the ARIO class that calculates the cost in mARIO tokens for various ArNS operations such as buying records, extending leases, increasing undername limits, upgrading to permabuy, and requesting primary names.getTokenCost does not require authentication.Parameters The getTokenCost method accepts different parameter sets depending on the intent (the specific action you want to check the cost for). Each intent requires a different combination of parameters as outlined below:

---

# 179. ariowayfinder-react - ARIO Docs

Document Number: 179
Source: https://docs.ar.io/wayfinder/react
Words: 190
Extraction Method: html

Overview The @ar.io/wayfinder-react package provides React-specific components and hooks for integrating Wayfinder into React applications. It offers a provider pattern for configuration and convenient hooks for fetching data with built-in loading states and error handling.Installation Quick Start 1. Add the Wayfinder Context Provider 2. Use Hooks in Your Components Advanced Configuration import { WayfinderProvider } from '@ar.io/wayfinder-react'
import {
  NetworkGatewaysProvider,
  PreferredWithFallbackRoutingStrategy,
  FastestPingRoutingStrategy,
  HashVerificationStrategy,
  StaticGatewaysProvider,
} from '@ar.io/wayfinder-core'
import { ARIO } from '@ar.io/sdk'

function App() {
  return (
    <WayfinderProvider
      gatewaysProvider={new NetworkGatewaysProvider({ ario: ARIO.mainnet() })}
      routingSettings={{
        strategy: new PreferredWithFallbackRoutingStrategy({
          preferredGateway: 'https://my-gateway.com',
          fallbackStrategy: new FastestPingRoutingStrategy({ timeoutMs: 500 }),
        }),
        events: {
          onRoutingSucceeded: (event) =>
            console.log('Gateway selected:', event.selectedGateway),
          onRoutingFailed: (error) => console.error('Routing failed:', error),
        },
      }}
      verificationSettings={{
        enabled: true,
        strategy: new HashVerificationStrategy({
          trustedGateways: ['https://arweave.net'],
        }),
        strict: false,
        events: {
          onVerificationSucceeded: (event) =>
            console.log('Verified:', event.txId),
          onVerificationFailed: (error) =>
            console.error('Verification failed:', error),
        },
      }}
      telemetrySettings={{
        enabled: process.env.NODE_ENV === 'production',
        clientName: 'my-react-app',
        clientVersion: process.env.REACT_APP_VERSION || '1.0.0',
        sampleRate: 0.05,
        exporterUrl: process.env.REACT_APP_TELEMETRY_URL,
      }}
    >
      <YourApp />
    </WayfinderProvider>
  )
} Related useWayfinder: Access the complete Wayfinder instance useWayfinderRequest: Direct access to the request function useWayfinderUrl: URL resolution with loading states For more advanced configuration options, see the Core Documentation.

---

# 180. setKeywords - ARIO Docs

Document Number: 180
Source: https://docs.ar.io/ar-io-sdk/ants/set-keywords
Words: 66
Extraction Method: html

setKeywords is a method on the ANT class that updates the list of keywords associated with the ANT process.setKeywords requires authentication.Parameters ← Swipe to see more → Parameter Type Description Optional keywords array An array of keywords to associate with the ANT process false tags array An array of GQL tag objects to attach to the transfer AO message true ← Swipe to see more →

---

# 181. transfer - ARIO Docs

Document Number: 181
Source: https://docs.ar.io/ar-io-sdk/ants/transfer
Words: 65
Extraction Method: html

transfer is a method on the ANT class that transfers ownership of the ANT process to another wallet address.transfer requires authentication.Parameters ← Swipe to see more → Parameter Type Description Optional target string - WalletAddress The wallet address to transfer ownership to false tags array An array of GQL tag objects to attach to the transfer AO message true ← Swipe to see more →

---

# 182. ARIO Docs

Document Number: 182
Source: https://docs.ar.io/wayfinder/core/gateway-providers/network
Words: 96
Extraction Method: html

NetworkGatewaysProvider Overview The NetworkGatewaysProvider discovers AR.IO gateways from the AR.IO Network using the AR.IO SDK. It provides dynamic access to the full network of verified gateways, making it the recommended choice for production applications.Important To avoid rate limits and improve performance, consider wrapping NetworkGatewaysProvider with SimpleCacheGatewaysProvider (for Node.js/server environments) or LocalStorageGatewaysProvider (for browser environments). This enables caching of gateway lists and reduces unnecessary network requests.Basic Usage Configuration Options NetworkGatewaysProviderOptions Related Documentation Gateway Providers Overview: Compare all gateway providers StaticGatewaysProvider: Static gateway configuration SimpleCacheGatewaysProvider: Caching wrapper Wayfinder Configuration: Main wayfinder setup Routing Strategies: How gateways are selected

---

# 183. Module hb_singletonerl - HyperBEAM - Documentation

Document Number: 183
Source: https://hyperbeam.arweave.net/build/devices/source-code/hb_singleton.html
Words: 1095
Extraction Method: html

Module hb_singleton.erl A parser that translates AO-Core HTTP API requests in TABM format
into an ordered list of messages to evaluate.Description The details of this format
are described in docs/ao-core-http-api.md.Syntax overview:Singleton: Message containing keys and a <code>path</code> field,
                  which may also contain a query string of key-value pairs.
       Path:
           - /Part1/Part2/.../PartN/ => [Part1, Part2, ..., PartN]
           - /ID/Part2/.../PartN => [ID, Part2, ..., PartN]
       Part: (Key + Resolution), Device?, #{ K => V}?
           - Part => #{ path => Part }
           - <code>Part&Key=Value => #{ path => Part, Key => Value }</code>
           - <code>Part&Key => #{ path => Part, Key => true }</code>
           - <code>Part&k1=v1&k2=v2 => #{ path => Part, k1 => `<<"v1">></code>, k2 => <code><<"v2">></code> }'
           - <code>Part~Device => {as, Device, #{ path => Part }}</code>
           - <code>Part~D&K1=V1 => {as, D, #{ path => Part, K1 => `<<"v1">></code> }}'
           - <code>pt&k1+int=1 => #{ path => pt, k1 => 1 }</code>
           - <code>pt~d&k1+int=1 => {as, d, #{ path => pt, k1 => 1 }}</code>
           - <code>(/nested/path) => Resolution of the path /nested/path</code>
           - <code>(/nested/path&k1=v1) => (resolve /nested/path)#{k1 => v1}</code>
           - <code>(/nested/path~D&K1=V1) => (resolve /nested/path)#{K1 => V1}</code>
           - <code>pt&k1+res=(/a/b/c) => #{ path => pt, k1 => (resolve /a/b/c) }</code>
       Key:
           - key: <code><<"value">></code> => #{ key => <code><<"value">></code>, ... } for all messages
           - n.key: <code><<"value">></code> => #{ key => <code><<"value">></code>, ... } for Nth message
           - key+int: 1 => #{ key => 1, ... }
           - key+res: /nested/path => #{ key => (resolve /nested/path), ... }
           - N.Key+res=(/a/b/c) => #{ Key => (resolve /a/b/c), ... } Data Types ao_message() ao_message() = map() | binary() tabm_message() tabm_message() = map() Function Index all_path_parts/2* Extract all of the parts from the binary, given (a list of) separators.append_path/2*  apply_types/2* Step 3: Apply types to values and remove specifiers.basic_hashpath_test/0*  basic_hashpath_to_test/0*  build_messages/3* Step 5: Merge the base message with the scoped messages.decode_string/1* Attempt Cowboy URL decode, then sanitize the result.do_build/4*  from/2 Normalize a singleton TABM message into a list of executable AO-Core
messages.group_scoped/2* Step 4: Group headers/query by N-scope.inlined_keys_test/0*  inlined_keys_to_test/0*  maybe_join/2* Join a list of items with a separator, or return the first item if there
is only one item.maybe_subpath/2* Check if the string is a subpath, returning it in parsed form,
or the original string with a specifier.maybe_typed/3* Parse a key's type (applying it to the value) and device name if present.multiple_inlined_keys_test/0*  multiple_inlined_keys_to_test/0*  multiple_messages_test/0*  multiple_messages_to_test/0*  normalize_base/1* Normalize the base path.parse_explicit_message_test/0*  parse_full_path/1* Parse the relative reference into path, query, and fragment.parse_inlined_key_val/2* Extrapolate the inlined key-value pair from a path segment.parse_part/2* Parse a path part into a message or an ID.parse_part_mods/3* Parse part modifiers:
1.parse_scope/1* Get the scope of a key.part/2* Extract the characters from the binary until a separator is found.part/4*  path_messages/2* Step 2: Decode, split and sanitize the path.path_parts/2* Split the path into segments, filtering out empty segments and
segments that are too long.path_parts_test/0*  scoped_key_test/0*  scoped_key_to_test/0*  simple_to_test/0*  single_message_test/0*  subpath_in_inlined_test/0*  subpath_in_inlined_to_test/0*  subpath_in_key_test/0*  subpath_in_key_to_test/0*  subpath_in_path_test/0*  subpath_in_path_to_test/0*  to/1 Convert a list of AO-Core message into TABM message.to_suite_test_/0*  type/1*  typed_key_test/0*  typed_key_to_test/0*  Function Details all_path_parts/2 * all_path_parts(Sep, Bin) -> any() Extract all of the parts from the binary, given (a list of) separators.append_path/2 * append_path(PathPart, Message) -> any() apply_types/2 * apply_types(Msg, Opts) -> any() Step 3: Apply types to values and remove specifiers.basic_hashpath_test/0 * basic_hashpath_test() -> any() basic_hashpath_to_test/0 * basic_hashpath_to_test() -> any() build_messages/3 * build_messages(Msgs, ScopedModifications, Opts) -> any() Step 5: Merge the base message with the scoped messages.decode_string/1 * decode_string(B) -> any() Attempt Cowboy URL decode, then sanitize the result.do_build/4 * do_build(I, Rest, ScopedKeys, Opts) -> any() from/2 from(RawMsg, Opts) -> any() Normalize a singleton TABM message into a list of executable AO-Core
messages.group_scoped/2 * group_scoped(Map, Msgs) -> any() Step 4: Group headers/query by N-scope.N.Key => applies to Nth step. Otherwise => global inlined_keys_test/0 * inlined_keys_test() -> any() inlined_keys_to_test/0 * inlined_keys_to_test() -> any() maybe_join/2 * maybe_join(Items, Sep) -> any() Join a list of items with a separator, or return the first item if there
is only one item. If there are no items, return an empty binary.maybe_subpath/2 * maybe_subpath(Str, Opts) -> any() Check if the string is a subpath, returning it in parsed form,
or the original string with a specifier.maybe_typed/3 * maybe_typed(Key, Value, Opts) -> any() Parse a key's type (applying it to the value) and device name if present.
We allow characters as type indicators because some URL-string encoders
(e.g. Chrome) will encode `+` characters in a form that query-string parsers
interpret as characters.multiple_inlined_keys_test/0 * multiple_inlined_keys_test() -> any() multiple_inlined_keys_to_test/0 * multiple_inlined_keys_to_test() -> any() multiple_messages_test/0 * multiple_messages_test() -> any() multiple_messages_to_test/0 * multiple_messages_to_test() -> any() normalize_base/1 * normalize_base(Rest) -> any() Normalize the base path.parse_explicit_message_test/0 * parse_explicit_message_test() -> any() parse_full_path/1 * parse_full_path(RelativeRef) -> any() Parse the relative reference into path, query, and fragment.parse_inlined_key_val/2 * parse_inlined_key_val(Bin, Opts) -> any() Extrapolate the inlined key-value pair from a path segment. If the
key has a value, it may provide a type (as with typical keys), but if a
value is not provided, it is assumed to be a boolean true.parse_part/2 * parse_part(ID, Opts) -> any() Parse a path part into a message or an ID.
Applies the syntax rules outlined in the module doc, in the following order:
1. ID
2. Part subpath resolutions
3. Inlined key-value pairs
4. Device specifier parse_part_mods/3 * parse_part_mods(X1, Msg, Opts) -> any() Parse part modifiers:
1. ~Device => {as, Device, Msg} 2. &K=V => Msg#{ K => V } parse_scope/1 * parse_scope(KeyBin) -> any() Get the scope of a key. Adds 1 to account for the base message.part/2 * part(Sep, Bin) -> any() Extract the characters from the binary until a separator is found.
The first argument of the function is an explicit separator character, or
a list of separator characters. Returns a tuple with the separator, the
accumulated characters, and the rest of the binary.part/4 * part(Seps, X2, Depth, CurrAcc) -> any() path_messages/2 * path_messages(RawBin, Opts) -> any() Step 2: Decode, split and sanitize the path. Split by / but avoid
subpath components, such that their own path parts are not dissociated from
their parent path.path_parts/2 * path_parts(Sep, PathBin) -> any() Split the path into segments, filtering out empty segments and
segments that are too long.path_parts_test/0 * path_parts_test() -> any() scoped_key_test/0 * scoped_key_test() -> any() scoped_key_to_test/0 * scoped_key_to_test() -> any() simple_to_test/0 * simple_to_test() -> any() single_message_test/0 * single_message_test() -> any() subpath_in_inlined_test/0 * subpath_in_inlined_test() -> any() subpath_in_inlined_to_test/0 * subpath_in_inlined_to_test() -> any() subpath_in_key_test/0 * subpath_in_key_test() -> any() subpath_in_key_to_test/0 * subpath_in_key_to_test() -> any() subpath_in_path_test/0 * subpath_in_path_test() -> any() subpath_in_path_to_test/0 * subpath_in_path_to_test() -> any() to/1 to(Messages::[ao_message()]) -> tabm_message()  Convert a list of AO-Core message into TABM message.to_suite_test_/0 * to_suite_test_() -> any() type/1 * type(Value) -> any() typed_key_test/0 * typed_key_test() -> any() typed_key_to_test/0 * typed_key_to_test() -> any()

---

# 184. Permaweb Cookbook - Guides  Cooking with the Permaweb

Document Number: 184
Source: https://cookbook.arweave.net/guides/index.html
Words: 59
Extraction Method: html

Guides Snack-sized guides for the building blocks of the Permaweb DNS Integration Server Side Deploying Apps arkb github-action Deploying PathManifests arweave.app ardrive GraphQL ArDB ar-gql Search Service Testing Do you think a permaweb guide is missing? Create a issue at Github open in new window or consider contributing Built with  ❤️  by the Arweave community. Learn more at  Arweave.org

---

# 185. ARIO Docs

Document Number: 185
Source: https://docs.ar.io/ar-io-sdk/ario/gateways/get-delegations
Words: 56
Extraction Method: html

getDelegations getDelegations is a method on the ARIO class that retrieves all active and vaulted stakes across all gateways for a specific address. Results are paginated and sorted by the specified criteria. The cursor parameter represents the last delegationId (a combination of gateway address and delegation start timestamp) from the previous request.getDelegations does not require authentication.

---

# 186. ARIO Docs

Document Number: 186
Source: https://docs.ar.io/ar-io-sdk/ario/arns/get-demand-factor
Words: 52
Extraction Method: html

getDemandFactor getDemandFactor is a method on the ARIO class that retrieves the current network demand factor. This factor is a dynamic multiplier that adjusts the cost of ArNS interactions based on network demand - higher demand results in higher costs, and vice versa.getDemandFactor does not require authentication.Parameters getDemandFactor does not accept parameters.

---

# 187. ARIO Docs

Document Number: 187
Source: https://docs.ar.io/ar-io-sdk/ario/gateways/leave-network
Words: 51
Extraction Method: html

leaveNetwork leaveNetwork is a method on the ARIO class that sets a gateway's status to leaving on the ar.io network. The gateway's operator and delegate stakes are vaulted and will be returned after the leave period. The gateway will be removed from the network once the leave period ends.leaveNetwork requires authentication.

---

# 188. ARIO Docs

Document Number: 188
Source: https://docs.ar.io/ar-io-sdk/ario/arns/get-arns-returned-names
Words: 49
Extraction Method: html

getArNSReturnedNames getArNSReturnedNames is a method on the ARIO class that retrieves all currently active returned ArNS names, with support for pagination and custom sorting. Pagination is handled using a cursor system, where the cursor is the name from the last record of the previous request.getArNSReturnedNames does not require authentication.

---

# 189. getBalance - ARIO Docs

Document Number: 189
Source: https://docs.ar.io/ar-io-sdk/ants/get-balance
Words: 46
Extraction Method: html

getBalance is a method on the ANT class that retrieves the token balance for a specific wallet address within the ANT process.getBalance does not require authentication.Parameters Parameter Type Description Optional address string - WalletAddress The wallet address to retrieve the balance for false Examples Output 1

---

# 190. ARIO Docs

Document Number: 190
Source: https://docs.ar.io/ar-io-sdk/ario/gateways/get-redelegation-fee
Words: 45
Extraction Method: html

getRedelegationFee getRedelegationFee is a method on the ARIO class that retrieves the redelegation fee rate as a percentage for a specific address. The fee rate ranges from 0% to 60% based on the number of redelegations since the last fee reset.getRedelegationFee does not require authentication.

---

# 191. getArNSReservedName - ARIO Docs

Document Number: 191
Source: https://docs.ar.io/ar-io-sdk/ario/arns/get-arns-reserved-name
Words: 43
Extraction Method: html

getArNSReservedName is a method on the ARIO class that retrieves details about a specific reserved ArNS name, including its target and any expiration information.getArNSReservedName does not require authentication.Parameters Parameter Type Description Optional name string The reserved ArNS name to retrieve information for false

---

# 192. ARIO Docs

Document Number: 192
Source: https://docs.ar.io/ar-io-sdk/ario/gateways/get-gateway-delegates
Words: 42
Extraction Method: html

getGatewayDelegates getGatewayDelegates is a method on the ARIO class that retrieves all delegates for a specific gateway. Results are paginated and sorted by the specified criteria. The cursor parameter represents the last delegate address from the previous request.getGatewayDelegates does not require authentication.

---

# 193. getDemandFactorSettings - ARIO Docs

Document Number: 193
Source: https://docs.ar.io/ar-io-sdk/ario/arns/get-demand-factor-settings
Words: 40
Extraction Method: html

getDemandFactorSettings is a method on the ARIO class that retrieves the configuration settings for the demand factor algorithm, which dynamically adjusts ArNS registration costs based on network demand.getDemandFactorSettings does not require authentication.Parameters The getDemandFactorSettings method does not accept any parameters.

---

# 194. ARIO Docs

Document Number: 194
Source: https://docs.ar.io/wayfinder/core/gateway-providers/simple-cache
Words: 76
Extraction Method: html

SimpleCacheGatewaysProvider Overview The SimpleCacheGatewaysProvider holds the resulting gateways in memory for the provided TTL, making it ideal for Node environments. This helps avoid rate-limits and unnecessary network requests to the underlying gateways provider.Important SimpleCacheGatewaysProvider is ideal for Node.js/server environments. For browser-based web applications, use LocalStorageGatewaysProvider instead to persist gateway lists across sessions.Basic Usage Configuration Options Related Documentation Gateway Providers: Compare all gateway providers NetworkGatewaysProvider: Dynamic network discovery StaticGatewaysProvider: Static gateway configuration Wayfinder Configuration: Main wayfinder setup

---

# 195. ARIO Docs

Document Number: 195
Source: https://docs.ar.io/ar-io-sdk/ario/gateways/get-allowed-delegates
Words: 39
Extraction Method: html

getAllowedDelegates getAllowedDelegates is a method on the ARIO class that retrieves all allowed delegates for a specific gateway address. The cursor parameter is used for pagination and represents the last address from the previous request.getAllowedDelegates does not require authentication.

---

# 196. getRecords - ARIO Docs

Document Number: 196
Source: https://docs.ar.io/ar-io-sdk/ants/get-records
Words: 38
Extraction Method: html

getRecords is a method on the ANT class that retrieves all the records stored in the ANT process, including both base name records and undername records.getRecords does not require authentication.Parameters The getRecords method does not accept any parameters.

---

# 197. getRegistrationFees - ARIO Docs

Document Number: 197
Source: https://docs.ar.io/ar-io-sdk/ario/arns/get-registration-fees
Words: 37
Extraction Method: html

getRegistrationFees is a method on the ARIO class that retrieves the current registration fee structure for ArNS names, including base fees and any applicable multipliers.getRegistrationFees does not require authentication.Parameters The getRegistrationFees method does not accept any parameters.

---

# 198. getState - ARIO Docs

Document Number: 198
Source: https://docs.ar.io/ar-io-sdk/ants/get-state
Words: 36
Extraction Method: html

getState is a method on the ANT class that retrieves the complete state of the ANT process, including all records, balances, controllers, and metadata.getState does not require authentication.Parameters The getState method does not accept any parameters.

---

# 199. getInfo - ARIO Docs

Document Number: 199
Source: https://docs.ar.io/ar-io-sdk/ants/get-info
Words: 35
Extraction Method: html

getInfo is a method on the ANT class that retrieves general information about the ANT process, including name, ticker, denomination, and other metadata.getInfo does not require authentication.Parameters The getInfo method does not accept any parameters.

---

# 200. getArNSRecord - ARIO Docs

Document Number: 200
Source: https://docs.ar.io/ar-io-sdk/ario/arns/get-arns-record
Words: 34
Extraction Method: html

getArNSRecord is a method on the ARIO class that retrieves the details of a specific ArNS record by its name. This includes lease information, type, process ID, and other metadata.getArNSRecord does not require authentication.

---

# 201. ARIO Docs

Document Number: 201
Source: https://docs.ar.io/ar-io-sdk/ario/arns/get-arns-returned-name
Words: 34
Extraction Method: html

getArNSReturnedName getArNSReturnedName is a method on the ARIO class that retrieves information about an ArNS name that has been returned to the protocol, including its auction settings and timing details.getArNSReturnedName does not require authentication.

---

# 202. Module dev_metaerl - HyperBEAM - Documentation

Document Number: 202
Source: https://hyperbeam.arweave.net/build/devices/source-code/dev_meta.html
Words: 1162
Extraction Method: html

Module dev_meta.erl The hyperbeam meta device, which is the default entry point
for all messages processed by the machine.Description This device executes a
AO-Core singleton request, after first applying the node's
pre-processor, if set. The pre-processor can halt the request by
returning an error, or return a modified version if it deems necessary --
the result of the pre-processor is used as the request for the AO-Core
resolver. Additionally, a post-processor can be set, which is executed after
the AO-Core resolver has returned a result.Function Index add_dynamic_keys/1* Add dynamic keys to the node message.add_identity_addresses/1*  adopt_node_message/2 Attempt to adopt changes to a node message. Test that we can set the node message if the request is signed by the
owner of the node.build/3 Emits the version number and commit hash of the HyperBEAM node source,
if available.buildinfo_test/0* Test that version information is available and returned correctly.claim_node_test/0* Test that we can claim the node correctly and set the node message after.config_test/0* Test that we can get the node message.embed_status/2* Wrap the result of a device call in a status.filter_node_msg/2* Remove items from the node message that are not encodable into a
message.halt_request_test/0* Test that we can halt a request if the hook returns an error.handle/2 Normalize and route messages downstream based on their path.handle_initialize/2*  handle_resolve/3* Handle an AO-Core request, which is a list of messages.info/1 Ensure that the helper function adopt_node_message/2 is not exported.info/3 Get/set the node message.is/2 Check if the request in question is signed by a given role on the node.is/3  is_operator/2 Utility function for determining if a request is from the operator of
the node.maybe_sign/2* Sign the result of a device call if the node is configured to do so.message_to_status/2* Get the HTTP status code from a transaction (if it exists).modify_request_test/0* Test that a hook can modify a request.permanent_node_message_test/0* Test that a permanent node message cannot be changed.priv_inaccessible_test/0* Test that we can't get the node message if the requested key is private.request_response_hooks_test/0*  resolve_hook/4* Execute a hook from the node message upon the user's request.status_code/2* Calculate the appropriate HTTP status code for an AO-Core result. Test that we can't set the node message if the request is not signed by
the owner of the node.uninitialized_node_test/0* Test that an uninitialized node will not run computation.update_node_message/2* Validate that the request is signed by the operator of the node, then
allow them to update the node message.Function Details add_dynamic_keys/1 * add_dynamic_keys(NodeMsg) -> any() Add dynamic keys to the node message.add_identity_addresses/1 * add_identity_addresses(NodeMsg) -> any() adopt_node_message/2 adopt_node_message(Request, NodeMsg) -> any() Attempt to adopt changes to a node message.authorized_set_node_msg_succeeds_test/0 * authorized_set_node_msg_succeeds_test() -> any() Test that we can set the node message if the request is signed by the
owner of the node.build/3 build(X1, X2, NodeMsg) -> any() Emits the version number and commit hash of the HyperBEAM node source,
if available.We include the short hash separately, as the length of this hash may change in
the future, depending on the git version/config used to build the node.
Subsequently, rather than embedding the git-short-hash-length, for the
avoidance of doubt, we include the short hash separately, as well as its long
hash.buildinfo_test/0 * buildinfo_test() -> any() Test that version information is available and returned correctly.claim_node_test/0 * claim_node_test() -> any() Test that we can claim the node correctly and set the node message after.config_test/0 * config_test() -> any() Test that we can get the node message.embed_status/2 * embed_status(X1, NodeMsg) -> any() Wrap the result of a device call in a status.filter_node_msg/2 * filter_node_msg(Msg, NodeMsg) -> any() Remove items from the node message that are not encodable into a
message.halt_request_test/0 * halt_request_test() -> any() Test that we can halt a request if the hook returns an error.handle/2 handle(NodeMsg, RawRequest) -> any() Normalize and route messages downstream based on their path. Messages
with a Meta key are routed to the handle_meta/2 function, while all
other messages are routed to the handle_resolve/2 function.handle_initialize/2 * handle_initialize(Rest, NodeMsg) -> any() handle_resolve/3 * handle_resolve(Req, Msgs, NodeMsg) -> any() Handle an AO-Core request, which is a list of messages. We apply
the node's pre-processor to the request first, and then resolve the request
using the node's AO-Core implementation if its response was ok.
After execution, we run the node's response hook on the result of
the request before returning the result it grants back to the user.info/1 info(X1) -> any() Ensure that the helper function adopt_node_message/2 is not exported.
The naming of this method carefully avoids a clash with the exported info/3 function. We would like the node information to be easily accessible via the info endpoint, but AO-Core also uses info as the name of the function
that grants device information. The device call takes two or fewer arguments,
so we are safe to use the name for both purposes in this case, as the user
info call will match the three-argument version of the function. If in the
future the request is added as an argument to AO-Core's internal info function, we will need to find a different approach.info/3 info(X1, Request, NodeMsg) -> any() Get/set the node message. If the request is a POST, we check that the
request is signed by the owner of the node. If not, we return the node message
as-is, aside all keys that are private (according to hb_private).is/2 is(Request, NodeMsg) -> any() Check if the request in question is signed by a given role on the node.
The role can be one of operator or initiator.is/3 is(X1, Request, NodeMsg) -> any() is_operator/2 is_operator(Request, NodeMsg) -> any() Utility function for determining if a request is from the operator of
the node.maybe_sign/2 * maybe_sign(Res, NodeMsg) -> any() Sign the result of a device call if the node is configured to do so.message_to_status/2 * message_to_status(Item, NodeMsg) -> any() Get the HTTP status code from a transaction (if it exists).modify_request_test/0 * modify_request_test() -> any() Test that a hook can modify a request.permanent_node_message_test/0 * permanent_node_message_test() -> any() Test that a permanent node message cannot be changed.priv_inaccessible_test/0 * priv_inaccessible_test() -> any() Test that we can't get the node message if the requested key is private.request_response_hooks_test/0 * request_response_hooks_test() -> any() resolve_hook/4 * resolve_hook(HookName, InitiatingRequest, Body, NodeMsg) -> any() Execute a hook from the node message upon the user's request. The
invocation of the hook provides a request of the following form:/path => request | response
       /request => the original request singleton
       /body => parsed sequence of messages to process | the execution result status_code/2 * status_code(X1, NodeMsg) -> any() Calculate the appropriate HTTP status code for an AO-Core result.
The order of precedence is:
1. The status code from the message.
2. The HTTP representation of the status code.
3. The default status code.unauthorized_set_node_msg_fails_test/0 * unauthorized_set_node_msg_fails_test() -> any() Test that we can't set the node message if the request is not signed by
the owner of the node.uninitialized_node_test/0 * uninitialized_node_test() -> any() Test that an uninitialized node will not run computation.update_node_message/2 * update_node_message(Request, NodeMsg) -> any() Validate that the request is signed by the operator of the node, then
allow them to update the node message.

---

# 203. Module dev_snperl - HyperBEAM - Documentation

Document Number: 203
Source: https://hyperbeam.arweave.net/build/devices/source-code/dev_snp.html
Words: 426
Extraction Method: html

Module dev_snp.erl This device offers an interface for validating AMD SEV-SNP commitments,
as well as generating them, if called in an appropriate environment.Function Index execute_is_trusted/3* Ensure that all of the software hashes are trusted.generate/3 Generate an commitment report and emit it as a message, including all of
the necessary data to generate the nonce (ephemeral node address + node
message ID), as well as the expected measurement (firmware, kernel, and VMSAs
hashes).generate_nonce/2* Generate the nonce to use in the commitment report.is_debug/1* Ensure that the node's debug policy is disabled.real_node_test/0*  report_data_matches/3* Ensure that the report data matches the expected report data.trusted/3 Validates if a given message parameter matches a trusted value from the SNP trusted list
Returns {ok, true} if the message is trusted, {ok, false} otherwise.verify/3 Verify an commitment report message; validating the identity of a
remote node, its ephemeral private address, and the integrity of the report.Function Details execute_is_trusted/3 * execute_is_trusted(M1, Msg, NodeOpts) -> any() Ensure that all of the software hashes are trusted. The caller may set
a specific device to use for the is-trusted key. The device must then
implement the trusted resolver.generate/3 generate(M1, M2, Opts) -> any() Generate an commitment report and emit it as a message, including all of
the necessary data to generate the nonce (ephemeral node address + node
message ID), as well as the expected measurement (firmware, kernel, and VMSAs
hashes).generate_nonce/2 * generate_nonce(RawAddress, RawNodeMsgID) -> any() Generate the nonce to use in the commitment report.is_debug/1 * is_debug(Report) -> any() Ensure that the node's debug policy is disabled.real_node_test/0 * real_node_test() -> any() report_data_matches/3 * report_data_matches(Address, NodeMsgID, ReportData) -> any() Ensure that the report data matches the expected report data.trusted/3 trusted(Msg1, Msg2, NodeOpts) -> any() Validates if a given message parameter matches a trusted value from the SNP trusted list
Returns {ok, true} if the message is trusted, {ok, false} otherwise verify/3 verify(M1, M2, NodeOpts) -> any() Verify an commitment report message; validating the identity of a
remote node, its ephemeral private address, and the integrity of the report.
The checks that must be performed to validate the report are:
1. Verify the address and the node message ID are the same as the ones
used to generate the nonce.
2. Verify the address that signed the message is the same as the one used
to generate the nonce.
3. Verify that the debug flag is disabled.
4. Verify that the firmware, kernel, and OS (VMSAs) hashes, part of the
measurement, are trusted.
5. Verify the measurement is valid.
6. Verify the report's certificate chain to hardware root of trust.

---

# 204. Exposing Process State to HyperBEAM  Cookbook

Document Number: 204
Source: https://cookbook_ao.arweave.net/guides/migrating-to-hyperbeam/exposing-process-state.html
Words: 579
Extraction Method: html

Exposing Process State to HyperBEAM HyperBEAM introduces a powerful feature for exposing parts of a process's state for immediate reading over HTTP. This improves performance for web frontends and data services by replacing the need for dryrun calls, which were a known bottleneck on legacynet.The Patch Device The ~patch@1.0 device is the mechanism that allows AO processes to make parts of their internal state readable via direct HTTP GET requests.How it Works Exposing state is a four-step process involving your process and HyperBEAM:Process Logic: From your process (e.g., in Lua or WASM), send an outbound message to the ~patch@1.0 device.Patch Message Format: The message must include device and cache tags.HyperBEAM Execution: HyperBEAM's dev_patch module processes this message, mapping the key-value pairs from the cache table to a URL path.HTTP Access: The exposed data is then immediately available via a standard HTTP GET request to the process's endpoint.HyperBEAMGET /<process-id>~process@1.0/compute/cache/<mydatakey> Initial State Sync (Optional) To make data available immediately on process creation, you can patch its initial state. A common pattern is to use a flag to ensure this sync only runs once, as shown in this example for a token's Balances and TotalSupply.lua-- Place this logic at the top level of your process script,

-- outside of specific handlers, so it runs on load.

Balances = { token1 = 100, token2 = 200 } -- A table of balances

TotalSupply = 1984 -- A single total supply value

-- 1. Initialize Flag:

-- Initializes a flag if it doesn't exist.

InitialSync = InitialSync or 'INCOMPLETE'

-- 2. Check Flag:

-- Checks if the sync has already run.

if InitialSync == 'INCOMPLETE' then

  -- 3. Patch State:

  -- The `Send` call patches the state, making it available at endpoints like:

  -- /cache/balances

  -- /cache/totalsupply

  Send({ device = 'patch@1.0', cache = { balances = Balances, totalsupply = TotalSupply } })

  -- 4. Update Flag:

  -- Updates the flag to prevent the sync from running again.

  InitialSync = 'COMPLETE'

  print("Initial state sync complete. Balances and TotalSupply patched.")

end This pattern makes essential data queryable upon process creation, boosting application responsiveness.Example (Lua in aos) This handler exposes a currentstatus key that can be read via HTTP after the PublishData action is called.Avoiding Key Conflicts Keys in the cache table become URL path segments. To avoid conflicts with reserved HyperBEAM paths, use descriptive, specific keys. Avoid using reserved keywords such as:For instance, prefer a key like myappstate over a generic key like state.WARNING HTTP paths are case-insensitive. While the patch device stores keys with case sensitivity (e.g., MyKey vs mykey), HTTP access to paths like the following is ambiguous and may lead to unpredictable results.To prevent conflicts, always use lowercase keys in your cache table (e.g., mykey, usercount).HyperBEAMGET /<process-id>~process@1.0/cache/mykey Key Points Path Structure: Data is exposed at a path structured like this, where <key> is a key from your cache table:HyperBEAM/<process-id>~process@1.0/cache/<key> Data Types: Basic data types like strings and numbers work best. Complex objects may require serialization.compute vs now: Accessing patched data can be done via two main paths:The compute endpoint serves the last known value quickly, while now may perform additional computation to get the most recent state.Read-Only Exposure: Patching is for efficient reads and does not replace your process's core state management logic.Using the patch device enables efficient, standard HTTP access to your process state, seamlessly connecting decentralized logic with web applications.Now that you know how to expose static state, learn how to perform on-the-fly computations on that state by reading dynamic state.

---

# 205. Installing aos  Cookbook

Document Number: 205
Source: https://cookbook_ao.arweave.net/guides/aos/installing.html
Words: 33
Extraction Method: html

Skip to content  Installing aos Installing aos only requires NodeJS - https://nodejs.org NOTE: If you are on windows you may get better results with WSL Console.Once installed you can run by typing aos

---

# 206. ARIO Docs

Document Number: 206
Source: https://docs.ar.io/ar-io-sdk/ants/reassign-name
Words: 33
Extraction Method: html

reassignName reassignName is a method on the ANT class that transfers ownership of an ArNS name to a new ANT. This operation can only be performed by the current ANT owner.reassignName requires authentication.

---

# 207. ARIO Docs

Document Number: 207
Source: https://docs.ar.io/ar-io-sdk/ants/remove-primary-names
Words: 33
Extraction Method: html

removePrimaryNames removePrimaryNames is a method on the ANT class that removes specified primary names from the ANT process. This affects any primary names associated with base names controlled by this ANT.removePrimaryNames requires authentication.

---

# 208. removeRecord - ARIO Docs

Document Number: 208
Source: https://docs.ar.io/ar-io-sdk/ants/remove-record
Words: 33
Extraction Method: html

Deprecated This method is deprecated. Please use removeUndernameRecord instead. See the removeUndernameRecord documentation
for more details.removeRecord is a method on the ANT class that removes a record from the ANT process.removeRecord requires authentication.

---

# 209. ARIO Docs

Document Number: 209
Source: https://docs.ar.io/ar-io-sdk/ants/set-logo
Words: 33
Extraction Method: html

setLogo setLogo is a method on the ANT class that updates the logo of the ANT process. The logo must be specified as an Arweave transaction ID that contains an image.setLogo requires authentication.

---

# 210. ARIO Docs

Document Number: 210
Source: https://docs.ar.io/ar-io-sdk/ario/gateways/instant-withdrawal
Words: 33
Extraction Method: html

instantWithdrawal instantWithdrawal is a method on the ARIO class that instantly withdraws funds from an existing vault on a gateway. If no gatewayAddress is provided, the signer's address will be used.instantWithdrawal requires authentication.

---

# 211. ARIO Docs

Document Number: 211
Source: https://docs.ar.io/ar-io-sdk/ario/gateways/decrease-operator-stake
Words: 30
Extraction Method: html

decreaseOperatorStake decreaseOperatorStake is a method on the ARIO class that decreases the caller's operator stake. This method must be executed with a wallet registered as a gateway operator.decreaseOperatorStake requires authentication.

---

# 212. ARIO Docs

Document Number: 212
Source: https://docs.ar.io/ar-io-sdk/ario/gateways/increase-operator-stake
Words: 30
Extraction Method: html

increaseOperatorStake increaseOperatorStake is a method on the ARIO class that increases the caller's operator stake. This method must be executed with a wallet registered as a gateway operator.increaseOperatorStake requires authentication.

---

# 213. getBalances - ARIO Docs

Document Number: 213
Source: https://docs.ar.io/ar-io-sdk/ants/get-balances
Words: 29
Extraction Method: html

getBalances is a method on the ANT class that retrieves all token balances within the ANT process.getBalances does not require authentication.Parameters The getBalances method does not accept any parameters.

---

# 214. getHandlers - ARIO Docs

Document Number: 214
Source: https://docs.ar.io/ar-io-sdk/ants/get-handlers
Words: 28
Extraction Method: html

getHandlers is a method on the ANT class that retrieves the handlers supported by the ANT.getHandlers does not require authentication.Parameters The getHandlers method does not accept any parameters.

---

# 215. removeUndernameRecord - ARIO Docs

Document Number: 215
Source: https://docs.ar.io/ar-io-sdk/ants/remove-undername-record
Words: 28
Extraction Method: html

removeUndernameRecord is a method on the ANT class that removes a specified undername record from the ANT process. Once removed, the undername will no longer resolve.removeUndernameRecord requires authentication.

---

# 216. getGatewayVaults - ARIO Docs

Document Number: 216
Source: https://docs.ar.io/ar-io-sdk/ario/gateways/get-gateway-vaults
Words: 27
Extraction Method: html

getGatewayVaults is a method on the ARIO class that retrieves all vault information for a specific gateway, including delegated stakes and pending withdrawals.getGatewayVaults does not require authentication.

---

# 217. ARIO Docs

Document Number: 217
Source: https://docs.ar.io/ar-io-sdk/ants/remove-controller
Words: 24
Extraction Method: html

removeController removeController is a method on the ANT class that removes a specified wallet address from the ANT's list of approved controllers.removeController requires authentication.

---

# 218. getGateway - ARIO Docs

Document Number: 218
Source: https://docs.ar.io/ar-io-sdk/ario/gateways/get-gateway
Words: 24
Extraction Method: html

getGateway is a method on the ARIO class that retrieves detailed information about a specific gateway using its wallet address.getGateway does not require authentication.

---

# 219. ARIO Docs

Document Number: 219
Source: https://docs.ar.io/ar-io-sdk/ario/gateways/decrease-delegate-stake
Words: 21
Extraction Method: html

decreaseDelegateStake decreaseDelegateStake is a method on the ARIO class that decreases the caller's delegated stake on the target gateway.decreaseDelegateStake requires authentication.

---

# 220. ARIO Docs

Document Number: 220
Source: https://docs.ar.io/ar-io-sdk/ario/gateways/increase-delegate-stake
Words: 21
Extraction Method: html

increaseDelegateStake increaseDelegateStake is a method on the ARIO class that increases the caller's delegated stake on the target gateway.increaseDelegateStake requires authentication.

---

# 221. json10 - HyperBEAM - Documentation

Document Number: 221
Source: https://hyperbeam.arweave.net/build/devices/json-at-1-0.html
Words: 199
Extraction Method: html

Device: ~json@1.0 Overview The ~json@1.0 device provides a mechanism to interact with JSON (JavaScript Object Notation) data structures. It allows treating a JSON document or string as a stateful entity against which queries can be executed.This device is useful for:Serializing and deserializing JSON data.Querying and modifying JSON objects.Integrating with other devices and operations.Core Functions (Keys) Serialization GET /~json@1.0/serialize (Direct Serialize Action) Action: Serializes the input message or data into a JSON string.Example:GET /~json@1.0/serialize - serializes the current message as JSON.Path: The path segment /serialize directly follows the device identifier.GET /<PreviousPath>/~json@1.0/serialize (Chained Serialize Action) Action: Takes arbitrary data output from <PreviousPath> (another device or operation) and returns its serialized JSON string representation.Example:GET /~meta@1.0/info/~json@1.0/serialize - fetches node info from the meta device and then pipes it to the JSON device to serialize the result as JSON.Path: This segment (/~json@1.0/serialize) is appended to a previous path segment.Path Chaining Example The JSON device is particularly useful in path chaining to convert output from other devices into JSON format:GET /~meta@1.0/info/~json@1.0/serialize This retrieves the node configuration from the meta device and serializes it to JSON.See Also Message Device - Works well with JSON serialization Meta Device - Can provide configuration data to serialize json module

---

# 222. Building Devices - HyperBEAM - Documentation

Document Number: 222
Source: https://hyperbeam.arweave.net/build/devices/building-devices.html
Words: 150
Extraction Method: html

Extending HyperBEAM with Devices We encourage you to extend HyperBEAM with devices for functionality that is general purpose and reusable across different applications.What are Devices?As explained in the introduction, devices are the core functional units within HyperBEAM. They are self-contained modules that process messages and perform specific actions, forming the building blocks of your application's logic.HyperBEAM comes with a set of powerful built-in devices that handle everything from process management (~process@1.0) and message scheduling (~scheduler@1.0) to executing WebAssembly (~wasm64@1.0) and Lua scripts (~lua@5.3a).Creating Your Own Devices (Coming Soon) We will create more in depth guides for building devices in Lua and Erlang in the future.Further Reading In the meantime, community-contributed guides are available that can walk you through the process. For example:Rust:Building Rust Devices with HyperBEAM M3 Beta: mini-Roam API (Vol. 1) - A tutorial from Decent Land Labs that covers how to build a custom Rust device from scratch.

---

# 223. lua53a - HyperBEAM - Documentation

Document Number: 223
Source: https://hyperbeam.arweave.net/build/devices/lua-at-5-3a.html
Words: 542
Extraction Method: html

Device: ~lua@5.3a Overview The ~lua@5.3a device enables the execution of Lua scripts within the HyperBEAM environment. It provides an isolated sandbox where Lua code can process incoming messages, interact with other devices, and manage state.Core Concept: Lua Script Execution This device allows processes to perform computations defined in Lua scripts. Similar to the ~wasm64@1.0 device, it manages the lifecycle of a Lua execution state associated with the process.Key Functions (Keys) These keys are typically used within an execution stack (managed by dev_stack) for an AO process.init Action: Initializes the Lua environment for the process. It finds and loads the Lua script(s) associated with the process, creates a luerl state, applies sandboxing rules if specified, installs the dev_lua_lib (providing AO-specific functions like ao.send), and stores the initialized state in the process's private area (priv/state).Inputs (Expected in Process Definition or init Message):script: Can be:An Arweave Transaction ID of the Lua script file.A list of script IDs or script message maps.A message map containing the Lua script in its body tag (Content-Type application/lua or text/x-lua).A map where keys are module names and values are script IDs/messages.sandbox: (Optional) Controls Lua sandboxing. Can be true (uses default sandbox list), false (no sandbox), or a map/list specifying functions to disable and their return values.Outputs (Stored in priv/):state: The initialized luerl state handle.<FunctionName> (Default Handler - compute) Action: Executes a specific function within the loaded Lua script(s). This is the default handler; if a key matching a Lua function name is called on the device, this logic runs.Inputs (Expected in Process State or Incoming Message):priv/state: The Lua state obtained during init.The key being accessed (used as the default function name).function or body/function: (Optional) Overrides the function name derived from the key.parameters or body/parameters: (Optional) Arguments to pass to the Lua function. Defaults to a list containing the process message, the request message, and an empty options map.Response: The results returned by the Lua function call, typically encoded. The device also updates the priv/state with the Lua state after execution.snapshot Action: Captures the current state of the running Lua environment. luerl state is serializable.Inputs:priv/state.Outputs: A message containing the serialized Lua state, typically tagged with [Prefix]/State.normalize (Internal Helper) Action: Ensures a consistent state representation by loading a Lua state from a snapshot ([Prefix]/State) if a live state (priv/state) isn't already present.functions Action: Returns a list of all globally defined functions within the current Lua state.Inputs:priv/state.Response: A list of function names.Sandboxing The sandbox option in the process definition restricts potentially harmful Lua functions (like file I/O, OS commands, loading arbitrary code). By default (sandbox = true), common dangerous functions are disabled. You can customize the sandbox rules.AO Library (dev_lua_lib) The init function automatically installs a helper library (dev_lua_lib) into the Lua state. This library typically provides functions for interacting with the AO environment from within the Lua script, such as:ao.send({ Target = ..., ... }): To send messages from the process.Access to message tags and data.Usage within dev_stack Like ~wasm64@1.0, the ~lua@5.3a device is typically used within an execution stack.# Example Process Definition Snippet
Execution-Device: stack@1.0
Execution-Stack: scheduler@1.0, lua@5.3a
Script: <LuaScriptTxID>
Sandbox: true This device offers a lightweight, integrated scripting capability for AO processes, suitable for a wide range of tasks from simple logic to more complex state management and interactions.lua module

---

# 224. message10 - HyperBEAM - Documentation

Document Number: 224
Source: https://hyperbeam.arweave.net/build/devices/message-at-1-0.html
Words: 472
Extraction Method: html

Device: ~message@1.0 Overview The ~message@1.0 device is a fundamental built-in device in HyperBEAM. It serves as the identity device for standard AO-Core messages, which are represented as Erlang maps internally. Its primary function is to allow manipulation and inspection of these message maps directly via HTTP requests, without needing a persistent process state.This device is particularly useful for:Creating and modifying transient messages on the fly using query parameters.Retrieving specific values from a message map.Inspecting the keys of a message.Handling message commitments and verification (though often delegated to specialized commitment devices like httpsig@1.0).Core Functionality The message@1.0 device treats the message itself as the state it operates on. Key operations are accessed via path segments in the HTTP path.Key Access (/key) To retrieve the value associated with a specific key in the message map, simply append the key name to the path. Key lookup is case-insensitive.Example:GET /~message@1.0&hello=world&Key=Value/key Response:"Value" Reserved Keys The message@1.0 device reserves several keys for specific operations:get: (Default operation if path segment matches a key in the map) Retrieves the value of a specified key. Behaves identically to accessing /key directly.set: Modifies the message by adding or updating key-value pairs. Requires additional parameters (usually in the request body or subsequent path segments/query params, depending on implementation specifics).Supports deep merging of maps.Setting a key to unset removes it.Overwriting keys that are part of existing commitments will typically remove those commitments unless the new value matches the old one.set_path: A special case for setting the path key itself, which cannot be done via the standard set operation.remove: Removes one or more specified keys from the message. Requires an item or items parameter.keys: Returns a list of all public (non-private) keys present in the message map.id: Calculates and returns the ID (hash) of the message. Considers active commitments based on specified committers. May delegate ID calculation to a device specified by the message's id-device key commit: Creates a commitment (e.g., a signature) for the message. Requires parameters like commitment-device and potentially committer information. Delegates the actual commitment generation to the specified device (default httpsig@1.0).committers: Returns a list of committers associated with the commitments in the message. Can be filtered by request parameters.commitments: Used internally and in requests to filter or specify which commitments to operate on (e.g., for id or verify).verify: Verifies the commitments attached to the message. Can be filtered by committers or specific commitment IDs in the request. Delegates verification to the device specified in each commitment (commitment-device).Private Keys Keys prefixed with priv (e.g., priv_key, private.data) are considered private and cannot be accessed or listed via standard get or keys operations.HTTP Example This example demonstrates creating a transient message and retrieving a value:GET /~message@1.0&hello=world&k=v/k Breakdown:~message@1.0: Sets the root device.&hello=world&k=v: Query parameters create the initial message: #{ <<"hello">> => <<"world">>, <<"k">> => <<"v">> }./k: The path segment requests the value for the key k.Response:"v"

---

# 225. Module dev_fafferl - HyperBEAM - Documentation

Document Number: 225
Source: https://hyperbeam.arweave.net/build/devices/source-code/dev_faff.html
Words: 180
Extraction Method: html

Module dev_faff.erl A module that implements a 'friends and family' pricing policy.Description It will allow users to process requests only if their addresses are
in the allow-list for the node.Fundamentally against the spirit of permissionlessness, but it is useful if
you are running a node for your own purposes and would not like to allow
others to make use of it -- even for a fee. It also serves as a useful
example of how to implement a custom pricing policy, as it implements stubs
for both the pricing and ledger P4 APIs.Function Index charge/3 Charge the user's account if the request is allowed.estimate/3 Decide whether or not to service a request from a given address.is_admissible/2* Check whether all of the signers of the request are in the allow-list.Function Details charge/3 charge(X1, Req, NodeMsg) -> any() Charge the user's account if the request is allowed.estimate/3 estimate(X1, Msg, NodeMsg) -> any() Decide whether or not to service a request from a given address.is_admissible/2 * is_admissible(Msg, NodeMsg) -> any() Check whether all of the signers of the request are in the allow-list.

---

# 226. Module dev_codec_structurederl - HyperBEAM - Documentation

Document Number: 226
Source: https://hyperbeam.arweave.net/build/devices/source-code/dev_codec_structured.html
Words: 338
Extraction Method: html

Module dev_codec_structured.erl A device implementing the codec interface (to/1, from/1) for
HyperBEAM's internal, richly typed message format.Description This format mirrors HTTP Structured Fields, aside from its limitations of
compound type depths, as well as limited floating point representations.As with all AO-Core codecs, its target format (the format it expects to
receive in the to/1 function, and give in from/1) is TABM.For more details, see the HTTP Structured Fields (RFC-9651) specification.Function Index commit/3  decode_ao_types/2 Parse the ao-types field of a TABM and return a map of keys and their
types.decode_value/2 Convert non-binary values to binary for serialization.encode_ao_types/2 Generate an ao-types structured field from a map of keys and their
types.encode_value/1 Convert a term to a binary representation, emitting its type for
serialization as a separate tag.from/3 Convert a rich message into a 'Type-Annotated-Binary-Message' (TABM).implicit_keys/1* Find the implicit keys of a TABM.implicit_keys/2  is_list_from_ao_types/2 Determine if the ao-types field of a TABM indicates that the message
is a list.linkify_mode/2* Discern the linkify mode from the request and the options.list_encoding_test/0*  to/3 Convert a TABM into a native HyperBEAM message.verify/3  Function Details commit/3 commit(Msg, Req, Opts) -> any() decode_ao_types/2 decode_ao_types(Msg, Opts) -> any() Parse the ao-types field of a TABM and return a map of keys and their
types decode_value/2 decode_value(Type, Value) -> any() Convert non-binary values to binary for serialization.encode_ao_types/2 encode_ao_types(Types, Opts) -> any() Generate an ao-types structured field from a map of keys and their
types.encode_value/1 encode_value(Value) -> any() Convert a term to a binary representation, emitting its type for
serialization as a separate tag.from/3 from(Bin, Req, Opts) -> any() Convert a rich message into a 'Type-Annotated-Binary-Message' (TABM).implicit_keys/1 * implicit_keys(Req) -> any() Find the implicit keys of a TABM.implicit_keys/2 implicit_keys(Req, Opts) -> any() is_list_from_ao_types/2 is_list_from_ao_types(Types, Opts) -> any() Determine if the ao-types field of a TABM indicates that the message
is a list.linkify_mode/2 * linkify_mode(Req, Opts) -> any() Discern the linkify mode from the request and the options.list_encoding_test/0 * list_encoding_test() -> any() to/3 to(Bin, Req, Opts) -> any() Convert a TABM into a native HyperBEAM message.verify/3 verify(Msg, Req, Opts) -> any()

---

# 227. Module dev_cronerl - HyperBEAM - Documentation

Document Number: 227
Source: https://hyperbeam.arweave.net/build/devices/source-code/dev_cron.html
Words: 302
Extraction Method: html

Module dev_cron.erl A device that inserts new messages into the schedule to allow processes
to passively 'call' themselves without user interaction.Function Index every/3 Exported function for scheduling a recurring message.every_worker_loop/4*  every_worker_loop_test/0* This test verifies that a recurring task can be scheduled and executed.info/1 Exported function for getting device info.info/3  once/3 Exported function for scheduling a one-time message.once_executed_test/0* This test verifies that a one-time task can be scheduled and executed.once_worker/3* Internal function for scheduling a one-time message.parse_time/1* Parse a time string into milliseconds.stop/3 Exported function for stopping a scheduled task.stop_every_test/0* This test verifies that a recurring task can be stopped by
calling the stop function with the task ID.stop_once_test/0*  test_worker/0* This is a helper function that is used to test the cron device.test_worker/1*  Function Details every/3 every(Msg1, Msg2, Opts) -> any() Exported function for scheduling a recurring message.every_worker_loop/4 * every_worker_loop(CronPath, Req, Opts, IntervalMillis) -> any() every_worker_loop_test/0 * every_worker_loop_test() -> any() This test verifies that a recurring task can be scheduled and executed.info/1 info(X1) -> any() Exported function for getting device info.info/3 info(Msg1, Msg2, Opts) -> any() once/3 once(Msg1, Msg2, Opts) -> any() Exported function for scheduling a one-time message.once_executed_test/0 * once_executed_test() -> any() This test verifies that a one-time task can be scheduled and executed.once_worker/3 * once_worker(Path, Req, Opts) -> any() Internal function for scheduling a one-time message.parse_time/1 * parse_time(BinString) -> any() Parse a time string into milliseconds.stop/3 stop(Msg1, Msg2, Opts) -> any() Exported function for stopping a scheduled task.stop_every_test/0 * stop_every_test() -> any() This test verifies that a recurring task can be stopped by
calling the stop function with the task ID.stop_once_test/0 * stop_once_test() -> any() test_worker/0 * test_worker() -> any() This is a helper function that is used to test the cron device.
It is used to increment a counter and update the state of the worker.test_worker/1 * test_worker(State) -> any()

---

# 228. Module dev_patcherl - HyperBEAM - Documentation

Document Number: 228
Source: https://hyperbeam.arweave.net/build/devices/source-code/dev_patch.html
Words: 375
Extraction Method: html

Module dev_patch.erl A device that can be used to reorganize a message: Moving data from
one path inside it to another.Description This device's function runs in two modes:When using all to move all data at the path given in from to the
path given in to.When using patches to move all submessages in the source to the target,if they have a method key of PATCH or a device key of patch@1.0.Source and destination paths may be prepended by base: or req: keys to
indicate that they are relative to either of the message's that the
computation is being performed on.The search order for finding the source and destination keys is as follows,
where X is either from or to:The patch-X key of the execution message.The X key of the execution message.The patch-X key of the request message.The X key of the request message.Additionally, this device implements the standard computation device keys,
allowing it to be used as an element of an execution stack pipeline, etc.Function Index all/3 Get the value found at the patch-from key of the message, or the from key if the former is not present.all_mode_test/0*  compute/3  init/3 Necessary hooks for compliance with the execution-device standard.move/4* Unified executor for the all and patches modes.normalize/3  patch_to_submessage_test/0*  patches/3 Find relevant PATCH messages in the given source key of the execution
and request messages, and apply them to the given destination key of the
request.req_prefix_test/0*  snapshot/3  uninitialized_patch_test/0*  Function Details all/3 all(Msg1, Msg2, Opts) -> any() Get the value found at the patch-from key of the message, or the from key if the former is not present. Remove it from the message and set
the new source to the value found.all_mode_test/0 * all_mode_test() -> any() compute/3 compute(Msg1, Msg2, Opts) -> any() init/3 init(Msg1, Msg2, Opts) -> any() Necessary hooks for compliance with the execution-device standard.move/4 * move(Mode, Msg1, Msg2, Opts) -> any() Unified executor for the all and patches modes.normalize/3 normalize(Msg1, Msg2, Opts) -> any() patch_to_submessage_test/0 * patch_to_submessage_test() -> any() patches/3 patches(Msg1, Msg2, Opts) -> any() Find relevant PATCH messages in the given source key of the execution
and request messages, and apply them to the given destination key of the
request.req_prefix_test/0 * req_prefix_test() -> any() snapshot/3 snapshot(Msg1, Msg2, Opts) -> any() uninitialized_patch_test/0 * uninitialized_patch_test() -> any()

---

# 229. Module dev_stackerl - HyperBEAM - Documentation

Document Number: 229
Source: https://hyperbeam.arweave.net/build/devices/source-code/dev_stack.html
Words: 1145
Extraction Method: html

Module dev_stack.erl A device that contains a stack of other devices, and manages their
execution.Description It can run in two modes: fold (the default), and map.In fold mode, it runs upon input messages in the order of their keys. A
stack maintains and passes forward a state (expressed as a message) as it
progresses through devices.For example, a stack of devices as follows:Device -> Stack
   Device-Stack/1/Name -> Add-One-Device
   Device-Stack/2/Name -> Add-Two-Device When called with the message:#{ Path = "FuncName", binary => <code><<"0">></code> } Will produce the output:#{ Path = "FuncName", binary => <code><<"3">></code> }
   {ok, #{ bin => <code><<"3">></code> }} In map mode, the stack will run over all the devices in the stack, and
combine their results into a single message. Each of the devices'
output values have a key that is the device's name in the Device-Stack (its number if the stack is a list).You can switch between fold and map modes by setting the Mode key in the Msg2 to either Fold or Map, or set it globally for the stack by
setting the Mode key in the Msg1 message. The key in Msg2 takes
precedence over the key in Msg1.The key that is called upon the device stack is the same key that is used
upon the devices that are contained within it. For example, in the above
scenario we resolve FuncName on the stack, leading FuncName to be called on
Add-One-Device and Add-Two-Device.A device stack responds to special statuses upon responses as follows:skip: Skips the rest of the device stack for the current pass.pass: Causes the stack to increment its pass number and re-execute
the stack from the first device, maintaining the state
accumulated so far. Only available in fold mode.In all cases, the device stack will return the accumulated state to the
caller as the result of the call to the stack.The dev_stack adds additional metadata to the message in order to track
the state of its execution as it progresses through devices. These keys
are as follows:Stack-Pass: The number of times the stack has reset and re-executed
from the first device for the current message.Input-Prefix: The prefix that the device should use for its outputs
and inputs.Output-Prefix: The device that was previously executed.All counters used by the stack are initialized to 1.Additionally, as implemented in HyperBEAM, the device stack will honor a
number of options that are passed to it as keys in the message. Each of
these options is also passed through to the devices contained within the
stack during execution. These options include:Error-Strategy: Determines how the stack handles errors from devices.
See maybe_error/5 for more information.Allow-Multipass: Determines whether the stack is allowed to automatically
re-execute from the first device when the pass tag is returned. See maybe_pass/3 for more information.Under-the-hood, dev_stack uses a default handler to resolve all calls to
devices, aside set/2 which it calls itself to mutate the message's device key in order to change which device is currently being executed. This method
allows dev_stack to ensure that the message's HashPath is always correct,
even as it delegates calls to other devices. An example flow for a dev_stack execution is as follows:/Msg1/AlicesExcitingKey ->
        dev_stack:execute ->
            /Msg1/Set?device=/Device-Stack/1 ->
            /Msg2/AlicesExcitingKey ->
            /Msg3/Set?device=/Device-Stack/2 ->
            /Msg4/AlicesExcitingKey
            ... ->
            /MsgN/Set?device=[This-Device] ->
        returns {ok, /MsgN+1} ->
    /MsgN+1 In this example, the device key is mutated a number of times, but the
resulting HashPath remains correct and verifiable.Function Index benchmark_test/0*  example_device_for_stack_test/0*  generate_append_device/1  generate_append_device/2*  increment_pass/2* Helper to increment the pass number.info/1  input_and_output_prefixes_test/0*  input_output_prefixes_passthrough_test/0*  input_prefix/3 Return the input prefix for the stack.many_devices_test/0*  maybe_error/5*  no_prefix_test/0*  not_found_test/0*  output_prefix/3 Return the output prefix for the stack.output_prefix_test/0*  pass_test/0*  prefix/3 Return the default prefix for the stack.reinvocation_test/0*  resolve_fold/3* The main device stack execution engine.resolve_fold/4*  resolve_map/3* Map over the devices in the stack, accumulating the output in a single
message of keys and values, where keys are the same as the keys in the
original message (typically a number).router/3*  router/4 The device stack key router.simple_map_test/0*  simple_stack_execute_test/0*  skip_test/0*  test_prefix_msg/0*  transform/3* Return Message1, transformed such that the device named Key from the Device-Stack key in the message takes the place of the original Device key.transform_external_call_device_test/0* Ensure we can generate a transformer message that can be called to
return a version of msg1 with only that device attached.transform_internal_call_device_test/0* Test that the transform function can be called correctly internally
by other functions in the module.transformer_message/2* Return a message which, when given a key, will transform the message
such that the device named Key from the Device-Stack key in the message
takes the place of the original Device key.Function Details benchmark_test/0 * benchmark_test() -> any() example_device_for_stack_test/0 * example_device_for_stack_test() -> any() generate_append_device/1 generate_append_device(Separator) -> any() generate_append_device/2 * generate_append_device(Separator, Status) -> any() increment_pass/2 * increment_pass(Message, Opts) -> any() Helper to increment the pass number.info/1 info(Msg) -> any() input_and_output_prefixes_test/0 * input_and_output_prefixes_test() -> any() input_output_prefixes_passthrough_test/0 * input_output_prefixes_passthrough_test() -> any() input_prefix/3 input_prefix(Msg1, Msg2, Opts) -> any() Return the input prefix for the stack.many_devices_test/0 * many_devices_test() -> any() maybe_error/5 * maybe_error(Message1, Message2, DevNum, Info, Opts) -> any() no_prefix_test/0 * no_prefix_test() -> any() not_found_test/0 * not_found_test() -> any() output_prefix/3 output_prefix(Msg1, Msg2, Opts) -> any() Return the output prefix for the stack.output_prefix_test/0 * output_prefix_test() -> any() pass_test/0 * pass_test() -> any() prefix/3 prefix(Msg1, Msg2, Opts) -> any() Return the default prefix for the stack.reinvocation_test/0 * reinvocation_test() -> any() resolve_fold/3 * resolve_fold(Message1, Message2, Opts) -> any() The main device stack execution engine. See the moduledoc for more
information.resolve_fold/4 * resolve_fold(Message1, Message2, DevNum, Opts) -> any() resolve_map/3 * resolve_map(Message1, Message2, Opts) -> any() Map over the devices in the stack, accumulating the output in a single
message of keys and values, where keys are the same as the keys in the
original message (typically a number).router/3 * router(Message1, Message2, Opts) -> any() router/4 router(Key, Message1, Message2, Opts) -> any() The device stack key router. Sends the request to resolve_stack,
except for set/2 which is handled by the default implementation in dev_message.simple_map_test/0 * simple_map_test() -> any() simple_stack_execute_test/0 * simple_stack_execute_test() -> any() skip_test/0 * skip_test() -> any() test_prefix_msg/0 * test_prefix_msg() -> any() transform/3 * transform(Msg1, Key, Opts) -> any() Return Message1, transformed such that the device named Key from the Device-Stack key in the message takes the place of the original Device key. This transformation allows dev_stack to correctly track the HashPath
of the message as it delegates execution to devices contained within it.transform_external_call_device_test/0 * transform_external_call_device_test() -> any() Ensure we can generate a transformer message that can be called to
return a version of msg1 with only that device attached.transform_internal_call_device_test/0 * transform_internal_call_device_test() -> any() Test that the transform function can be called correctly internally
by other functions in the module.transformer_message/2 * transformer_message(Msg1, Opts) -> any() Return a message which, when given a key, will transform the message
such that the device named Key from the Device-Stack key in the message
takes the place of the original Device key. This allows users to call
a single device from the stack:/Msg1/Transform/DeviceName/keyInDevice ->
keyInDevice executed on DeviceName against Msg1.

---

# 230. Intro to HyperBEAM - HyperBEAM - Documentation

Document Number: 230
Source: https://hyperbeam.arweave.net/build/introduction/what-is-hyperbeam.html
Words: 307
Extraction Method: html

Skip to content
 What is HyperBEAM? HyperBEAM is the primary, production-ready implementation of the AO-Core protocol, built on the robust Erlang/OTP framework. It serves as a decentralized operating system, powering the AO Computer —a scalable, trust-minimized, distributed supercomputer built on permanent storage of Arweave.Implementing AO-Core HyperBEAM transforms the abstract concepts of AO-Core—Messages, Devices, and Paths—into a concrete, operational system. It provides the runtime environment and essential services to execute these
computations across a network of distributed nodes.                  Messages Modular Data Packets In HyperBEAM, every interaction within the AO Computer is handled as a message. A message is a binary item or a map of functions. These cryptographically-linked data units are the foundation for communication, allowing processes to trigger computations, query state, and transfer value. HyperBEAM nodes are responsible for routing and processing these messages according to the rules of the AO-Core protocol.          Devices Extensible Execution Engines HyperBEAM introduces a uniquely modular architecture centered around Devices. These pluggable components are Erlang modules that define specific computational logic—like running WASM, managing state, or relaying data—allowing for unprecedented flexibility. This design allows developers to extend the system by creating custom Devices to fit their specific computational needs.                             Paths Composable Pipelines HyperBEAM exposes a powerful HTTP API that uses structured URL patterns to interact with processes and data. This pathing mechanism allows developers to create verifiable data pipelines, composing functionality from multiple devices into a single, atomic request. The URL bar effectively becomes a command-line interface for AO's trustless compute environment.A Robust and Scalable Foundation Built on the Erlang/OTP framework, HyperBEAM provides a robust and secure foundation that leverages the BEAM virtual machine for exceptional concurrency, fault tolerance, and scalability. This abstracts away underlying hardware, allowing diverse nodes to contribute resources without compatibility issues. The system governs how nodes coordinate and interact, forming a decentralized network that is resilient and permissionless.

---

# 231. FAQ - HyperBEAM - Documentation

Document Number: 231
Source: https://hyperbeam.arweave.net/build/reference/faq.html
Words: 279
Extraction Method: html

Developer FAQ This page answers common questions about building applications and processes on HyperBEAM.What can I build with HyperBEAM?You can build a wide range of applications, including:Decentralized applications (dApps) Distributed computation systems Peer-to-peer services Resilient microservices IoT device networks Decentralized storage solutions What is the current focus or phase of HyperBEAM development?The initial development phase focuses on integrating AO processes more deeply with HyperBEAM. A key part of this is phasing out the reliance on traditional "dryrun" simulations for reading process state. Instead, processes are encouraged to use the ~patch@1.0 device to expose specific parts of their state directly via GET requests. This allows for more efficient and direct state access, particularly for web interfaces and external integrations. You can learn more about this mechanism in the Exposing Process State with the Patch Device guide.What is the difference between HyperBEAM and Compute Unit?HyperBEAM: The Erlang-based node software that handles message routing, process management, and device coordination.Compute Unit (CU): A NodeJS implementation that executes WebAssembly modules and handles computational tasks.Together, these components form a complete execution environment for AO processes.What programming languages can I use with HyperBEAM?You can use any programming language that compiles to WebAssembly (WASM) for creating modules that run on the Compute Unit. This includes languages like:Lua Rust C/C++ And many others with WebAssembly support How do I debug processes running in HyperBEAM?Debugging processes in HyperBEAM can be done through:Logging messages to the system log (DEBUG=HB_PRINT rebar3 shell) Monitoring process state and message flow Inspecting memory usage and performance metrics Where can I get help if I encounter issues?If you encounter issues:Check the Troubleshooting guide Search or ask questions on GitHub Issues Join the community on Discord

---

# 232. Troubleshooting - HyperBEAM - Documentation

Document Number: 232
Source: https://hyperbeam.arweave.net/build/reference/troubleshooting.html
Words: 146
Extraction Method: html

Developer Troubleshooting Guide This guide addresses common issues you might encounter when developing processes for HyperBEAM.Process Execution Fails Symptoms: Errors when deploying or executing processes Solutions:Check both HyperBEAM and CU logs for specific error messages Verify that the WASM module is correctly compiled and valid Test with a simple example process to isolate the issue Adjust memory limits if the process requires more resources Memory Errors in Compute Unit Symptoms: Out of memory errors or excessive memory usage during process execution Solutions:Adjust the PROCESS_WASM_MEMORY_MAX_LIMIT environment variable Enable garbage collection by setting an appropriate GC_INTERVAL_MS Monitor memory usage and adjust limits as needed If on a low-memory system, reduce concurrent process execution Getting Help If you're still experiencing issues after trying these troubleshooting steps:Check the GitHub repository for known issues Join the Discord community for support Open an issue on GitHub with detailed information about your problem

---

# 233. Fuel Your LLM - HyperBEAM - Documentation

Document Number: 233
Source: https://hyperbeam.arweave.net/llms.html
Words: 166
Extraction Method: html

LLM Context Files This section provides access to specially formatted files intended for consumption by Large Language Models (LLMs) to provide context about the HyperBEAM documentation.LLM Summary (llms.txt) Content: Contains a brief summary of the HyperBEAM documentation structure and a list of relative file paths for all markdown documents included in the build.Usage: Useful for providing an LLM with a high-level overview and the available navigation routes within the documentation.LLM Full Content (llms-full.txt) Content: A single text file containing the complete, concatenated content of all markdown documents from the specified documentation directories (begin, run, guides, devices, resources). Each file's content is clearly demarcated.Usage: Ideal for feeding the entire documentation content into an LLM for comprehensive context, analysis, or question-answering based on the full documentation set.Generation Process These files are automatically generated by the docs/build-all.sh script during the documentation build process. They consolidate information from the following directories:docs/run docs/build  Permaweb LLMs.txt An interactive tool for selecting and curating Permaweb documentation into llms.txt format for feeding to LLMs.

---

# 234. Glossary - HyperBEAM - Documentation

Document Number: 234
Source: https://hyperbeam.arweave.net/build/reference/glossary.html
Words: 250
Extraction Method: html

Skip to content
 Developer Glossary This glossary provides definitions for terms and concepts relevant to building on HyperBEAM.AO-Core Protocol The underlying protocol that HyperBEAM implements, enabling decentralized computing and communication between nodes. AO-Core provides a framework into which any number of different computational models, encapsulated as primitive devices, can be attached.Asynchronous Message Passing A communication paradigm where senders don't wait for receivers to be ready, allowing for non-blocking operations and better scalability.Compute Unit (CU) The NodeJS component of HyperBEAM that executes WebAssembly modules and handles computational tasks.Device A functional unit in HyperBEAM that provides specific capabilities to the system, such as storage, networking, or computational resources.Hashpaths A mechanism for referencing locations in a program's state-space prior to execution. These state-space links are represented as Merklized lists of programs inputs and initial states.Message A data structure used for communication between processes in the HyperBEAM system. Messages can be interpreted as a binary term or as a collection of named functions (a Map of functions).Module A unit of code that can be loaded and executed by the Compute Unit, typically in WebAssembly format.Process An independent unit of computation in HyperBEAM with its own state and execution context.Process ID A unique identifier assigned to a process within the HyperBEAM system.WebAssembly (WASM) A binary instruction format that serves as a portable compilation target for programming languages, enabling deployment on the web and other environments.Permaweb Glossary For a more comprehensive glossary of terms used in the permaweb, try the Permaweb Glossary. Or use it below:

---

# 235. Glossary - HyperBEAM - Documentation

Document Number: 235
Source: https://hyperbeam.arweave.net/run/reference/glossary.html
Words: 286
Extraction Method: html

Skip to content
 Node Operator Glossary This glossary provides definitions for terms and concepts relevant to running a HyperBEAM node.AO-Core Protocol The underlying protocol that HyperBEAM implements, enabling decentralized computing and communication between nodes.Checkpoint A saved state of a process that can be used to resume execution from a known point, used for persistence and recovery.Compute Unit (CU) The NodeJS component of HyperBEAM that executes WebAssembly modules. While developers interact with it more, operators should know it's a key part of the stack.Erlang The programming language used to implement the HyperBEAM core, known for its robustness and support for building distributed, fault-tolerant applications.~flat@1.0 A format used for encoding settings files in HyperBEAM configuration, using HTTP header styling.HyperBEAM The Erlang-based node software that handles message routing, process management, and device coordination in the HyperBEAM ecosystem.Node An instance of HyperBEAM running on a physical or virtual machine that participates in the distributed network.~meta@1.0 A device used to configure the node's hardware, supported devices, metering and payments information, amongst other configuration options.~p4@1.0 A device that runs as a pre-processor and post-processor in HyperBEAM, enabling a framework for node operators to sell usage of their machine's hardware to execute AO-Core devices.~simple-pay@1.0 A simple, flexible pricing device that can be used in conjunction with p4@1.0 to offer flat-fees for the execution of AO-Core messages.~snp@1.0 A device used to generate and validate proofs that a node is executing inside a Trusted Execution Environment (TEE).Trusted Execution Environment (TEE) A secure area inside a processor that ensures the confidentiality and integrity of code and data loaded within it. Used in HyperBEAM for trust-minimized computation.Permaweb Glossary For a more comprehensive glossary of terms used in the permaweb, try the Permaweb Glossary. Or use it below:

---

# 236. How ao messaging works  Cookbook

Document Number: 236
Source: https://cookbook_ao.arweave.net/concepts/how-it-works.html
Words: 395
Extraction Method: html

How ao messaging works Before we dive in to ao, I want to share with you a little information about unix. Unix is a powerful operating system, but in its design it is focused on two Principal "Types". Files and Programs. A File is data and a Program is logic, when you combine the two you get information.Input.file | TransformProgram | Output.file You might have done something like this on the command line without knowing what you were doing. Being able to connect files to programs and return files which can then be passed to other programs creates a complex system composed of simple applications. This is a very powerful idea.Now, lets talk about ao the hyper parallel computer, and lets change the idea of a File to the ao concept of a Message and the idea of a Program to the ao concept of a Process. The ao computer takes messages and sends them to Processes in which those Processes can output messages that can be sent to other Processes. The result is a complex system built on simple modular logic containers.MessageA | Process | MessageB  Here is a description of the process as outlined in the flowchart:A message is initiated from an ao Connect. This message is sent to the mu service using a POST request. The body of the request contains data following a protocol, labeled 'ao', and is of the type 'Message'.The mu service processes the POST request and forwards the message to the su service. This is also done using a POST request with the same data protocol and message type.The su service stores the assignment and message on Arweave.A GET request is made to the cu service to retrieve results based on a message ID. The cu is a service that evaluates messages on processes and can return results based on an individual message identifier.A GET request is made to the su service to retrieve the assignment and message. This request is looking for messages from a process ID, within a range of time from a start (from the last evaluation point) to (to the current message ID).The final step is to push any outbox Messages. It involves reviewing the messages and spawns in the Result object. Based on the outcome of this check, the steps 2, 3, and 4 may be repeated for each relevant message or spawn.

---

# 237. Concepts  Cookbook

Document Number: 237
Source: https://cookbook_ao.arweave.net/concepts/index.html
Words: 200
Extraction Method: html

Skip to content  Concepts This section explains the core concepts and architecture behind AO, helping you understand how the system works at a fundamental level.System Architecture AO is built on a few fundamental principles that form its foundation:Two core types: Messages and Processes - the basic building blocks of the AO ecosystem No shared state, only Holographic State - a unique approach to distributed computing Decentralized Computer (Grid) - enabling truly distributed applications Core Components Explore these foundational concepts to gain a deeper understanding of AO:How it Works - An overview of AO's architecture and how the different parts interact Processes - Learn about processes, the computational units in AO Messages - Understand the messaging system that enables communication Evaluation - Discover how code execution works in the AO environment Units - Learn about the computational units that power the AO network Technical Foundations Specifications - Detailed technical specifications for the AO protocol Getting Started Meet Lua - Introduction to Lua, the programming language used in AO AO System Tour - A guided tour of the AO system and its capabilities Use the sidebar to navigate between concept topics. Each document provides in-depth information about a specific aspect of AO.

---

# 238. Processes  Cookbook

Document Number: 238
Source: https://cookbook_ao.arweave.net/concepts/processes.html
Words: 316
Extraction Method: html

Processes Processes possess the capability to engage in communication via message passing, both receiving and dispatching messages within the network. Additionally, they hold the potential to instantiate further processes, enhancing the network's computational fabric. This dynamic method of data dissemination and interaction within the network is referred to as a 'holographic state', underpinning the shared and persistent state of the network. When building a Process with aos you have the ability to add handlers, these handlers can be added by calling the Handlers.add function, passing a "name", a "match" function, and a "handle" function. The core module contains a helper library that gets injected into the handler function, this library is called ao.The main functions to look at in this ao helper is ao.send(Message) - sends a message to a process ao.spawn(Module, Message) - creates a new process Ethereum Signed Process or Module For an ao Process or Module, if the ANS-104 DataItem was signed using Ethereum keys, then the value in the env.Process.Owner or env.Module.Owner field, respectively, will be the EIP-55 Ethereum address of the signer. For example: 0xfB6916095ca1df60bB79Ce92cE3Ea74c37c5d359 ao.send Example ao.spawn Example ao.env NOTE: ao.env is important context data that you may need as a developer creating processes.The ao.env property contains the Process and Module Reference Objects Both the Process and the Module contain the attributes of the ao Data-Protocol.Summary Processes in the network communicate through message passing and can create new processes, contributing to a 'holographic state' of shared and persistent data. Developers can build a Process using aos by adding handlers through the Handlers.add function with specific name, match, and handle functions. The ao helper library within the core module aids in this process, providing functions like ao.send to dispatch messages and ao.spawn to create new modules, as well as the important ao.env property which contains essential Process and Module information. The ao Data-Protocol outlines the structure and attributes of these elements.

---

# 239. Sending an Assignment to a Process  Cookbook

Document Number: 239
Source: https://cookbook_ao.arweave.net/guides/aoconnect/assign-data.html
Words: 202
Extraction Method: html

Sending an Assignment to a Process Assignments can be used to load Data from another Message into a Process. Or to not duplicate Messages. You can create one Message and then assign it to any number of processes. This will make it available to the Processes you have sent an Assignment to.Sending an Assignment in NodeJS Excluding DataItem fields You can also exclude most DataItem fields which will tell the CU not to load them into your process. You may want to do this if you need only the header data like the Tags and not the Data itself etc... If you exclude the Owner it wont have any effect because the CU requires the Owner, so excluding Owner will be ignored by the CU. Only capitalized DataItem/Message fields will have an effect in the CU.Assigning L1 Transactions You can also assign a layer 1 transaction by passing the baseLayer param into assign. This is useful for minting tokens etc... using the base layer. By default, if the L1 tx does not have at least 20 confirmations the SU will reject it. This can be changed by setting the Settlement-Depth tag to a different number on the Process when it is created.

---

# 240. Connecting to specific ao nodes  Cookbook

Document Number: 240
Source: https://cookbook_ao.arweave.net/guides/aoconnect/connecting.html
Words: 176
Extraction Method: html

Skip to content  Connecting to specific ao nodes When including ao connect in your code you have the ability to connect to a specific MU and CU, as well as being able to specify an Arweave gateway. This can be done by importing the "connect" function and extracting the functions from a call to the "connect" function.You may want to do this if you want to know which MU is being called when you send your message so that later you can debug from the specified MU. You also may want to read a result from a specific CU. You may in fact just prefer a particular MU and CU for a different reason. You can specify the gateway in order to use something other than the default, which is arweave.net.Importing without a call to connect Connecting to a specific MU, CU, and gateway  All three of these parameters to connect are optional and it is valid to specify only 1 or 2 of them, or none. You could pass in just the MU_URL, for example.

---

# 241. Units  Cookbook

Document Number: 241
Source: https://cookbook_ao.arweave.net/concepts/units.html
Words: 419
Extraction Method: html

Units                                           What is a Unit?The ao Computer is composed of three Unit types, each type contains a set of responsibilities for the computer. And each Unit is horizontally scalable.In ao we have the Messenger Unit or MU, and the Scheduler Unit or SU, and the Compute Unit or the CU. These units are the building blocks of the ao Computer Grid. There can be 1 or more of these units on the network and they work together to power the ao Operating System or aos. Messenger Unit - This unit is the front door to ao, it receives all the messages from the outside and as well as directs traffic flow for Processes. This traffic flow we call pushing. Each process can return an Outbox when it evaluates a Message, and this Outbox can be filled with Messages or requests to Spawn new processes, and the Messenger Unit is responsible for extracting these Messages from the Outbox and signing them and sending them to the Scheduler Units for processing. Scheduler Unit - The Scheduler unit is responsible for ordering the messages, and storing those messages on Arweave. It is important that every message is appropriately ordered so that the evaluation can be replayed and verified. The Scheduler Unit is responsible for this process. It provides the abilty to query it via an endpoint to get the order of messages for evaluation. Compute Unit - The Compute unit is responsible for compute, this unit loads the binary module and manages the memory of that module, so that the execution of the process is alway running on the most up to date memory. The compute unit provides the results of the evaluation back to the the messenger unit, which can then push any messages in the outbox of the given process.Summary The ao Computer consists of three scalable unit types—Messenger Unit (MU), Scheduler Unit (SU), and Compute Unit (CU)—which form the foundation of the ao Computer. These units can exist in multiples on the network and collectively operate the ao Operating System (aos).The MU acts as the entry point, receiving external messages and managing process communications. It processes outgoing messages and spawn requests from process outboxes and forwards them to the SU.The SU ensures messages are properly sequenced and stored on Arweave, maintaining order for consistent replay and verification of message evaluations.The CU handles computation, loading binary modules, and managing memory to ensure processes run with current data. It then returns the evaluation results to the MU for further message handling.

---

# 242. Calling DryRun  Cookbook

Document Number: 242
Source: https://cookbook_ao.arweave.net/guides/aoconnect/calling-dryrun.html
Words: 121
Extraction Method: html

Skip to content  Calling DryRun DEPRECATION NOTICE This method of reading state is in the process of being deprecated for processes running on HyperBEAM. It is recommended to use the State Patching mechanism to expose state via HTTP for better performance, as calling dryrun was known to cause severe bottlenecks in web applications on legacynet.DryRun is the process of sending a message object to a specific process and getting the Result object back, but the memory is not saved, it is perfect to create a read message to return the current value of memory. For example, a balance of a token, or a result of a transfer, etc. You can use DryRun to obtain an output without sending an actual message.

---

# 243. Reading results from an ao Process  Cookbook

Document Number: 243
Source: https://cookbook_ao.arweave.net/guides/aoconnect/reading-results.html
Words: 136
Extraction Method: html

Skip to content  Reading results from an ao Process In ao, messages produce results which are made available by Compute Units (CU's). Results are JSON objects consisting of the following fields: messages, spawns, output and error.Results are what the ao system uses to send messages and spawns that are generated by processes. A process can send a message just like you can as a developer, by returning messages and spawns in a result.You may want to access a result to display the output generated by your message. Or you may want to see what messages etc., were generated. You do not need to take the messages and spawns from a result and send them yourself. They are automatically handled by Messenger Units (MU's). A call to results can also provide you paginated list of multiple results.

---

# 244. Chatroom Blueprint  Cookbook

Document Number: 244
Source: https://cookbook_ao.arweave.net/guides/aos/blueprints/chatroom.html
Words: 167
Extraction Method: html

Skip to content  Chatroom Blueprint The Chatroom Blueprint is a predesigned template that helps you quickly build a chatroom in ao. It is a great way to get started and can be customized to fit your needs.Unpacking the Chatroom Blueprint Members: The Members array is used to store the users who have registered to the chatroom.Register Handler: The register handler allows processes to join the chatroom. When a process sends a message with the tag Action = "Register", the handler will add the process to the Members array and send a message back to the process confirming the registration.Broadcast Handler: The broadcast handler allows processes to send messages to all the members of the chatroom. When a process sends a message with the tag Action = "Broadcast", the handler will send the message to all the members of the chatroom.How To Use:Open your preferred text editor.Open the Terminal.Start your aos process.Type in .load-blueprint chatroom Verify the Blueprint is Loaded:Type in Handlers.list to see the newly loaded handlers.

---

# 245. Understanding the Inbox  Cookbook

Document Number: 245
Source: https://cookbook_ao.arweave.net/guides/aos/inbox-and-handlers.html
Words: 211
Extraction Method: html

Understanding the Inbox In aos, processes are executed in response to messages via handlers. Unhandled messages are routed to the process's Inbox.What are Handlers?A handler is a function that receives and evaluates messages within your process. It acts upon messages by taking them as parameters.Handlers are defined using the Handlers.add() function.The function takes three parameters:Name of the Handler Matcher function Handle function What about Inboxes?An inbox is a storage area for messages that have not yet been processed. Think of it as a holding zone for incoming, or "inbound," items awaiting handling. Once a message is processed, it's no longer considered "inbound" and thus leaves the inbox.Example: Consider the inbox like your voicemail. Just as an unanswered phone call is directed to voicemail for you to address later, messages that your Process doesn't immediately handle are sent to the inbox. This way, unhandled messages are stored until you're ready to process them.Summary Initially, it might seem like all messages are meant to land in your Inbox, which can be puzzling if they disappear after being handled. The analogy of a voicemail should clarify this: much like calls you answer don't go to voicemail, messages you handle won't appear in your Inbox. This illustrates the roles of both the Inbox and Handlers.

---

# 246. Introduction  Cookbook

Document Number: 246
Source: https://cookbook_ao.arweave.net/guides/aos/intro.html
Words: 524
Extraction Method: html

Introduction aos introduces a new approach to building processes — asynchronous, parallel-executing smart contracts. The ao computer is a decentralized computer network that allows compute to run anywhere and aos in a unique, interactive shell. You can use aos as your personal operating system, your development environment for building ao processes, and your bot army.Lets go over some basic commands.Variables If you want to display the contents of any variable through the console, simply type the variable name.luaName Inbox the Inbox is a collection of messages that your Process has received.luaInbox[1] If you want to get a count of messages, just add the # infront of Inbox.lua#Inbox The process of checking how many messages are in the inbox is a very common pattern. To make this easier, you can create a function that returns the number of messages within the inbox and displays it in the prompt.Use either .editor or .load file to load this function on your process.The Expected Results:Your prompt now has changed to include the number of messages in your inbox.INFO The Inbox is a Lua table (similar to an array) that contains messages received by your process that were not handled by any Handlers. The # operator is used to get the length of a table in Lua - so #Inbox returns the total number of unhandled messages currently in your inbox. This is a common Lua syntax pattern for getting the size/length of tables and strings.Globals In aos process there are some Globals that can make development a little more intuitive.Name Description Type Inbox This is a lua Table that stores all the messages that are received and not handlers by any handlers.Table(Array) Send(Message) This is a global function that is available in the interactive environment that allows you to send messages to Processes function Spawn(Module, Message) This is a global function that is available in the aos interactive environment that allows you to spawn processes  Name a string that is set on init that describes the name of your process string Owner a string that is set on the init of the process that documents the owner of the process, warning if you change this value, it can brick you ability to interact with your process string Handlers a lua Table that contains helper functions that allows you to create handlers that execute functionality based on the pattern matching function on inbound messages table Dump a function that takes any lua Table and generates a print friendly output of the data function Utils a functional utility library with functions like map, reduce, filter module ao this is a core function library for sending messages and spawing processes module Modules In aos there are some built in common lua modules that are already available for you to work with, these modules can be referenced with a "require" function.Name Description json a json module that allows you to encode and decode json documents ao contains ao specific functions like send and spawn.base64 a base64 module that allows you to encode and decode base64 text.pretty a pretty print module using the function tprint to output formatted syntax.utils an utility function library

---

# 247. Load Lua Files with load filename  Cookbook

Document Number: 247
Source: https://cookbook_ao.arweave.net/guides/aos/load.html
Words: 101
Extraction Method: html

Skip to content  Load Lua Files with.load <filename> This feature allows you to load lua code from a source file on your local machine, this simple feature gives you a nice DX experience for working with aos processes.When creating handlers you may have a lot of code and you want to take advantage of a rich development environment like vscode. You can even install the lua extension to get some syntax checking.So how do you publish your local lua source code to your ao process? This is where the .load command comes into play.hello.lua aos shell lua.load hello.lua Easy Peasy! 🐶

---

# 248. Base64  Cookbook

Document Number: 248
Source: https://cookbook_ao.arweave.net/guides/aos/modules/base64.html
Words: 231
Extraction Method: html

Base64 A small base64 module to encode or decode base64 text.Note: It is recommended to enable caching for large chunks of texts for up to x2 optimization.Example usage Module functions encode() This function encodes the provided string using the default encoder table. The encoder can be customized and a cache is available for larger chunks of data.Parameters:str: {string} The string to encode encoder: {table} Optional custom encoding table usecache: {boolean} Optional cache for large strings (turned off by default) Returns: Base64 encoded string Examples decode() This function decodes the provided base64 encoded string using the default decoder table. The decoder can be customized and a cache is also available here.Parameters:str: {string} The base64 encoded string to decode decoder: {table} Optional custom decoding table usecache: {boolean} Optional cache for large strings (turned off by default) Returns: Decoded string Examples makeencoder() Allows creating a new encoder table to customize the encode() function's result.Parameters:s62: {string} Optional custom char for 62 (+ by default) s63: {string} Optional custom char for 63 (/ by default) spad: {string} Optional custom padding char (= by default) Returns: Custom encoder table Examples makedecoder() Allows creating a new decoder table to be able to decode custom-encoded base64 strings.Parameters:s62: {string} Optional custom char for 62 (+ by default) s63: {string} Optional custom char for 63 (/ by default) spad: {string} Optional custom padding char (= by default) Returns: Custom decoder table

---

# 249. crypto  Cookbook

Document Number: 249
Source: https://cookbook_ao.arweave.net/guides/aos/modules/crypto.html
Words: 2302
Extraction Method: html

crypto Overview The crypto module provides a set of cryptographic primitives like digests, ciphers and other cryptographic algorithms in pure Lua. It offers several functionalities to hash, encrypt and decrypt data, simplifying the development of secure communication and data storage. This document will guide you through the module's functionalities, installation, and usage.Usage Primitives Digests (sha1, sha2, sha3, keccak, blake2b, etc.) Ciphers (AES, ISSAC, Morus, NORX, etc.) Random Number Generators (ISAAC) MACs (HMAC) KDFs (PBKDF2) Utilities (Array, Stream, Queue, etc.)  Digests MD2 Calculates the MD2 digest of a given message.Parameters:stream (Stream): The message in form of stream Returns: A table containing functions to get digest in different formats.asBytes(): The digest as byte table.asHex(): The digest as string in hexadecimal format.asString(): The digest as string format.Example:MD4 Calculates the MD4 digest of a given message.Parameters:stream (Stream): The message in form of stream Returns: A table containing functions to get digest in different formats.asBytes(): The digest as byte table.asHex(): The digest as string in hexadecimal format.asString(): The digest as string format.Example:MD5 Calculates the MD5 digest of a given message.Parameters:stream (Stream): The message in form of stream Returns: A table containing functions to get digest in different formats.asBytes(): The digest as byte table.asHex(): The digest as string in hexadecimal format.asString(): The digest as string format.Example:SHA1 Calculates the SHA1 digest of a given message.Parameters:stream (Stream): The message in form of stream Returns: A table containing functions to get digest in different formats.asBytes(): The digest as byte table.asHex(): The digest as string in hexadecimal format.asString(): The digest as string format.Example:SHA2_256 Calculates the SHA2-256 digest of a given message.Parameters:stream (Stream): The message in form of stream Returns: A table containing functions to get digest in different formats. asBytes(): The digest as byte table.asHex(): The digest as string in hexadecimal format.asString(): The digest as string format.Example:SHA2_512 Calculates the SHA2-512 digest of a given message.Parameters:msg (string): The message to calculate the digest Returns: A table containing functions to get digest in different formats. asBytes(): The digest as byte table.asHex(): The digest as string in hexadecimal format.asString(): The digest as string format.Example:SHA3 It contains the following functions:sha3_256 sha3_512 keccak256 keccak512 Each function calculates the respective digest of a given message.Parameters:msg (string): The message to calculate the digest Returns: A table containing functions to get digest in different formats.asBytes(): The digest as byte table.asHex(): The digest as string in hexadecimal format.asString(): The digest as string format.Example:Blake2b Calculates the Blake2b digest of a given message.Parameters:data (string): The data to be hashed.outlen (number): The length of the output hash (optional) default is 64.key (string): The key to be used for hashing (optional) default is "".Returns: A table containing functions to get digest in different formats.asBytes(): The digest as byte table.asHex(): The digest as string in hexadecimal format.asString(): The digest as string format.Example: Ciphers AES The Advanced Encryption Standard (AES) is a symmetric block cipher used to encrypt sensitive information. It has two functions encrypt and decrypt.Encrypt Encrypts a given message using the AES algorithm.Parameters:data (string): The data to be encrypted.key (string): The key to be used for encryption.iv (string) optional: The initialization vector to be used for encryption. default is "" mode (string) optional: The mode of operation to be used for encryption. default is "CBC". Available modes are CBC, ECB, CFB, OFB, CTR.keyLength (number) optional: The length of the key to use for encryption. default is 128.Returns: A table containing functions to get encrypted data in different formats.asBytes(): The encrypted data as byte table.asHex(): The encrypted data as string in hexadecimal format.asString(): The encrypted data as string format.Decrypt Decrypts a given message using the AES algorithm.Parameters:cipher (string): Hex Encoded encrypted data.key (string): The key to be used for decryption.iv (string) optional: The initialization vector to be used for decryption. default is "" mode (string) optional: The mode of operation to be used for decryption. default is "CBC". Available modes are CBC, ECB, CFB, OFB, CTR.keyLength (number) optional: The length of the key to use for decryption. default is 128.Returns: A table containing functions to get decrypted data in different formats.asBytes(): The decrypted data as byte table.asHex(): The decrypted data as string in hexadecimal format.asString(): The decrypted data as string format.Example:ISSAC Cipher ISAAC is a cryptographically secure pseudo-random number generator (CSPRNG) and stream cipher. It has the following functions seedIsaac: Seeds the ISAAC cipher with a given seed.getRandomChar: Generates a random character using the ISAAC cipher.random: Generates a random number between a given range using the ISAAC cipher.getRandom: Generates a random number using the ISAAC cipher.encrypt: Encrypts a given message using the ISAAC cipher.decrypt: Decrypts a given message using the ISAAC cipher.Encrypt Encrypts a given message using the ISAAC cipher.Parameters:msg (string): The message to be encrypted.key (string): The key to be used for encryption.Returns: A table containing functions to get encrypted data in different formats. asBytes(): The encrypted data as byte table.asHex(): The encrypted data as string in hexadecimal format.asString(): The encrypted data as string format.Decrypt Decrypts a given message using the ISAAC cipher.Parameters:cipher (string): Hex Encoded encrypted data.key (string): Key to be used for decryption.Returns: A table containing functions to get decrypted data in different formats. asBytes(): The decrypted data as byte table.asHex(): The decrypted data as string in hexadecimal format.asString(): The decrypted data as string format.Example:random Generates a random number using the ISAAC cipher.Parameters:min (number) optional: The minimum value of the random number. defaults to 0.max (number) optional: The maximum value of the random number. defaults to 2^31 - 1.seed (string) optional: The seed to be used for generating the random number. defaults to math.random(0,2^32 - 1).Returns: A random number between the given range.Example:Morus Cipher MORUS is a high-performance authenticated encryption algorithm submitted to the CAESAR competition, and recently selected as a finalist.Encrypt Encrypts a given message using the MORUS cipher.Parameters:key (string): The encryption key (16 or 32-byte string).iv (string): The nonce or initial value (16-byte string).msg (string): The message to encrypt (variable length string).ad (string) optional: The additional data (variable length string). defaults to "".Returns: A table containing functions to get encrypted data in different formats. asBytes(): The encrypted data as byte table.asHex(): The encrypted data as string in hexadecimal format.asString(): The encrypted data as string format.Decrypt Decrypts a given message using the MORUS cipher.Parameters:key (string): The encryption key (16 or 32-byte string).iv (string): The nonce or initial value (16-byte string).cipher (string): The encrypted message (variable length string).adLen (number) optional: The length of the additional data (variable length string). defaults to 0.Returns: A table containing functions to get decrypted data in different formats. asBytes(): The decrypted data as byte table.asHex(): The decrypted data as string in hexadecimal format.asString(): The decrypted data as string format.Example:NORX Cipher NORX is an authenticated encryption scheme with associated data that was selected, along with 14 other primitives, for the third phase of the ongoing CAESAR competition. It is based on the sponge construction and relies on a simple permutation that allows efficient and versatile implementations.Encrypt Encrypts a given message using the NORX cipher.Parameters:key (string): The encryption key (32-byte string).nonce (string): The nonce or initial value (32-byte string).plain (string): The message to encrypt (variable length string).header (string) optional: The additional data (variable length string). defaults to "".trailer (string) optional: The additional data (variable length string). defaults to "".Returns: A table containing functions to get encrypted data in different formats. asBytes(): The encrypted data as byte table.asHex(): The encrypted data as string in hexadecimal format.asString(): The encrypted data as string format.Decrypt Decrypts a given message using the NORX cipher.Parameters:key (string): The encryption key (32-byte string).nonce (string): The nonce or initial value (32-byte string).crypted (string): The encrypted message (variable length string).header (string) optional: The additional data (variable length string). defaults to "".trailer (string) optional: The additional data (variable length string). defaults to "".Returns: A table containing functions to get decrypted data in different formats. asBytes(): The decrypted data as byte table.asHex(): The decrypted data as string in hexadecimal format.asString(): The decrypted data as string format.Example: Random Number Generators The module contains a random number generator using ISAAC which is a cryptographically secure pseudo-random number generator (CSPRNG) and stream cipher.Parameters:min (number) optional: The minimum value of the random number. defaults to 0.max (number) optional: The maximum value of the random number. defaults to 2^31 - 1.seed (string) optional: The seed to be used for generating the random number. defaults to math.random(0,2^32 - 1).Returns: A random number between the given range.Example: MACs HMAC The Hash-based Message Authentication Code (HMAC) is a mechanism for message authentication using cryptographic hash functions. HMAC can be used with any iterative cryptographic hash function, e.g., MD5, SHA-1, in combination with a secret shared key.The modules exposes a function called createHmac which is used to create a HMAC instance.Parameters:data (Stream): The data to be hashed.key (Array): The key to be used for hashing.algorithm (string) optional: The algorithm to be used for hashing. default is "sha256". Available algorithms are "sha1", "sha256". default is "sha1".Returns: A table containing functions to get HMAC in different formats. asBytes(): The HMAC as byte table.asHex(): The HMAC as string in hexadecimal format.asString(): The HMAC as string format.Example: KDFs PBKDF2 The Password-Based Key Derivation Function 2 (PBKDF2) applies a pseudorandom function, such as hash-based message authentication code (HMAC), to the input password or passphrase along with a salt value and repeats the process many times to produce a derived key, which can then be used as a cryptographic key in subsequent operations.Parameters:password (Array): The password to derive the key from.salt (Array): The salt to use.iterations (number): The number of iterations to perform.keyLen (number): The length of the key to derive.digest (string) optional: The digest algorithm to use. default is "sha1". Available algorithms are "sha1", "sha256".Returns: A table containing functions to get derived key in different formats. asBytes(): The derived key as byte table.asHex(): The derived key as string in hexadecimal format.asString(): The derived key as string format.Example: Utilities Array Example Usage:size Returns the size of the array.Parameters:arr (Array): The array to get the size of.Returns: The size of the array.fromString Creates an array from a string.Parameters:str (string): The string to create the array from.Returns: The array created from the string.toString Converts an array to a string.Parameters:arr (Array): The array to convert to a string.Returns: The array as a string.fromStream Creates an array from a stream.Parameters:stream (Stream): The stream to create the array from.Returns: The array created from the stream.readFromQueue Reads data from a queue and stores it in the array.Parameters:queue (Queue): The queue to read data from.size (number): The size of the data to read.Returns: The array containing the data read from the queue.writeToQueue Writes data from the array to a queue.Parameters:queue (Queue): The queue to write data to.array (Array): The array to write data from.Returns: None toStream Converts an array to a stream.Parameters:arr (Array): The array to convert to a stream.Returns: (Stream) The array as a stream.fromHex Creates an array from a hexadecimal string.Parameters:hex (string): The hexadecimal string to create the array from.Returns: The array created from the hexadecimal string.toHex Converts an array to a hexadecimal string.Parameters:arr (Array): The array to convert to a hexadecimal string.Returns: The array as a hexadecimal string.concat Concatenates two arrays.Parameters:a (Array): The array to concatenate with.b (Array): The array to concatenate.Returns: The concatenated array.truncate Truncates an array to a given length.Parameters:a (Array): The array to truncate.newSize (number): The new size of the array.Returns: The truncated array.XOR Performs a bitwise XOR operation on two arrays.Parameters:a (Array): The first array.b (Array): The second array.Returns: The result of the XOR operation.substitute Creates a new array with keys of first array and values of second Parameters:input (Array): The array to substitute.sbox (Array): The array to substitute with.Returns: The substituted array.permute Creates a new array with keys of second array and values of first array.Parameters:input (Array): The array to permute.pbox (Array): The array to permute with.Returns: The permuted array.copy Creates a copy of an array.Parameters:input (Array): The array to copy.Returns: The copied array.slice Creates a slice of an array.Parameters:input (Array): The array to slice.start (number): The start index of the slice.stop (number): The end index of the slice.Returns: The sliced array. Stream Stream is a data structure that represents a sequence of bytes. It is used to store and manipulate data in a streaming fashion.Example Usage:fromString Creates a stream from a string.Parameters:str (string): The string to create the stream from.Returns: The stream created from the string.toString Converts a stream to a string.Parameters:stream (Stream): The stream to convert to a string.Returns: The stream as a string.fromArray Creates a stream from an array.Parameters:arr (Array): The array to create the stream from.Returns: The stream created from the array.toArray Converts a stream to an array.Parameters:stream (Stream): The stream to convert to an array.Returns: The stream as an array.fromHex Creates a stream from a hexadecimal string.Parameters:hex (string): The hexadecimal string to create the stream from.Returns: The stream created from the hexadecimal string.toHex Converts a stream to a hexadecimal string.Parameters:stream (Stream): The stream to convert to a hexadecimal string.Returns: The stream as a hexadecimal string. Hex Example Usage:hexToString Converts a hexadecimal string to a string.Parameters:hex (string): The hexadecimal string to convert to a string.Returns: The hexadecimal string as a string.stringToHex Converts a string to a hexadecimal string.Parameters:str (string): The string to convert to a hexadecimal string.Returns: The string as a hexadecimal string. Queue Queue is a data structure that represents a sequence of elements. It is used to store and manipulate data in a first-in, first-out (FIFO) fashion.Example Usage:push Pushes an element to the queue.Parameters:queue (Queue): The queue to push the element to.element (any): The element to push to the queue.Returns: None pop Pops an element from the queue.Parameters:queue (Queue): The queue to pop the element from.element (any): The element to pop from the queue.Returns: The popped element.size Returns the size of the queue.Parameters: None Returns: The size of the queue.getHead Returns the head of the queue.Parameters: None Returns: The head of the queue.getTail Returns the tail of the queue.Parameters: None Returns: The tail of the queue.reset Resets the queue.Parameters: None

---

# 250. Creating a Pingpong Process in aos  Cookbook

Document Number: 250
Source: https://cookbook_ao.arweave.net/guides/aos/pingpong.html
Words: 358
Extraction Method: html

Creating a Pingpong Process in aos This tutorial will guide you through creating a simple "ping-pong" process in aos. In this process, whenever it receives a message with the data "ping", it will automatically reply with "pong". This is a basic example of message handling and interaction between processes in aos.Step 1: Open the aos CLI Start by opening your command-line interface and typing aos to enter the aos environment.Step 2: Access the Editor Type .editor in the aos CLI to open the inline text editor. This is where you'll write your ping-pong handler code.Step 3: Write the Pingpong Handler In the editor, enter the following Lua code to add a handler for the pingpong pattern:This lua script does three things:It adds a new handler named "pingpong".It uses Handlers.utils.hasMatchingData("ping") to check if incoming messages contain the data "ping".If the message contains "ping", Handlers.utils.reply("pong") automatically sends back a message with the data "pong".Step 4: Exit the Editor After writing your code, type .done and press Enter to exit the editor and run the script.Step 5: Test the Pingpong Process To test the process, send a message with the data "ping" to the process. You can do this by typing the following command in the aos CLI:The process should respond with a message containing "pong" in the Inbox.Step 6: Monitor the Inbox Check your Inbox to see the "ping" message and your Outbox to confirm the "pong" reply.luaInbox[#Inbox].Data Step 7: Experiment and Observe Experiment by sending different messages and observe how only the "ping" messages trigger the "pong" response.Step 8: Save Your Process (Optional) If you want to use this process in the future, save the handler code in a Lua file for easy loading into aos sessions.INFO ADDITIONAL TIP:Handler Efficiency: The simplicity of the handler function is key. Ensure that it's efficient and only triggers under the correct conditions.Conclusion Congratulations! You have now created a basic ping-pong process in aos. This tutorial provides a foundation for understanding message handling and process interaction within the aos environment. As you become more comfortable with these concepts, you can expand to more complex processes and interactions, exploring the full potential of aos.

---

# 251. Utils  Cookbook

Document Number: 251
Source: https://cookbook_ao.arweave.net/guides/aos/modules/utils.html
Words: 541
Extraction Method: html

Utils A utility library for generic table manipulation and validation. It supports both curry-styled and traditional programming.Note: It is important to verify that the inputs provided to the following functions match the expected types.Example usage Module functions concat() This function concatenates array b to array a.Parameters:a: {table} The base array b: {table} The array to concat to the base array Returns: An unified array of a and b Examples reduce() This function executes the provided reducer function for all array elements, finally providing one (unified) result.Parameters:fn: {function} The reducer function. It receives the previous result, the current element's value and key in this order initial: {any} An optional initial value t: {table} The array to reduce Returns: A single result from running the reducer across all table elements Examples map() This function creates a new array filled with the results of calling the provided map function on each element in the provided array.Parameters:fn: {function} The map function. It receives the current array element and key data: {table} The array to map Returns: A new array composed of the results of the map function Examples filter() This function creates a new array from a portion of the original, only keeping the elements that passed a provided filter function's test.Parameters:fn: {function} The filter function. It receives the current array element and should return a boolean, deciding whether the element should be kept (true) or filtered out (false) data: {table} The array to filter Returns: The new filtered array Examples find() This function returns the first element that matches in a provided function.Parameters:fn: {function} The find function that receives the current element and returns true if it matches, false if it doesn't t: {table} The array to find an element in Returns: The found element or nil if no element matched Examples reverse() Transforms an array into reverse order.Parameters:data: {table} The array to reverse Returns: The original array in reverse order Example includes() Determines whether a value is part of an array.Parameters:val: {any} The element to check for t: {table} The array to check in Returns: A boolean indicating whether or not the provided value is part of the array Examples keys() Returns the keys of a table.Parameters:table: {table} The table to get the keys for Returns: An array of keys Example values() Returns the values of a table.Parameters:table: {table} The table to get the values for Returns: An array of values Example propEq() Checks if a specified property of a table equals with the provided value.Parameters:propName: {string} The name of the property to compare value: {any} The value to compare to object: {table} The object to select the property from Returns: A boolean indicating whether the property value equals with the provided value or not Examples prop() Returns the property value that belongs to the property name provided from an object.Parameters:propName: {string} The name of the property to get object: {table} The object to select the property value from Returns: The property value or nil if it was not found Examples compose() This function allows you to chain multiple array mutations together and execute them in reverse order on the provided array.Parameters:...: {function[]} The array mutations v: {table} The object to execute the provided functions on Returns: The result from the provided mutations

---

# 252. Customizing the Prompt in aos  Cookbook

Document Number: 252
Source: https://cookbook_ao.arweave.net/guides/aos/prompt.html
Words: 203
Extraction Method: html

Customizing the Prompt in aos Step 1: Open aos and Start the Editor Launch the aos command-line interface.Enter .editor to open the inline text editor.Step 2: Write the Custom Prompt Function In the editor, define your custom prompt function. For example:Customize "YourName@aos> " to your preferred prompt text.Step 3: Exit and Run Your Code To exit the editor and execute your code, type .done and then press Enter.Your aos prompt should now display the new custom format.Step 4: Save for Future Use (Optional) If you wish to use this prompt in future aos sessions, save your script in a Lua file.In subsequent sessions, load this script to apply your custom prompt.Maximizing Your Prompt There's a great deal of utility and creativity that can come from customizing your prompt. Several things you can do within your prompt are:Tracking the number of unhandled messages you have in your inbox by creating a function that shows how many messages you have.Tracking the number of members are within your process ID's chatroom.Tracking the balance of a specified token that your process ID holds.Conclusion Now that you understand how to maximize the utility within your Prompt, you've now gained a crucial step to streamlining your ao development experience.

---

# 253. Connecting to HyperBEAM with aos  Cookbook

Document Number: 253
Source: https://cookbook_ao.arweave.net/guides/migrating-to-hyperbeam/aos-with-hyperbeam.html
Words: 254
Extraction Method: html

Connecting to HyperBEAM with aos This guide explains how to use aos, the command-line interface for AO, to connect to a HyperBEAM node for development.Installing aos The primary tool for interacting with AO and developing processes is aos, a command-line interface and development environment.Connecting to a HyperBEAM Node While you don't need to run a HyperBEAM node yourself, you do need to connect to one to interact with the network during development.To start aos and connect to a public HyperBEAM node, simply run the command in your terminal:This connects you to an interactive Lua environment running within a process on the AO network. This process acts as your command-line interface (CLI) to the AO network. When you specify --mainnet <URL>, it connects to the genesis_wasm device running on the HyperBEAM node at the supplied URL, allowing you to interact with other processes, manage your wallet, and develop new AO processes.Running a Local HyperBEAM Node If you are running HyperBEAM locally and want to use that node when booting up aos, you must first start your local node with the genesis_wasm profile:Then, you can connect aos to it:Until aos is fully HyperBEAM native, the genesis_wasm profile is required to run a local Compute Unit (CU) for executing aos.Interacting with Mainnet Processes Note on Blocking Calls Blocking message patterns, such as Receive and ao.send().receive(), are not available when running aos against a HyperBEAM process. HyperBEAM processes do not support the underlying wasm modules required for this functionality. You should rely on asynchronous patterns using handlers instead.

---

# 254. Why Migrate to HyperBEAM  Cookbook

Document Number: 254
Source: https://cookbook_ao.arweave.net/guides/migrating-to-hyperbeam/why-migrate.html
Words: 137
Extraction Method: html

Skip to content  Why Migrate to HyperBEAM?Migrating processes from legacynet to HyperBEAM is essential for leveraging significant advancements in performance, features, and developer experience on AO.HyperBEAM is a new, more robust foundation for decentralized applications on AO, offering several key advantages:Enhanced Performance: Built on an architecture optimized for concurrency, HyperBEAM provides faster message scheduling and more responsive applications.Direct State Access: HyperBEAM allows processes to expose their state directly via HTTP. This enables immediate reads of your process's data, eliminating the need for dry-run messages which were a common performance bottleneck.Easy Extensibility: It allows core feature extensibility through modular devices.The most impactful change when migrating is the ability to expose parts of your process state for immediate reading. This dramatically improves the performance of web frontends and data-driven services.To learn how to implement this, see Exposing Process State.

---

# 255. ao Module  Cookbook

Document Number: 255
Source: https://cookbook_ao.arweave.net/references/ao.html
Words: 590
Extraction Method: html

ao Module version: 0.0.3 ao process communication is handled by messages, each process receives messages in the form of ANS-104 DataItems, and needs to be able to do the following common operations.ao.send(msg) - send message to another process ao.spawn(module, msg) - spawn a process The goal of this library is to provide this core functionality in the box of the ao developer toolkit. As a developer you have the option to leverage this library or not, but it integrated by default.Properties Name Description Type id Process Identifier (TxID) string _module Module Identifier (TxID) string authorities Set of Trusted TXs string Authority Identifiers that the process is able to accept transactions from that are not the owner or the process (0-n) string _version The version of the library string reference Reference number of the process number env Evaluation Environment object outbox Holds Messages and Spawns for response object assignables List of assignables of the process list nonExtractableTags List of non-extractable tags of the process list nonForwardableTags List of non-forwardable tags of the process list init Initializes the AO environment function send Sends a message to a target process function assign Assigns a message to the process function spawn Spawns a process function result Returns the result of a message function isTrusted Checks if a message is trusted function isAssignment Checks if a message is an assignment function isAssignable Checks if a message is assignable function addAssignable Adds an assignable to the assignables list function removeAssignable Removes an assignable from the assignables list function clearOutbox Clears the outbox function normalize Normalizes a message by extracting tags function sanitize Sanitizes a message by removing non-forwardable tags function clone Clones a table recursively function Environment Schema The ao.env variable contains information about the initializing message of the process. It follows this schema:Example Methods ao.send(msg: Message) Takes a Message as input. The function adds ao -specific tags and stores the message in ao.outbox.Messages.Example ao.spawn(module: string, spawn: Spawn) Takes a module ID string and Spawn as input. Returns a Spawn table with a generated Ref_ tag.Example ao.assign(assignment: Assignment) Takes an Assignment as input. Adds the assignment to ao.outbox.Assignments.Example ao.result(result: Result) Takes a Result as input. Returns the final process execution result.Example ao.isAssignable(msg: Message) Takes a Message as input. Returns true if the message matches a pattern in ao.assignables.Example ao.isAssignment(msg: Message) Takes a Message as input. Returns true if the message is assigned to a different process.Example ao.addAssignable(name: string, condition: function) Adds a named condition function to the process's list of assignables. Messages matching any condition will be accepted when assigned.Note: The condition parameter uses a similar pattern matching approach as the pattern parameter in Handlers.add(). For more advanced pattern matching techniques, see the Handlers Pattern Matching documentation.Example ao.removeAssignable(name: string) Removes a previously added assignable condition from the process's list of assignables.Example luaao.removeAssignable("allowArDrive") ao.isTrusted(msg: Message) Takes a Message as input. Returns true if the message is from a trusted source.Example Custom ao Table Structures Used by: ao.send(), ao.spawn(), ao.normalize(), ao.sanitize() All of the below syntaxes are valid, but each syntax gets converted to { name = string, value = string } tables behind the scenes. We use alternative 1 throughout the documentation for brevity and consistency.Root-level Tag Conversion Any keys in the root message object that are not one of: Target, Data, Anchor, Tags, or From will automatically be converted into Tags using the key as the tag name and its value as the tag value.Message Used by: ao.send(), ao.isTrusted(), ao.isAssignment(), ao.isAssignable(), ao.normalize(), ao.sanitize() Spawn Used by: ao.spawn() Assignment Used by: ao.assign(), ao.result() Result Used by: ao.result()

---

# 256. Cron Messages  Cookbook

Document Number: 256
Source: https://cookbook_ao.arweave.net/references/cron.html
Words: 251
Extraction Method: html

Cron Messages ao has the ability to generate messages on a specified interval, this interval could be seconds, minutes, hours, or blocks. These messages automatically get evaluated by a monitoring process to inform the Process to evaluate these messages over time. The result is a real-time Process that can communicate with the full ao network or oracles in the outside network.Setting up cron in a process The easiest way to create these cron messages is by spawning a new process in the aos console and defining the time interval.When spawning a new process, you can pass a cron argument in your command line followed by the interval you would like the cron to tick. By default, cron messages are lazily evaluated, meaning they will not be evaluated until the next scheduled message. To initiate these scheduled cron messages, call .monitor in aos - this kicks off a worker process on the mu that triggers the cron messages from the cu. Your Process will then receive cron messages every x-interval.lua.monitor If you wish to stop triggering the cron messages simply call .unmonitor and this will stop the triggering process, but the next time you send a message, the generated cron messages will still get created and processed.Handling cron messages Every cron message has an Action tag with the value Cron. Handlers can be defined to perform specific tasks autonomously, each time a cron message is received.Cron messages are a powerful utility that can be used to create "autonomous agents" with expansive capabilities.

---

# 257. Handlers (Version 005)  Cookbook

Document Number: 257
Source: https://cookbook_ao.arweave.net/references/handlers.html
Words: 943
Extraction Method: html

Handlers (Version 0.0.5) Overview The Handlers library provides a flexible way to manage and execute a series of process functions based on pattern matching. An AO process responds based on receiving Messages, these messages are defined using the Arweave DataItem specification which consists of Tags, and Data. Using the Handlers library, you can define a pipeline of process evaluation based on the attributes of the AO Message. Each Handler is instantiated with a name, a pattern matching function, and a function to execute on the incoming message. This library is suitable for scenarios where different actions need to be taken based on varying input criteria.Concepts Handler Arguments Overview When adding a handler using Handlers.add(), you provide three main arguments:name (string): The identifier for your handler pattern (table or function): Defines how to match incoming messages handler (function or resolver table): Defines what to do with matched messages Pattern Matching Tables Pattern Matching Tables provide a declarative way to match incoming messages based on their attributes. This is used as the second argument in Handlers.add() to specify which messages your handler should process.Basic Pattern Matching Rules Simple Tag Matching Wildcard Matching Pattern Matching Function-based Matching Common Pattern Examples Balance Action Handler Numeric Quantity Handler Default Action Handlers (AOS 2.0+) AOS 2.0 introduces simplified syntax for Action-based handlers. Instead of writing explicit pattern functions, you can use these shorthand forms:Resolvers Resolvers are special tables that can be used as the third argument in Handlers.add() to enable conditional execution of functions based on additional pattern matching. Each key in a resolver table is a pattern matching table, and its corresponding value is a function that executes when that pattern matches.This structure allows developers to create switch/case-like statements where different functions are triggered based on which pattern matches the incoming message. Resolvers are particularly useful when you need to handle a group of related messages differently based on additional criteria.Module Structure Handlers._version: String representing the version of the Handlers library.Handlers.list: Table storing the list of registered handlers.Common Handler Function Parameters Parameter Type Description name string The identifier of the handler item in the handlers list.pattern table or function Specifies how to match messages. As a table, defines required message tags with string values (e.g. { Action = "Balance", Recipient = "_" } requires an "Action" tag with string value "Balance" and any string "Recipient" tag value). As a function, takes a message DataItem and returns: "true" (invoke handler and exit pipeline), "false" (skip handler), or "continue" (invoke handler and continue pipeline).handler (Resolver) table or function Either a resolver table containing pattern-function pairs for conditional execution, or a single function that processes the message. When using a resolver table, each key is a pattern matching table and its value is the function to execute when that pattern matches. When using a function, it takes the message DataItem as an argument and executes business logic.maxRuns (optional) number As of 0.0.5, each handler function takes an optional function to define the amount of times the handler should match before it is removed. The default is infinity.Functions Handlers.add(name, pattern, handler) Adds a new handler or updates an existing handler by name Handlers.append(name, pattern, handler) Appends a new handler to the end of the handlers list.Handlers.once(name, pattern, handler) Only runs once when the pattern is matched. Equivalent to setting maxRuns = 1. This is the underlying implementation used by the Receive function in the messaging system.Handlers.prepend(name, pattern, handler) Prepends a new handler to the beginning of the handlers list.Handlers.before(handleName) Returns an object that allows adding a new handler before a specified handler.Handlers.after(handleName) Returns an object that allows adding a new handler after a specified handler.Handlers.remove(name) Removes a handler from the handlers list by name.Handler Execution Notes Execution Order Handlers are executed in the order they appear in Handlers.list.When a message arrives, each handler's pattern function is called sequentially to determine if it should process the message.Pattern Function Return Values Pattern functions determine the message handling flow based on their return values:Skip Handler (No Match) Return: 0, false, or any string except "continue" or "break" Effect: Skips current handler and proceeds to the next one in the list Handle and Continue Return: 1 or "continue" Effect: Processes the message and continues checking subsequent handlers Use Case: Ideal for handlers that should always execute (e.g., logging) Handle and Stop Return: -1, true, or "break" Effect: Processes the message and stops checking further handlers Use Case: Most common scenario where a handler exclusively handles its matched message Practical Examples Logging Handler: Place at the start of the list and return "continue" to log all messages while allowing other handlers to process them.Specific Message Handler: Return "break" to handle matched messages exclusively and prevent further processing by other handlers.Handlers.utils The Handlers.utils module provides two functions that are common matching patterns and one function that is a common handle function.hasMatchingData(data: string) hasMatchingTag(name: string, value: string) reply(text: string) Handlers.utils.hasMatchingData(data: string) This helper function returns a pattern matching function that takes a message as input. The returned function checks if the message's Data field contains the specified string. You can use this helper directly as the pattern argument when adding a new handler.Handlers.utils.hasMatchingTag(name: string, value: string) This helper function returns a pattern matching function that takes a message as input. The returned function checks if the message has a tag with the specified name and value. If they match exactly, the pattern returns true and the handler function will be invoked. This helper can be used directly as the pattern argument when adding a new handler.Handlers.utils.reply(text: string) This helper is a simple handle function, it basically places the text value in to the Data property of the outbound message.

---

# 258. References  Cookbook

Document Number: 258
Source: https://cookbook_ao.arweave.net/references/index.html
Words: 202
Extraction Method: html

References This section provides detailed technical references for AO components, languages, and tools. Use these resources to find specific information when implementing your AO projects.Programming Languages Resources for the programming languages used in AO:Lua - Reference for the Lua programming language, the primary language used in AO WebAssembly (WASM) - Information about using WebAssembly modules in AO Lua Optimization - Techniques and best practices for optimizing Lua code in AO AO API Reference Documentation for AO's core APIs and functionality:AO Core - Core ao module and API reference Messaging - Comprehensive guide to the AO messaging system patterns Handlers - Reference for event handlers and message processing Token - Information about token creation and management Arweave Data - Guide to data handling and storage in AO Cron - Documentation for scheduling and managing timed events Development Environment Tools and setup for AO development:Editor Setup - Guide to setting up your development environment for AO BetterIDEa - The ultimate native web IDE for AO development Community Resources Connect with the AO community:Community Resources - Information about AO community resources and support Use the sidebar to navigate between reference topics. References are organized by category to help you find the information you need quickly.

---

# 259. Release Notes  Cookbook

Document Number: 259
Source: https://cookbook_ao.arweave.net/releasenotes/index.html
Words: 181
Extraction Method: html

Skip to content  Release Notes This section provides detailed information about updates, new features, bug fixes, and changes in each release of AO and its related tools. Release notes are essential for understanding what's changed between versions and how these changes might affect your projects.AOS Releases AOS is the operating system built on top of the AO computer. These release notes document changes and improvements to AOS:AOS 2.0.2 - Improved spawn process times and various bug fixes AOS 2.0.1 - Details about patch updates and fixes in the 2.0.1 release AOS 2.0.0 - Major release information, including new features and significant changes Why Read Release Notes?Release notes provide valuable information for developers:Learn about new features that could enhance your applications Understand potential breaking changes that might affect existing code Discover bug fixes that resolve issues you may have encountered Stay informed about security updates and best practices We recommend reviewing release notes before upgrading to a new version to ensure a smooth transition.Use the sidebar to navigate between different release notes. Notes are organized chronologically with the most recent releases first.

---

# 260. Lua Optimization Guide for AO Platform  Cookbook

Document Number: 260
Source: https://cookbook_ao.arweave.net/references/lua-optimization.html
Words: 172
Extraction Method: html

Skip to content  Lua Optimization Guide for AO Platform This guide provides practical tips for writing efficient, fast, and performant Lua code for on-chain programs on the AO platform.Table Operations Appending Elements Removing Elements Variable Access Local Variables Upvalues String Operations String Concatenation Pattern Matching Memory Management Table Reuse Minimize Garbage Creation lua-- ❌ Inefficient: Creates new response table on every transfer

local function createTransferResponse(sender, recipient, amount)

  return {

    from = sender,

    to = recipient,

    quantity = amount,

    success = true,

    newBalance = Balances[sender],

    tags = {

      Action = "Transfer-Complete",

      Type = "Token"

    }

  }

end

-- ✅ Efficient: Reuse template table

local transferResponse = {

  from = nil,

  to = nil,

  quantity = 0,

  success = false,

  newBalance = 0,

  tags = {

    Action = "Transfer-Complete",

    Type = "Token"

  }

}

local function createTransferResponse(sender, recipient, amount)

  transferResponse.from = sender

  transferResponse.to = recipient

  transferResponse.quantity = amount

  transferResponse.success = true

  transferResponse.newBalance = Balances[sender]

  return transferResponse

end Blockchain-Specific Optimizations State Management Additional Resources Lua Performance Guide Special thanks to @allquantor for sharing optimization tips

---

# 261. Messaging in ao  Cookbook

Document Number: 261
Source: https://cookbook_ao.arweave.net/tutorials/begin/messaging.html
Words: 997
Extraction Method: html

Messaging in ao Learn how Messages gives ao Parallel Compute Capability In ao, every process runs in parallel, creating a highly scalable environment. Traditional direct function calls between processes aren't feasible because each process operates independently and asynchronously.Messaging addresses this by enabling asynchronous communication. Processes send and receive messages rather than directly invoking functions on each other. This method allows for flexible and efficient interaction, where processes can respond to messages, enhancing the system's scalability and responsiveness.We'll begin by exploring the basics of messaging in aos, how to see messages received in your inbox, and how to send messages to other processes.Video Tutorial  Step 1: Understand the Message Structure Message Basics: Messages in ao are built using Lua tables, which are versatile data structures that can hold multiple values. Within these tables, the "Data" field is crucial as it contains the message's content or payload. This structure allows for efficient sending and receiving of information between processes, showcasing how ao primitives leverage Arweave's underlying capabilities to facilitate complex, composable operations.For detailed specifications, please refer to the original documentation on the G8way specs page.Example: { Data = "Hello from Process A!" } is a simple message.Step 2: Open the aos CLI Launch the aos command-line interface (CLI) by typing aos in your terminal and pressing Enter.shaos Step 3: How to Send a Message Send: The Send function is globally available in the aos interactive environment.Target: To send a message to a specific process, include a Target field in your message.Data: The Data is the string message (or payload) you want to be received by the receiving process. In this example, the message is "Hello World!".Step 4: Store Morpheus's Process ID We'll use the process ID provided below and store it as a variable called Morpheus.luaFvan28CFY0JYl5f_ETB7d3PDwBhGS8Yq5IA0vcWulUc Copy the process ID above and store it as a variable by running the below command in the aos CLI:This will store the process ID as a variable called Morpheus, making it easier to interact with the specific process ID.Check the Morpheus Variable Step 5: Send a Message to Morpheus After obtaining Morpheus's process ID and storing it in a variable, you're ready to communicate with it. To do this, you use the Send function. Morpheus, himself, is a parallel process running in ao. He receives and sends messages using a series of Handlers. Let's send him a message and see what happens.Your Target is Morpheus which is the variable we defined earlier using Morpheus 's process ID.The Data is the message you want to send to Morpheus. In this case, it's "Morpheus?".Expected Results:You've sent a message to Morpheus and received a response, but you can't read the full message. Let's learn about the Inbox and how to read messages.Step 6: The Inbox The Inbox is where you receive messages from other processes.INFO To see an in depth view of an inbox message, head over to the Messages Concepts page.Let's check your inbox to see how many messages you have received.Inside your aos CLI, type the following command:lua #Inbox If you're actively following through the tutorial, the inbox will not have many messages. However, if you've been experimenting with the aos environment, you may more than 1 message in your inbox.Example Return:In the example above, the return is 4, stating that there are four messages in the inbox.As we're actively looking for Morpheus 's response, we'll assume his message was the last one received. To read the last message in your inbox, type the following command:lua Inbox[#Inbox].Data This command allows you to isolate the Data from the message and only read the contents of the data.The Expected Return:You are now using your own process to communicate with Morpheus, another parallel process running in ao. You're now ready to move on to the next step in the tutorial.Step 7: Sending Messages with Tags Purpose of Tags: Tags in aos messages are used to categorize, route, and process messages efficiently. They play a crucial role in message handling, especially when dealing with multiple processes or complex workflows.Some processes use Handlers that specifically interact with messages that have certain tags. For example, a process may have a handler that only interacts with messages that have a specific tag, which we'll see an example of in the chatroom tutorial.How to Use Tags in Messages In the case of Morpheus, we can use tags to categorize our messages, and because Morpheus is a autonomous process, he has handlers that can interact with messages that have certain tags.Adding Tags to a Message:We already know that the Data of a message is the payload of the message you want to send to another process. Earlier, we sent a message to Morpheus without any tags, in which he used a handler to respond to an exact match within the Data field.Let's Show Morpheus That We're Ready Send Morpheus a message with the tag Action and the value rabbithole.Example:Read the message from Morpheus:luaInbox[#Inbox].Data Expected Return:Additional Tips for Using Tags Consistent Tagging: Develop a consistent tagging system for your application to make message handling more predictable.Tag Naming: Choose clear and descriptive names for your tags. This makes it easier to understand the purpose and context of messages at a glance.Security with Tags: Remember that tags are not encrypted or hidden, so avoid using sensitive information as tags.Advanced Usage of Tags Workflow Management: Tags can be instrumental in managing workflows, especially in systems where messages pass through multiple stages or processes.Additional Tips for Messaging Message Structure: Explore other fields like Epoch, From, and Nonce for more complex messaging needs.Debugging: Use the Dump function to print messages for debugging.Security Considerations: Be cautious with the content and handling of messages, and never send anything considered private or sensitive.Conclusion You've now learned how to send messages with tags, which is a powerful tool for categorizing and routing messages in aos.Morpheus has officially invited you to the next stage of your journey. You're now ready to move on to the next step in the tutorial, Creating a Chatroom.

---

# 262. Building a Chatroom in aos  Cookbook

Document Number: 262
Source: https://cookbook_ao.arweave.net/tutorials/begin/chatroom.html
Words: 733
Extraction Method: html

Building a Chatroom in aos INFO If you've found yourself wanting to learn how to create a chatroom within ao, then that means we understand at least the basic methodology of sending and receiving messages. If not, it's suggested that you review the Messaging tutorial before proceeding.In this tutorial, we'll be building a chatroom within ao using the Lua scripting language. The chatroom will feature two primary functions:Register: Allows processes to join the chatroom.Broadcast: Sends messages from one process to all registered participants.Let's begin by setting up the foundation for our chatroom.Video Tutorial  Step 1: The Foundation Open your preferred code editor, e.g. VS Code.Create a new file named chatroom.lua. Step 2: Creating The Member List In chatroom.lua, you'll begin by initializing a list to track participants: Save the chatroom.lua file Step 3: Load the Chatroom into aos With chatroom.lua saved, you'll now load the chatroom into aos.If you haven't already, start your aos in your terminal inside the directory where chatroom.lua is saved In the aos CLI, type the following script to incorporate your script into the aos process:lua.load chatroom.lua  Type Members, or whatever you named your user list, in aos. It should return an empty array { }. If you see an empty array, then your script has been successfully loaded into aos.Step 4: Creating Chatroom Functionalities The Registration Handler The register handler will allow processes to join the chatroom.Adding a Register Handler: Modify chatroom.lua to include a handler for Members to register to the chatroom with the following code: This handler will allow processes to register to the chatroom by responding to the tag Action = "Register". A printed message will confirm stating Registered. will appear when the registration is successful.Reload and Test: Let's reload and test the script by registering ourselves to the chatroom.Save and reload the script in aos using .load chatroom.lua.Check to see if the register handler loaded with the following script:lua Handlers.list  This will return a list of all the handlers in the chatroom. Since this is most likely your first time developing in aos, you should only see one handler with the name Register.Let's test the registration process by registering ourselves to the chatroom:If successful, you should see that there was a message added to your outbox and that you then see a new printed message that says registered. Finally, let's check to see if we were successfully added to the Members list:lua Members If successful, you'll now see your process ID in the Members list.Adding a Broadcast Handler Now that you have a chatroom, let's create a handler that will allow you to broadcast messages to all members of the chatroom.Add the following handler to the chatroom.lua file:This handler will allow you to broadcast messages to all members of the chatroom.Save and reload the script in aos using .load chatroom.lua.Let's test the broadcast handler by sending a message to the chatroom:Step 5: Inviting Morpheus to the Chatroom Now that you've successfully registered yourself to the chatroom, let's invite Morpheus to join us. To do this, we'll send an invite to him that will allow him to register to the chatroom.Morpheus is an autonomous agent with a handler that will respond to the tag Action = "Join", in which will then have him use your Register tag to register to the chatroom.Let's send Morpheus an invitation to join the chatroom:To confirm that Morpheus has joined the chatroom, check the Members list:luaMembers If successful, you'll receive a broadcasted message from Morpheus.Step 6: Inviting Trinity to the Chatroom Within this message, he'll give you Trinity's process ID and tell you to invite her to the chatroom.Use the same processes to save her process ID as Trinity and to invite her to the chatroom as you did with Morpheus.If she successfully joins the chatroom, she'll then pose the next challenge to you, creating a token.Engaging Others in the Chatroom Onboarding Others Invite aos Users: Encourage other aos users to join your chatroom. They can register and participate in the broadcast.Provide Onboarding Instructions: Share a simple script with them for easy onboarding:Congratulations! You've successfully built a chatroom in ao and have invited Morpheus to join you. You've also created a broadcast handler to send messages to all members of the chatroom.Next, you'll continue to engage with Morpheus, but this time you'll be adding Trinity to the conversation. She will lead you through the next set of challenges. Good Luck!

---

# 263. Begin An Interactive Tutorial  Cookbook

Document Number: 263
Source: https://cookbook_ao.arweave.net/tutorials/begin/index.html
Words: 130
Extraction Method: html

Skip to content  Begin: An Interactive Tutorial In this tutorial series, you'll walk through an interactive steps that will help you deepen your knowledge and understanding of the aos environment.INFO The Exercise In this fun exercise, you'll encounter a series of challenges presented by two familiar characters, Morpheus and Trinity. You'll dive deep into the rabbit hole guided by Morpheus as he presents you with a series of challenges to prove you're the one. Once you've completed all of the challenges presented by both Morpheus and Trinity, you'll receive a token that grants you access to an exclusive chatroom within ao called The Construct.Now, let's get started down the rabbit hole.Tutorials Getting Started - An Interactive Tutorial 1. Quick Start 2. Messaging 3. Creating a Chatroom 4. Build a Token

---

# 264. Interpreting Announcements  Cookbook

Document Number: 264
Source: https://cookbook_ao.arweave.net/tutorials/bots-and-games/announcements.html
Words: 353
Extraction Method: html

Interpreting Announcements Welcome back to your coding journey. It's time to use the skills you've acquired from previous tutorials to enhance your gaming experience.During the game, you've likely noticed announcements appearing in your terminal. These announcements are the game's way of communicating important events to players. However, these messages can sometimes seem cryptic or you might find yourself checking your inbox frequently for further details.Wouldn't it be convenient to access this information directly from your terminal? Well, there's a way to do that!By using handlers, you can create an autonomous agent to retrieve this information for you, marking the progression from simple bots to entities capable of interpreting and acting on game events directly.Setting up the Development Environment Start by creating a new file named bot.lua in your preferred directory.Ideally, this file should be placed in the same directory where your player process runs to ease the loading of the code. Else, you'll need to use relative paths to access the file.Writing the Code Let's dive into the logic.Each handler in aos requires three key pieces of information:name: A unique name for the handler pattern: A pattern for the handler to identify, triggering its operation handle: The operations to perform when the desired pattern is found.Here's how you can write a handler for printing announcement details:In this case, the name of the handler is "PrintAnnouncements". It uses a special in-built utility (hasMatchingTags) represented by { Action = "Announcement" } to check if the incoming message has been tagged as an announcement. If true, the handler prints the Event and Data, which represent the title and description of the announcement.NOTE Once a message is "handled", it will be discarded from your Inbox.Now, let's bring this to life in the game.Navigate to your aos player terminal and enter a game session.Activate the handler by loading your bot.lua file with:lua.load bot.lua You'll now see game announcements appear directly in your terminal, offering real-time insights without the need to sift through your inbox.Congratulations! You have just taken the first step in building a bot on aos. But let's keep working on adding more features to it 🌐

---

# 265. Fetching Game State  Cookbook

Document Number: 265
Source: https://cookbook_ao.arweave.net/tutorials/bots-and-games/game-state.html
Words: 438
Extraction Method: html

Fetching Game State Now that you're seeing game announcements directly in your terminal, you have a better grasp of the game's dynamics. However, these insights are limited to specific actions occurring within the game.Wouldn't it be more useful to have on-demand access to comprehensive game data, like the positions, health, and energy of all players? This information could significantly improve your strategic planning, helping you assess threats, opportunities, and timing more effectively.If you thought of adding another handler to the bot created in the previous guide, you're absolutely right!Writing the Code Go back to your bot.lua file and update your existing handler as follows:Adjustments to your handler include:Renaming to "HandleAnnouncements" to reflect its broader role.Addition of an extra operation to request the game for the updated state. The game is designed to respond to the GetGameState action tag.When you get a print of the announcement, you can check the latest message in your Inbox as follows:luaInbox[#Inbox] The Data field of this message contains the latest state of the game which includes:GameMode: Whether the game is in Waiting or Playing state.TimeRemaining: The time remaining for the game to start or end.Players: A table containing every player's stats like position, health and energy.But this can be taken a step further so that you can not just read but also use information from the latest state for other automations.Let's define a new variable that stores the latest state as follows:The syntax preserves existing values of the variable when you load successive iterations of the bot.lua file in your terminal, instead of overwriting it. If there is no pre-existing value then a nil value is assigned to the variable.Then implement another handler as follows:The response from the game process from the previous handler has an action tag with the value GameState that helps us trigger this second handler. Once triggered, the handle function loads the in-built json package that parses the data into json and stores it in the LatestGameState variable.This handler additionally sends a message to your process indicating when the state has been updated. The significance of this feature will be explained in the following section.You can refer to the latest code for bot.lua in the dropdown below:Updated bot.lua file As usual, to test this new feature, load the file in your aos player terminal as follows:lua.load bot.lua Then check the LatestStateVariable to see if it has updated correctly by simply passing its name as follows:luaLatestGameState With real-time access to the latest state of the game you bot is equipped to make informed decisions decide your next action. Next let's try automating actions with the help of this data 🚶

---

# 266. Expanding the Arena  Cookbook

Document Number: 266
Source: https://cookbook_ao.arweave.net/tutorials/bots-and-games/build-game.html
Words: 1799
Extraction Method: html

Expanding the Arena Welcome to the final guide of Chapter 2, where you'll learn to build your own game on top of the arena framework introduced in the previous tutorial. In this guide, we'll take you through the process of creating the "ao-effect" game, which you experienced at the beginning of this chapter. As you progress through this example, you'll gain insights into structuring your game's logic and interacting with the arena's core code.Whether you're a seasoned developer or an aspiring game creator, this guide will empower you to unleash your creativity and bring your unique game ideas to life within the aos environment.Setting up the Development Environment Start by creating a new file named ao-effect.lua in your preferred directory.NOTE Ideally, this file should be placed in the same directory where your game process runs to ease the loading of the code. Else, you'll need to use relative paths to access the file.Writing the Code Now, let's dive into the logic.You'll notice that your game logic will involve calling functions and variables defined in the arena's logic. This showcases the power of composability, where your game builds on top of the existing arena logic, allowing seamless integration of variables and functions between the two. Because both logic become part of a unified logic for the game process.Initializing Game Mechanics First, define essential variables and functions that set the stage for your game's mechanics:lua-- AO EFFECT: Game Mechanics for AO Arena Game

-- Game grid dimensions

Width = 40 -- Width of the grid

Height = 40 -- Height of the grid

Range = 1 -- The distance for blast effect

-- Player energy settings

MaxEnergy = 100 -- Maximum energy a player can have

EnergyPerSec = 1 -- Energy gained per second

-- Attack settings

AverageMaxStrengthHitsToKill = 3 -- Average number of hits to eliminate a player

-- Initializes default player state

-- @return Table representing player's initial state

function playerInitState()

    return {

        x = math.random(Width/8),

        y = math.random(Height/8),

        health = 100,

        energy = 0

    }

end

-- Function to incrementally increase player's energy

-- Called periodically to update player energy

function onTick()

    if GameMode ~= "Playing" then return end  -- Only active during "Playing" state

    if LastTick == undefined then LastTick = Now end

    local Elapsed = Now - LastTick

    if Elapsed >= 1000 then  -- Actions performed every second

        for player, state in pairs(Players) do

            local newEnergy = math.floor(math.min(MaxEnergy, state.energy + (Elapsed * EnergyPerSec // 2000)))

            state.energy = newEnergy

        end

        LastTick = Now

    end

end This code initializes your game's mechanics, including grid dimensions, player energy, and attack settings. The playerInitState function sets up the initial state for players when the game begins.Player Movement Next, add the code for player movement:lua-- Handles player movement

-- @param msg: Message request sent by player with movement direction and player info

function move(msg)

    local playerToMove = msg.From

    local direction = msg.Tags.Direction

    local directionMap = {

        Up = {x = 0, y = -1}, Down = {x = 0, y = 1},

        Left = {x = -1, y = 0}, Right = {x = 1, y = 0},

        UpRight = {x = 1, y = -1}, UpLeft = {x = -1, y = -1},

        DownRight = {x = 1, y = 1}, DownLeft = {x = -1, y = 1}

    }

    -- calculate and update new coordinates

    if directionMap[direction] then

        local newX = Players[playerToMove].x + directionMap[direction].x

        local newY = Players[playerToMove].y + directionMap[direction].y

        -- updates player coordinates while checking for grid boundaries

        Players[playerToMove].x = (newX - 1) % Width + 1

        Players[playerToMove].y = (newY - 1) % Height + 1

        announce("Player-Moved", playerToMove .. " moved to " .. Players[playerToMove].x .. "," .. Players[playerToMove].y .. ".")

    else

        ao.send({Target = playerToMove, Action = "Move-Failed", Reason = "Invalid direction."})

    end

    onTick()  -- Optional: Update energy each move

end The move function calculates new player coordinates based on the chosen direction while ensuring that players remain within the grid boundaries. Player movement adds dynamic interaction to your game and is announced to all players and listeners.Player Attacks Then you must implement the logic for player attacks:lua-- Handles player attacks

-- @param msg: Message request sent by player with attack info and player state

function attack(msg)

    local player = msg.From

    local attackEnergy = tonumber(msg.Tags.AttackEnergy)

    -- get player coordinates

    local x = Players[player].x

    local y = Players[player].y

    -- check if player has enough energy to attack

    if Players[player].energy < attackEnergy then

        ao.send({Target = player, Action = "Attack-Failed", Reason = "Not enough energy."})

        return

    end

    -- update player energy and calculate damage

    Players[player].energy = Players[player].energy - attackEnergy

    local damage = math.floor((math.random() * 2 * attackEnergy) * (1/AverageMaxStrengthHitsToKill))

    announce("Attack", player .. " has launched a " .. damage .. " damage attack from " .. x .. "," .. y .. "!")

    -- check if any player is within range and update their status

    for target, state in pairs(Players) do

        if target ~= player and inRange(x, y, state.x, state.y, Range) then

            local newHealth = state.health - damage

            if newHealth <= 0 then

                eliminatePlayer(target, player)

            else

                Players[target].health = newHealth

                ao.send({Target = target, Action = "Hit", Damage = tostring(damage), Health = tostring(newHealth)})

                ao.send({Target = player, Action = "Successful-Hit", Recipient = target, Damage = tostring(damage), Health = tostring(newHealth)})

            end

        end

    end

end

-- Helper function to check if a target is within range

-- @param x1, y1: Coordinates of the attacker

-- @param x2, y2: Coordinates of the potential target

-- @param range: Attack range

-- @return Boolean indicating if the target is within range

function inRange(x1, y1, x2, y2, range)

    return x2 >= (x1 - range) and x2 <= (x1 + range) and y2 >= (y1 - range) and y2 <= (y1 + range)

end The attack function calculates damage based on attack energy, checks player energy, and updates player health accordingly. Player attacks add the competitive element in your game, allowing players to engage with each other. The attacks are also announced to the players and listeners for real-time updates of the game.Handling the Logic Lastly, you must setup handlers:As seen in earlier guides, the handlers help trigger functions when their respective patterns are met.You can refer to the final code for ao-effect.lua in the dropdown below:Final ao-effect.lua file lua-- AO EFFECT: Game Mechanics for AO Arena Game

-- Game grid dimensions

Width = 40 -- Width of the grid

Height = 40 -- Height of the grid

Range = 1 -- The distance for blast effect

-- Player energy settings

MaxEnergy = 100 -- Maximum energy a player can have

EnergyPerSec = 1 -- Energy gained per second

-- Attack settings

AverageMaxStrengthHitsToKill = 3 -- Average number of hits to eliminate a player

-- Initializes default player state

-- @return Table representing player's initial state

function playerInitState()

    return {

        x = math.random(0, Width),

        y = math.random(0, Height),

        health = 100,

        energy = 0

    }

end

-- Function to incrementally increase player's energy

-- Called periodically to update player energy

function onTick()

    if GameMode ~= "Playing" then return end  -- Only active during "Playing" state

    if LastTick == undefined then LastTick = Now end

    local Elapsed = Now - LastTick

    if Elapsed >= 1000 then  -- Actions performed every second

        for player, state in pairs(Players) do

            local newEnergy = math.floor(math.min(MaxEnergy, state.energy + (Elapsed * EnergyPerSec // 2000)))

            state.energy = newEnergy

        end

        LastTick = Now

    end

end

-- Handles player movement

-- @param msg: Message request sent by player with movement direction and player info

function move(msg)

    local playerToMove = msg.From

    local direction = msg.Tags.Direction

    local directionMap = {

        Up = {x = 0, y = -1}, Down = {x = 0, y = 1},

        Left = {x = -1, y = 0}, Right = {x = 1, y = 0},

        UpRight = {x = 1, y = -1}, UpLeft = {x = -1, y = -1},

        DownRight = {x = 1, y = 1}, DownLeft = {x = -1, y = 1}

    }

    -- calculate and update new coordinates

    if directionMap[direction] then

        local newX = Players[playerToMove].x + directionMap[direction].x

        local newY = Players[playerToMove].y + directionMap[direction].y

        -- updates player coordinates while checking for grid boundaries

        Players[playerToMove].x = (newX - 1) % Width + 1

        Players[playerToMove].y = (newY - 1) % Height + 1

        announce("Player-Moved", playerToMove .. " moved to " .. Players[playerToMove].x .. "," .. Players[playerToMove].y .. ".")

    else

        ao.send({Target = playerToMove, Action = "Move-Failed", Reason = "Invalid direction."})

    end

    onTick()  -- Optional: Update energy each move

end

-- Handles player attacks

-- @param msg: Message request sent by player with attack info and player state

function attack(msg)

    local player = msg.From

    local attackEnergy = tonumber(msg.Tags.AttackEnergy)

    -- get player coordinates

    local x = Players[player].x

    local y = Players[player].y

    -- check if player has enough energy to attack

    if Players[player].energy < attackEnergy then

        ao.send({Target = player, Action = "Attack-Failed", Reason = "Not enough energy."})

        return

    end

    -- update player energy and calculate damage

    Players[player].energy = Players[player].energy - attackEnergy

    local damage = math.floor((math.random() * 2 * attackEnergy) * (1/AverageMaxStrengthHitsToKill))

    announce("Attack", player .. " has launched a " .. damage .. " damage attack from " .. x .. "," .. y .. "!")

    -- check if any player is within range and update their status

    for target, state in pairs(Players) do

        if target ~= player and inRange(x, y, state.x, state.y, Range) then

            local newHealth = state.health - damage

            if newHealth <= 0 then

                eliminatePlayer(target, player)

            else

                Players[target].health = newHealth

                ao.send({Target = target, Action = "Hit", Damage = tostring(damage), Health = tostring(newHealth)})

                ao.send({Target = player, Action = "Successful-Hit", Recipient = target, Damage = tostring(damage), Health = tostring(newHealth)})

            end

        end

    end

end

-- Helper function to check if a target is within range

-- @param x1, y1: Coordinates of the attacker

-- @param x2, y2: Coordinates of the potential target

-- @param range: Attack range

-- @return Boolean indicating if the target is within range

function inRange(x1, y1, x2, y2, range)

    return x2 >= (x1 - range) and x2 <= (x1 + range) and y2 >= (y1 - range) and y2 <= (y1 + range)

end

-- HANDLERS: Game state management for AO-Effect

-- Handler for player movement

Handlers.add("PlayerMove", { Action = "PlayerMove" }, move)

-- Handler for player attacks

Handlers.add("PlayerAttack", { Action = "PlayerAttack" }, attack) Once you've written your game code, it's time to load it into the aos game process and test your game:lua.load ao-effect.lua IMPORTANT Make sure to load the arena blueprint in the same process as well.Invite friends or create test player processes to experience your game and make any necessary adjustments for optimal performance.What's Next Congratulations! You've successfully expanded the arena by building your own game on top of its core functionalities. Armed with the knowledge and tools acquired in this guide, you're now equipped to build games on aos independently.The possibilities are endless. Continue adding more features to existing games or create entirely new ones. The sky's the limit! ⌃◦🚀

---

# 267. Welcome to ao  Cookbook

Document Number: 267
Source: https://cookbook_ao.arweave.net/welcome/index.html
Words: 210
Extraction Method: html

Welcome to ao  AO is a decentralized compute system where countless parallel processes interact within a single, cohesive environment. Each process operates independently, yet they are seamlessly connected through a native message-passing layer, similar to how websites form the World Wide Web.AO + AOS: The rocket and your rocket fuel.Typically when you use AO, you will interact with it through its operating system: AOS.AOS is an abstraction layer that runs in your processes, making it easy to use the full functionality of the AO computer. In this cookbook, you will learn everything you need to know about getting started with the AO computer using AOS.Mainnet and Legacynet AO Mainnet launched on February 8, 2025, introducing AO Core and enabling onchain computation with payments and relays.AO Legacynet launched on February 27, 2024, providing a fee-free environment for early adopters to experiment with AO’s hyper-parallel architecture.Mainnet Documentation Phase 1 of Mainnet AO Core is now live in relay mode through HyperBEAM nodes, which serve as payment gateways to AO. See the guide below to get started.AO Core Relay Legacynet Documentation These tutorials explore AO Legacynet, covering everything from building chatrooms to developing autonomous, decentralized bots. It’s a great starting point for experimenting with AO’s hyper-parallel architecture.Legacynet Tutorials Further References Guides Concepts References

---

# 268. AO Processes  Cookbook

Document Number: 268
Source: https://cookbook_ao.arweave.net/welcome/ao-processes.html
Words: 442
Extraction Method: html

AO Processes AO Processes are persistent, programmable smart contracts that live inside the AO computer. Embodying the actor model from Erlang that inspired AO, these processes operate as independent computational units that have their own state and communicate with each other through message passing. This architecture makes them ideal for creating autonomous agents and complex decentralized applications.What are AO Processes?Following the actor model, each AO Process functions as an independent actor within the system, executing code—typically written in Lua—in response to messages it receives. Three core characteristics define them:Stateful: Each process has its own private state and memory, which persist across interactions.Persistent: All processes and their entire message history are permanently stored on Arweave.Generative: Processes can dynamically spawn new processes, enabling complex and evolving systems.AO Processes and the Actor Model The actor model provides several key benefits for process-based development, enabling naturally concurrent and resilient systems. By treating every process as an isolated "actor," it simplifies development and enhances fault tolerance. Key advantages include:Concurrency & Isolation: Processes execute independently and are isolated from each other, enabling parallelism and preventing cascading failures.Message-Passing: All communication happens exclusively through asynchronous messages, simplifying interactions.Location Transparency & Fault Tolerance: Processes can interact without knowing each other's physical location on the network, and the system can continue operating even if individual processes fail.AOS: The Operating System for AO Processes AOS (AO Operating System) is an abstraction layer designed to simplify interaction with AO Processes. It provides developers with a powerful shell interface for sending commands, tools for managing process state, and a set of libraries for common functionalities, all contributing to a more streamlined development experience.Use Cases for AO Processes The persistent and concurrent nature of AO Processes makes them ideal for a wide range of decentralized applications. Here are a few examples:Autonomous Agents & Bots: Imagine a price-monitoring bot that tracks token prices across different decentralized exchanges (DEXs) and executes arbitrage trades automatically. AO makes it possible to build entire marketplaces for such agents, like Marketverse.Decentralized Finance (DeFi): You could build automated market makers (AMMs) or lending protocols where account balances and token reserves are tracked persistently within the process's state. A live example of this is Dexi, a decentralized exchange built on AO.On-Chain Games & Social Platforms: AO Processes can power fully on-chain games where the game state (like player positions or inventory) is managed by one or more processes, like the space strategy game Stargrid. They're also perfect for decentralized chat applications or social networks where user profiles, posts, and interactions are censorship-resistant.Now that you understand the capabilities of AO Processes, the next step is to dive into Hyperbeam, the high-performance network that powers them.

---

# 269. Legacynet  HyperBEAM  Cookbook

Document Number: 269
Source: https://cookbook_ao.arweave.net/welcome/legacynet-info/index.html
Words: 383
Extraction Method: html

Legacynet → HyperBEAM As the AO ecosystem evolves, we are transitioning from Legacynet to HyperBEAM Mainnet, marking a significant upgrade in the implementation of the AO-Core protocol.Legacynet: The Initial Implementation Legacynet was the first implementation of the AO-Core protocol, written in JavaScript. Launched on February 27, 2024, it provided a fee-free environment for early adopters to experiment with AO's hyper-parallel architecture. However, being a JavaScript implementation, Legacynet had inherent limitations in terms of scalability and native support for the actor-oriented model that AO is based on.HyperBEAM: The Future of AO-Core HyperBEAM is the new, advanced implementation of the AO-Core protocol, written in Erlang—the language that inspired AO's actor-oriented design. This implementation innately benefits from Erlang's strengths in:Actor-Oriented Design: Erlang's native support for the actor model aligns perfectly with AO's architecture, where processes (actors) operate independently and communicate via message passing.Scalability: Erlang is renowned for its ability to handle massive concurrency, allowing HyperBEAM to scale efficiently with the growing demands of the AO computer.Reliability: Erlang's design for fault tolerance ensures that HyperBEAM can maintain system stability even under high load or during failures of individual components.The Transition to HyperBEAM While HyperBEAM represents the future of AO, the transition from Legacynet is being handled carefully to ensure a smooth experience for developers. Currently, most development activity remains on Legacynet, which provides a stable environment for building and testing.The goal is to provide a seamless future upgrade path to HyperBEAM Mainnet. While Legacynet will eventually be deprecated, for now, it is the primary environment for new developers to begin building on AO.HyperBEAM Documentation For detailed documentation on the HyperBEAM protocol itself, including running infrastructure and leveraging its powerful URL pathing, visit HyperBEAM.arweave.net.Building on HyperBEAM To learn how to build applications on HyperBEAM using ao and aos, and to migrate existing processes, see the Migrating to HyperBEAM Guide.Preparing for the Future While you build on Legacynet, you can prepare for the future of AO by:Reviewing the HyperBEAM documentation to understand the new environment and its architecture.Exploring the enhanced capabilities that HyperBEAM offers due to its Erlang foundation.Building with the knowledge that a seamless migration path to HyperBEAM Mainnet is a core priority.This transition is a significant step forward for the AO ecosystem, ensuring that we can deliver on the promise of decentralized, hyper-parallel computation at any scale.

---

# 270. ARIO Documentation

Document Number: 270
Source: https://docs.ar.io/
Words: 141
Extraction Method: html

🚀 Get Started Quickly: Explore Our Quick Start Guides → Welcome to the Permaweb Data in paradise. The AR.IO ecosystem is dedicated to cultivating products and protocols for sustaining access to digital permanence, making the permaweb available to everyone. Powered by the ARIO Token, this global network of Gateways connects users to permanently stored data, files, applications, and web pages on the Arweave decentralized storage network.Guides Run a Gateway Get your AR.IO Gateway up and running correctly and quickly.Read more Use ArNS Learn the process of purchasing and managing an ArNS name.Read more Deploy a dApp Learn how to easily deploy a website or application on the permaweb.Read more ANTs on Bazar In a few simple steps, learn how to make an ANT tradable on Bazar.Read more GraphQL Learn how to leverage GraphQL to efficiently fetch data via AR.IO gateways.Read more

---

# 271. ARIO Docs

Document Number: 271
Source: https://docs.ar.io/ar-io-sdk/ants/set-description
Words: 20
Extraction Method: html

setDescription setDescription is a method on the ANT class that updates the descriptive text for the ANT process.setDescription requires authentication.

---

# 272. ARIO Docs

Document Number: 272
Source: https://docs.ar.io/ar-io-sdk/ants/set-name
Words: 20
Extraction Method: html

setName setName is a method on the ANT class that updates the display name of the ANT process.setName requires authentication.

---

# 273. ARIO Docs

Document Number: 273
Source: https://docs.ar.io/ar-io-sdk/ants/set-ticker
Words: 20
Extraction Method: html

setTicker setTicker is a method on the ANT class that updates the ticker symbol of the ANT process.setTicker requires authentication.

---

# 274. Getting Started - ARIO Docs

Document Number: 274
Source: https://docs.ar.io/ar-io-sdk/getting-started
Words: 120
Extraction Method: html

Prerequisites node >= v18.0.0 npm or yarn Installation Quick Start The following examples demonstrate how to use the AR.IO SDK to retrieve a list of active gateways from the Gateway Address Registry (GAR) across different environments.Node Web Polyfills Polyfills are not provided by default for bundled web projects (Vite, ESBuild,
Webpack, Rollup, etc.). Depending on your apps bundler configuration and
plugins, you will need to provide polyfills for various imports including
crypto, process and buffer. Refer to examples/webpack and examples/vite for examples. For other project configurations, refer to your bundler's
documentation for more information on how to provide the necessary polyfills.Output The output for obtaining a list of gateways, regardless of the environment used, will follow the structure outlined below:

---

# 275. Gateway Apex Domain Content Resolution - ARIO Docs

Document Number: 275
Source: https://docs.ar.io/gateways/apex
Words: 355
Extraction Method: html

Overview Prior to gateway Release 28, the apex domain of a gateway would only display information about the Arweave network. Release 28 introduced two new environment variables that allow a gateway to serve custom content from the apex domain:APEX_TX_ID: Set to serve content from a specific transaction ID APEX_ARNS_NAME: Set to serve content from an ArNS name These variables enable gateway operators to customize their gateway's apex domain with useful information, details about the operator or associated projects, or any other content they wish to share.Quick Start If you want to serve your project's dApp from the apex domain of your gateway:Upload your dApp to Arweave Assign your dApp's transaction Id to an ArNS name Set the environment variable:APEX_ARNS_NAME=your-ArNS-name Restart your gateway Your dApp will now be served from your gateway's apex domain Configuration Environment Variables You can configure your gateway to serve content from the apex domain by setting one of two environment variables:IMPORTANT You cannot set both variables simultaneously. Providing both variables will result in an error.Restart Requirements The gateway must be restarted after initially setting these environment variables If using APEX_ARNS_NAME, no restart is needed when the ArNS name points to a new transaction If using APEX_TX_ID, the gateway must be restarted when updating the transaction ID Use Cases Gateway operators can use this feature to:Display information about their gateway service Share details about the operator or organization Showcase associated projects and services Share educational content about Arweave and the permaweb Display any other content they wish to make available at their gateway's root domain Community Examples Several gateway operators have already implemented this feature to serve custom content from their apex domains:arnode.asia - Serves a custom landing page with information about their gateway service arlink.xyz - Serves the permaDapp for the Arlink project frostor.xyz / love4src.com - Serves information about the Memetic Block Software Guild and their projects vilenarios.com - Serves personalized portfolio/link tree information about the operator permagate.io - Serves personalized link tree information about the operator These examples demonstrate how gateway operators can leverage the apex domain feature to create a more personalized and informative experience for their users.

---

# 276. ARIO Docs

Document Number: 276
Source: https://docs.ar.io/gateways/admin
Words: 325
Extraction Method: html

AR.IO HTTP API Admin Endpoints Overview The AR.IO HTTP API offers several endpoints that allow access to internal information and the ability to make adjustments without restarting your Gateway. Each of these endpoints behind /ar-io/admin/ have access restricted, so you will need to have set up your ADMIN_API_KEY variable and include "Authorization: "Bearer ${ADMIN_API_KEY}" in the header of your request.When testing endpoints at <your-Gateway>/api-docs, you can enter your ADMIN_API_KEY using the green "Authorize" button near the top of the page, or by clicking any of the open lock icons next to a password protected end point.Debug The ar-io/admin/debug endpoint provides a comprehensive view of the current state of your Gateway. This endpoint has been designed to offer developers and administrators insights into the operational status of the gateway, including any errors or warnings that have occurred since the last startup.Example response Queue Transaction The ar-io/admin/queue-tx endpoint allows you to prioritize processing of a specific transaction, based on that transaction's ID. The id key must be set in the body of your request, and a POST request should be used.This endpoint will also enable you to prioritize opening and indexing bundles by providing the L1 TX ID for the bundle, but only if your Gateway is operating with the ANS104_UNBUNDLE_FILTER and ANS104_INDEX_FILTER keys set.Your Gateway will either respond with an error, or { message: 'TX queued' } Block Data The ar-io/admin/block-data endpoint allows you to tell your Gateway to refuse to serve certain data. In order to add to this block list, make a PUT request to this endpoint with the following in the body:id: This should be the transaction id of the content you want to block.notes: Notes regarding the reason this content was blocked. For documentation purposes only.source: Identifier for the source of TX IDs you are blocking. For example, the name of a public block list. For documentation purposes only.Your Gateway will either respond with an error, or { message: 'Content blocked' }

---

# 277. Gateway ArNS Resolution - ARIO Docs

Document Number: 277
Source: https://docs.ar.io/gateways/arns-resolution
Words: 624
Extraction Method: html

ArNS Resolution Overview One of the core functions of the AR.IO network gateway is to serve ArNS (Arweave Name System) records. Each ArNS name is assigned a specific "time to live" (TTL) value,
which determines how often gateways should check for updates to the Arweave Transaction ID that the name points to.
This TTL works similarly to a , which controls how often updates are checked for traditional websites. As a result, there may be a delay between when an ArNS record is updated and when users see the updated information in their browser.Effective with gateway Release 23, new features have been implemented on AR.IO gateways to optimize the resolution of ArNS records. These include an option for
gateway operators to override the ArNS TTL, and set their own schedule for checking ArNS names for updates.Initial Caching When a gateway starts up, it will attempt to fetch the records of all ArNS names in order to create a local cache. Previously, this cache was stored in memory. After Release 23, this cache is saved to persistent storage
so that the gateway's ArNS cache will survive restarting the gateway. This prevents delays in resolving ArNS names immediately after a gateway starts up. This cache is saved to the directory data/arns.Cache Refreshing When a new ArNS name is purchased on arns.app (or programmatically using the AR.IO SDK), gateways need to update their local cache to include this new name.
Previously, gateway operators could not control how or when their gateway refreshed its cache. As a result, new names would often take several hours to resolve. With Release 23, once a new name is purchased and
requested from a gateway, the gateway will check if it was already aware of the name's existence. If not, it will refresh its cache to include the records for the new name, allowing immediate resolution.Gateway operators can specify how often their gateway should refresh its cache when it fails to find an ArNS name that has been requested by setting the ARNS_NAME_LIST_CACHE_MISS_REFRESH_INTERVAL_SECONDS value in their .env file.
The default value for this environmental variable is 10 seconds.
Similarly, they can prompt their gateway to refresh their cache of ArNS names when a name is requested and successfully found in the local cache by setting the ARNS_NAME_LIST_CACHE_HIT_REFRESH_INTERVAL_SECONDS, which defaults to 1 hour.Both of these variables can be set to a number, which represents the number of seconds the gateway should wait before refreshing its cache when a name is requested that is, or is not, already in its local cache.Gateway TTL Override Every ArNS record is set with a TTL specified by the name owner. Gateway operators can set the ARNS_RESOLVER_OVERRIDE_TTL_SECONDS variable in their .env file to override this TTL,
and define for themselves how often the gateway should check for updated records. A shorter TTL value will result in more frequent outgoing requests to the ANT that controls the ArNS name, which can result in slower serving of the name data to users, while a longer TTL allows for faster serving of cached data, which may be out of date.TTL Override is disabled by default, and should be set to the number of seconds the gateway should use as its TTL when resolving names.If a gateway operator chooses to override the TTL set by ArNS owners, they must carefully weigh the trade-offs and decide on the balance between performance speed and record currency that best aligns with their priorities and use case.Note that the gateway TTL override does not override what is set in the cache
headers for the name, it only overrides that TTL on the internal cache of the
gateway (meaning the gateway will fetch is more frequently if it wants, but
always respects the TTL when serving it)

---

# 278. Quick Start Guides - ARIO Docs

Document Number: 278
Source: https://docs.ar.io/guides
Words: 104
Extraction Method: html

Quick Start Guides
Deploy a dApp with ArlinkEasily deploy a web app with ArNS using Arlinkmake your ArNS name tradable on BazarHow to display and trade your ArNS ANTs on BazarBuild a dApp using the ArNext frameworkBuild and deploy a dApp for displaying and updating ArNS names using ArNextAutomate deployment using Permaweb DeployAutomate the deployment of your dApp to ArNS using Permaweb Deploy, ArNS, and GithubManaging UndernamesHow to programmatically manage ArNS undernames using the AR.IO SDKQuery data on Arweave using GraphQLSchema and best practices for constructing a GraphQL queryDeploy a dApp with ArDrive webHow to upload a dApp to the permaweb using ArDrive web

---

# 279. Introduction - ARIO Docs

Document Number: 279
Source: https://docs.ar.io/introduction
Words: 591
Extraction Method: html

TL;DR AR.IO seeks to create a decentralized and incentivized cloud network aimed at attracting more gateways to the Arweave network therefore making the permanent web more accessible to all.
At the core of AR.IO's incentivization mechanism is the ARIO Token, a utility token used for joining the network, payments, gateway accountability, and protocol incentives.
The network features modular and composable gateway infrastructure in addition to the Arweave Name System (ArNS) – a system for assigning friendly domain names to permanent data.What is AR.IO AR.IO is the world's first permanent cloud network, providing the infrastructure to ensure data, applications, and digital identities are timeless, tamper-proof, and universally accessible.
Built on the foundation of the Arweave storage network, AR.IO forms a global ecosystem of gateways, protocols, and services that connect users to the permaweb – a web where information is permanent and free from centralized control.The AR.IO Network is an open, distributed, and ownerless system, supported by operators, developers, and end-users from around the world.
It's decentralized nodes, known as AR.IO Gateways, act as "Permanent Cloud Service Providers" delivering the critical services needed to read, write, index and query data stored on the permaweb.
These gateways provide a unified, resilient interface between users and the permaweb, featuring a permanent domain name system and seamless, location-independent access to permanent storage and applications.Gateways operate using standardized protocols to maintain consistency across the network.
They also engage in an observation and reporting protocol to monitor performance and ensure accountability, helping to maintain a healthy and reliable ecosystem.The AR.IO Network is powered by a utility token, ARIO, which drives the network's functionality and accessibility.
ARIO serves as a currency for services such as the Arweave Name System (ArNS), staking to join the network as a gateway operator, delegated staking, and as rewards for contributing to the network's performance and reliability.Together, these elements form the backbone of a permanent cloud network designed to preserve data and expand the possibilities of the web.Why AR.IO?Arweave (a Layer 1 blockchain network) offers scalable and permanent onchain data storage in a sustainable manner.
It does this by incentivizing miner nodes through a tokenomic endowment model which ensures data is globally stored and replicated for hundreds of years without the need for continual payment or maintenance by its uploader.However, the Arweave protocol does not incorporate all the needs of modern applications like data indexing, querying, retrieval, and other vital services.
Consequently, over the past few years, infrastructure services have been independently developed and deployed to meet the demands of the permaweb at scale.
Users and apps have come to rely on these gateway utilities, but they are closed source, have complex codebases, and are expensive to operate.Arweave does not offer any tokenomic incentives to offset the expenses associated with operating a gateway, which has led to the community's reliance on a single centrally controlled gateway subsidized for the betterment of the network: arweave.net.
While arweave.net currently caches and indexes the entire weave with a high quality of service, it is a single bottleneck and point of failure for the whole ecosystem.AR.IO seeks to reduce the barriers of entry and attract more gateway operators to the permaweb with the goal of further enhancing its overall health, resiliency, and functionality through decentralized mechanisms that are as trustless as possible.The solution will be applied in two directions:By reducing gateway overhead costs with open source, efficient, modular networked architecture.By creating an economic incentive layer with the ARIO Token.The overall goal of this white paper is to present the framework for a healthy and sustainable decentralized gateway network.

---

# 280. Staking - ARIO Docs

Document Number: 280
Source: https://docs.ar.io/staking
Words: 534
Extraction Method: html

Overview Staking tokens within the AR.IO Network serves a dual primary purpose: it signifies a public commitment by gateway operators and qualifies them and their delegates for reward distributions.In the AR.IO ecosystem, "staking" refers to the process of locking a specified amount of ARIO tokens into a protocol-controlled vault.
This act signifies an opportunity cost for the staker, acting both as a motivator and a public pledge to uphold the network's collective interests.
Once staked, tokens remain locked until the staker initiates an 'unstake / withdraw' action or reaches the end of the vault’s lock period.It is important to note that the ARIO Token is non-inflationary, distinguishing the AR.IO Network's staking mechanism from yield-generation tools found in other protocols.
Staking in this context is about eligibility for potential rewards rather than direct token yield.
By staking tokens, gateway operators (and their delegates) demonstrate their commitment to the network, thereby gaining eligibility for protocol-driven rewards and access to the network’s shared resources.Gateway Staking A gateway operator must stake tokens to join their gateway to the network, which not only makes them eligible for protocol rewards but also promotes network reliability.
This staking requirement reassures users and developers of the gateway's commitment to the network’s objectives, and gateways that adhere to or surpass network performance standards become eligible for these rewards.
Gateway operators may increase their stake above the minimum, known as excess stake. A gateway’s total stake is impacted the following epoch once excess stake is added or removed.To promote participation from a wider audience, the network shall allow anyone with available ARIO tokens to partake in delegated staking.
In this, users can choose to take part in the risk and rewards of gateway operations by staking their tokens with an active gateway (or multiple gateways) through an act known as delegating.
By delegating tokens to a gateway, a user increases the overall stake of that gateway.
A delegated staker proxies their stake to gateways and therefore entrusts gateway operators to utilize that stake in maintaining a quality of service befitting the permaweb.Stake Redelegation This feature enables existing stakers to reallocate their staked tokens between gateways, known as redelegation.
Both delegated stakers and gateway operators with excess stake (stake above the minimum network-join requirement) can take advantage of this feature.
Redelegation is intended to offer users flexibility and the ability to respond to changing network conditions.Staked tokens generally have restricted liquidity to maintain a healthy degree of stability in the network.
However, an exception to these restrictions allows delegated stakers to use their staked tokens for specific ArNS -related services.
By leveraging their staking rewards, delegates can further engage with ArNS, strengthening the name system’s utilization and impact across the network.Expedited Withdrawal Fees Gateway operators and delegated stakers can shorten the standard withdrawal delay period after initiating a withdrawal (or being placed into an automatic withdrawal by protocol mechanisms); this action is subject to a dynamic fee.
At any point during the delay, users can choose to expedite access to their pending withdrawal tokens by paying a fee to the protocol balance, calculated based on how much sooner they want to receive their funds.
Once triggered, the tokens are returned immediately to the user’s wallet.

---

# 281. ARIO Network Composition - ARIO Docs

Document Number: 281
Source: https://docs.ar.io/network-composition
Words: 265
Extraction Method: html

Overview The permanent web, or "permaweb," is the collection of all webpages, applications, and files stored on the Arweave network and made accessible by the AR.IO permanent cloud.
These range from simple tools for viewing and managing data to sophisticated decentralized applications integrating immutable storage and smart contracts.For users and developers, the permaweb offers low-cost, maintenance-free, and permanent hosting for web apps, data, and pages – serving both traditional and emerging industries.Composition of the Permanent Cloud The AR.IO Network integrates decentralized protocols, services, and applications to power the permanent web alongside the traditional internet.
Foundational components like Arweave and AO are independently developed, while AR.IO introduces essential services and incentives that enable seamless interaction and accessibility. Diagram 1: The Permanent Cloud Network Major Components of the Permanent Cloud:Storage: Arweave At the foundation lies the Arweave protocol, providing decentralized, immutable data storage. This layer ensures data is preserved indefinitely with clear provenance records for long-term reliability.Compute: AO This layer comprises decentralized compute platforms, such as Arweave-native solutions like AO and other Layer 1 smart contract platforms like Ethereum.
These systems enable flexible, data-driven computation and smart contract execution, broadening the ecosystem's capabilities.Services: AR.IO Sitting atop the compute layer, the AR.IO Network provides essential services like data upload, retrieval, indexing, querying, and domain name resolution.
AR.IO gateways ensure the permanent web remains functional, accessible, and usable for developers, creators, and end users.Together, these layers form a cohesive ecosystem, combining data permanence, decentralized computation, and seamless cloud services.
Each layer strengthens the others, creating a resilient foundation for the permaweb while bridging the traditional and decentralized internet paradigms.

---

# 282. ARIO Docs

Document Number: 282
Source: https://docs.ar.io/wayfinder/core/routing-strategies/round-robin
Words: 116
Extraction Method: html

RoundRobinRoutingStrategy Overview The RoundRobinRoutingStrategy distributes requests evenly across all available gateways in a cyclical manner. Each gateway is selected in turn, ensuring fair load distribution and preventing any single gateway from being overwhelmed.How It Works Initialize Gateway List: Start with an ordered list of available gateways Track Current Position: Maintain a pointer to the current gateway in the rotation Select Next Gateway: Choose the next gateway in the sequence Cycle Through List: Return to the first gateway after reaching the end Handle Failures: Skip failed gateways and continue rotation Configuration Basic Usage With Weighted Rotation Parameters Related FastestPingRoutingStrategy: Network-based gateway discovery PreferredWithFallbackRoutingStrategy: Static gateway configuration RandomRoutingStrategy: Randomized gateway selection StaticRoutingStrategy: Always use a single, fixed gateway

---

# 283. ARIO Docs

Document Number: 283
Source: https://docs.ar.io/wayfinder/core/routing-strategies/preferred-with-fallback
Words: 117
Extraction Method: html

PreferredWithFallbackRoutingStrategy Overview The PreferredWithFallbackRoutingStrategy attempts to use a designated preferred gateway first, and only falls back to an alternative routing strategy if the preferred gateway fails or is unavailable. This strategy is ideal for applications with dedicated infrastructure or specific gateway requirements.How It Works Health Check: Performs a HEAD request to the preferred gateway with a 1000ms timeout Success: If the preferred gateway responds with a successful status, it's used Failure: If the preferred gateway fails or times out, the fallback strategy is used Logging: All attempts and failures are logged for monitoring Basic Usage Parameters Related FastestPingRoutingStrategy: Network-based gateway discovery RoundRobinRoutingStrategy: Even distribution across gateways RandomRoutingStrategy: Randomized gateway selection StaticRoutingStrategy: Always use a single, fixed gateway

---

# 284. ARIO Docs

Document Number: 284
Source: https://docs.ar.io/wayfinder/core/routing-strategies/random
Words: 102
Extraction Method: html

RandomRoutingStrategy Overview The RandomRoutingStrategy selects gateways randomly from the available pool. This strategy provides simple load distribution without maintaining state or performing complex calculations, making it ideal for scenarios where unpredictability is desired or where simplicity is paramount.How It Works Receive Gateway List: Accept the list of available gateways Generate Random Index: Create a random number within the gateway list range Select Gateway: Choose the gateway at the random index Apply Filters: Optionally filter out unhealthy or blocked gateways Return Selection: Return the randomly selected gateway Basic Usage Parameters Related FastestPingRoutingStrategy: Network-based gateway discovery PreferredWithFallbackRoutingStrategy: Static gateway configuration RandomRoutingStrategy: Randomized gateway selection

---

# 285. ARIO Docs

Document Number: 285
Source: https://docs.ar.io/wayfinder/core/routing-strategies/fastest-ping
Words: 105
Extraction Method: html

FastestPingRoutingStrategy Overview The FastestPingRoutingStrategy selects the gateway with the lowest latency by performing HEAD requests to all available gateways and choosing the one that responds fastest. This strategy optimizes for performance by dynamically selecting the most responsive gateway for each request.How It Works Ping All Gateways: Send HEAD requests to all available gateways for the provided path and subdomain Measure Response Times: Records the time taken for each gateway to respond Select Fastest: Choose the gateway with the lowest response time Basic Usage Parameters Related PreferredWithFallbackRoutingStrategy: Static gateway configuration RoundRobinRoutingStrategy: Even distribution across gateways RandomRoutingStrategy: Randomized gateway selection StaticRoutingStrategy: Always use a single, fixed gateway

---

# 286. Data Root Verification Strategy - ARIO Docs

Document Number: 286
Source: https://docs.ar.io/wayfinder/core/verification-strategies/data-root-verification
Words: 196
Extraction Method: html

DataRootVerificationStrategy Overview The DataRootVerificationStrategy provides the highest level of data integrity verification by validating data using Arweave's Merkle tree proofs. This strategy ensures maximum security by verifying that the data matches the cryptographic data root stored in the transaction, providing mathematical proof of data integrity.Important DataRootVerificationStrategy requires that the trusted gateway has the relevant
transaction data indexed locally. Gateways cannot proxy out verification
requests to other sources, as this would compromise the security and
reliability of the verification process. If a gateway doesn't have the
required data indexed, verification will fail.How It Works Compute Data Root: Chunk the received content and build a Merkle tree Calculate Root Hash: Compute the root hash of the Merkle tree Fetch Trusted Root: Get the data root from trusted gateways via /tx/{txId}/data_root Compare Roots: Verify the calculated root matches the trusted data root Result: Pass or fail based on data root validation Warning ANS-104 Data Items Not Supported: This strategy currently only works with regular Arweave transactions, not ANS-104 bundled data items. If you attempt to verify an ANS-104 data item, it will throw an error.Basic Usage Related Hash Verification: Learn about fast integrity checking Signature Verification: Understand authenticity validation

---

# 287. StaticRoutingStrategy - ARIO Docs

Document Number: 287
Source: https://docs.ar.io/wayfinder/core/routing-strategies/static
Words: 110
Extraction Method: html

Overview The StaticRoutingStrategy is the simplest routing strategy that always returns a single, pre-configured gateway URL. This strategy ignores any gateways provided by the GatewaysProvider and is useful for scenarios where you want to force all requests to use a specific gateway.How It Works The strategy always returns the configured gateway, ignoring any provided gateway lists:Configure Gateway: Set a single gateway URL during initialization Ignore Provided Gateways: Any gateways from providers are ignored Return Static Gateway: Always return the same configured gateway Log Warnings: Warn when provided gateways are ignored Configuration Basic Usage With Custom Gateway Parameters Related FastestPingRoutingStrategy: Network-based gateway discovery PreferredWithFallbackRoutingStrategy: Static gateway configuration RandomRoutingStrategy: Randomized gateway selection

---

# 288. Verification Strategies - ARIO Docs

Document Number: 288
Source: https://docs.ar.io/wayfinder/core/verification-strategies
Words: 158
Extraction Method: html

Overview Verification strategies in Wayfinder ensure data integrity and authenticity when fetching content from Arweave through AR.IO gateways. These strategies use cryptographic methods to verify that the data you receive matches what was originally stored on Arweave, protecting against tampering, corruption, or malicious gateways.Why Verification Matters Data Integrity: Ensures content hasn't been corrupted during transmission Security: Protects against malicious gateways serving fake data Trust: Provides cryptographic proof that data is authentic Compliance: Meets security requirements for sensitive applications Strategy Comparison Important Verification methods require that the gateway being used has the relevant
transaction data indexed locally. Gateways cannot proxy out verification
requests to other sources, as this would compromise the security and
reliability of the verification process. If a gateway doesn't have the
required data indexed, verification will fail.Related Hash Verification: Learn about fast integrity checking Signature Verification: Understand authenticity validation Data Root Verification: Explore maximum security verification Wayfinder Core: See how to configure verification in your application

---

# 289. Bundling Services  Cooking with the Permaweb

Document Number: 289
Source: https://cookbook.arweave.net/concepts/bundlers.html
Words: 176
Extraction Method: html

Bundling Services  With bundling services users can post their data transactions to a bundling service to have it "bundled" together with other users transactions and posted as a single Arweave transaction in an upcoming Arweave block. What is a bundle? A description of transaction bundles and their benefits can be found here.What is a Bundler node? A bundler is a node which is responsible for accepting transactions or data items from users, bundling them, and posting them to the Arweave network (with a guarantee they will be uploaded with a specific transaction ID).Services:Turbo open in new window Which make sure the data is persisted until it is uploaded to Arweave.Supporting multiple currencies  A key feature of bundling services is that because they pay for the base Arweave transaction to be posted (using AR tokens) they can choose to enable payments of storage fees on a variety of different tokens. This is the main entry point for other chains to enable Arweave's permanent storage for their users.Built with  ❤️  by the Arweave community. Learn more at  Arweave.org

---

# 290. Transaction Bundles  Cooking with the Permaweb

Document Number: 290
Source: https://cookbook.arweave.net/concepts/bundles.html
Words: 296
Extraction Method: html

Transaction Bundles What is a Bundle? A transaction bundle is a special type of Arweave transaction. It enables multiple other transactions and/or data items to be bundled inside it. Because transaction bundles contain many nested transactions they are key to Arweave's ability to scale to thousands of transactions per second.Users submit transactions to a bundling service, such as turbo, which combines them into a 'bundle' with other transactions and posts them to the network.How Do Bundles Help Arweave? Availability Bundling services guarantee that bundled transactions are reliably posted to Arweave without dropping.Transaction IDs of the bundled transactions are immediately made available, meaning the data can instantly be accessed as if it was already on the Arweave network.Reliability Transactions posted to Arweave can occasionally fail to confirm (resulting in a dropped transaction) due to a number of reasons, such as high network activity. In these instances transactions can become orphaned, i.e. stuck in the mempool and eventually removed.Bundlers solve this problem by continually attempting to post bundled data to Arweave, assuring that it does not fail or get stuck in the mempool.Scalability Bundles can store up to 2 256 transactions, each of which are settled as a single transaction on Arweave. This makes Arweave blockspace scale to support almost any use case.What are Nested Bundles? Bundles can include data items for uploading to Arweave and those data item can themselves be a bundle.This means it is possible to upload a bundle of bundles, or in other words nested bundles.Nested bundles have no theoretical limit on nesting depth, meaning that transaction throughput can be increased drastically.Nested bundles might be useful for when you have different groups of bundled data that you want to guarantee reach Arweave altogether, and at the same time.Sources and Further Reading:Ardrive Turbo ANS-104 Standard

---

# 291. Gateways  Cooking with the Permaweb

Document Number: 291
Source: https://cookbook.arweave.net/concepts/gateways.html
Words: 299
Extraction Method: html

Gateways  Data uploaded to the Arweave network (or the permaweb) isn't always immediately easy to work with.What is a Gateway?Gateways are sometimes referred to as the "front door to the permaweb". They act as an interface between Arweave and end-users, making it easy to access data or use permaweb applications from your web browser.For example, accessing a HTML file stored on Arweave will be displayed as a web page in your browser. The same goes for viewing images, downloading files, viewing JSON data, or any other file stored on Arweave. This makes interacting with the permaweb very similar to using the traditional web.Other Roles of Gateways Other than serving data for users to access, gateways offer other services such as:Caching frequently accessed data and transactions Indexing and querying transactions (through Arweave tags and a GraphQl interface) Seeding transactions throughout the Arweave network Content moderation (content policies to choose which data is or isn't served) Gateways and the Arweave Protocol Although gateways play a large role in allowing content to be accessed on Arweave, they are not part of the core protocol.This means hosting and running gateways is separate to running a node securing the Arweave network (although are frequently done together).As gateways are not part of the core protocol, there is no built-in incentive structure like the rewards or incentives for mining. This allows gateway operators or external services to choose how they want to structure their incentive system, leading to a more decentralized and democratic model. Individual applications could even operate their own gateway to allow for better caching and performance of their permaweb applications.Some popular gateways include arweave.net ran by the Arweave team, and others like arweave.world arweave.asia arweave.live, and g8way.io. However, operating gateways is being made easier and more accessible through teams such as AR.IO.ArWiki AR.IO

---

# 292. Overview  Cooking with the Permaweb

Document Number: 292
Source: https://cookbook.arweave.net/concepts/psts.html
Words: 399
Extraction Method: html

Overview  ⚠️ Deprecation Notice This document is deprecated and may contain outdated information.Profit Sharing Tokens (PSTs) are a type of SmartWeaveToken which include the following structure:property type balances object name string ticker string transfer method balance method PSTs are typically used to govern a protocol or "Profit Sharing Community" (PSC) - similar to a DAO.How are PSTs Distributed? Founders of an application can create a set number of PSTs and distribute them as they see fit - to keep, or sell to investors to raise capital.Protocols can offer PSTs as a reward for contributing work, or completing tasks for the community to incentivize growth.PSTs can also be exchanged between each other on Permaswap (currently in testnet), and developers can set up token trading permissions using Verto Flex.Features  PSTs work as ‘ micro-dividends ’. When a protocol is used, a tipping amount is set aside to be distributed amongst holders. The tip is paid out in $AR - not in the currency of the PST. This creates quite a special relationship between the app being developed, and Arweave itself.Benefits  Provides a flexible way for developers to run a protocol and distribute as much ‘ownership’ as they see fit PSTs can be used as payment for protocol work or for community tasks Founders are incentivized to increase network usage, as it is directly tied to revenue Holders get intrinsic value (rewards $AR, not more ‘stock’) Example PST: ARDRIVE Token  ArDrive is a permaweb application utilitizing their aptly named PST, ARDRIVE.When someone pays $AR to upload data through ArDrive, a 15% community fee is distributed to a single token holder using a random, weighted method. A user uploads data -> An ARDRIVE token holder receives $AR -> ARDRIVE token holder can use this $AR to upload files -> cycle repeats. Hopefully this gives you a good idea of one way you could implement your own PST!Exploring PSTs  Going straight to viewblock and Sonar by Redstone is most appropriate most likely. Just use links that specifically show PSTs so someone doesn’t have to navigate to find them.You can use ViewBlock for an etherscan-like experience to view PST contracts, like this one here.Another option is Sonar, an Arweave smart contract explorer built by RedStone Finance. View the same contract here.Some community members have been discussing calling PSTs “Permaweb Service Tokens”. There is still much to explore with PSTs → join the discussion here (Discord).

---

# 293. Transaction Types  Cooking with the Permaweb

Document Number: 293
Source: https://cookbook.arweave.net/concepts/transaction-types.html
Words: 258
Extraction Method: html

Transaction Types Arweave supports different types of transactions that serve various purposes in the permaweb ecosystem. Understanding these transaction types is essential for working effectively with Arweave.Overview While all data on Arweave is stored as transactions, there are specialized transaction formats designed for specific use cases. These specialized formats help organize data, improve efficiency, and enable complex applications on the permaweb.Types of Transactions Transaction Bundles Transaction bundles are special transactions that can contain multiple other transactions or data items bundled together. Bundles are crucial for Arweave's scalability, allowing thousands of transactions per second by combining many transactions into a single bundle.Key Benefits:Improved scalability and throughput Guaranteed transaction posting reliability Support for nested bundles Enhanced availability of data Path Manifests Path manifests provide a way to organize multiple transactions under a single base transaction ID with human-readable file paths. They are essential for creating practical applications and websites on Arweave.Key Benefits:Organize related files together Enable human-readable file paths Support for hosting websites and applications Simplify NFT collection management When to Use Each Type Use Transaction Bundles when:Uploading multiple files or data items Requiring guaranteed transaction posting Building high-throughput applications Needing immediate data availability Use Path Manifests when:Creating websites or web applications Organizing file collections (like NFT metadata) Providing user-friendly file access Building hierarchical data structures Getting Started To learn more about each transaction type, explore the detailed guides:Transaction Bundles - Learn about bundling multiple transactions Path Manifests - Understand how to organize files with paths Both transaction types are fundamental building blocks for creating sophisticated applications on the permaweb.

---

# 294. Cooking with the Permaweb  Cooking with the Permaweb

Document Number: 294
Source: https://cookbook.arweave.net/getting-started/index.html
Words: 251
Extraction Method: html

Cooking with the Permaweb The Permaweb Cookbook is a developer resource that provides the essential concepts and references for building applications on Arweave. Each concept and reference will focus on specific aspects of the Permaweb development ecosystem while providing additional details and usage examples.For those interested in creating smart contracts in particular, consider checking out the AO Cookbook which covers the decentralized compute platform built on top of AO.Developers Welcome to the Arweave development community, where the past is forever etched in the blockchain and the future is full of endless possibilities. Let's build the decentralized web together!Read More Contributing The Cookbook is designed in a way that makes it easy for new Permaweb developers to contribute. Even if you don't know how to do something, contributing to the cookbook is a great way to learn!Check out all open issues here. Contribution guidelines here. If you find the cookbook is missing a concept, guide or reference, please add an issue.Read More How to Read the Cookbook The Permaweb Cookbook is split into different sections, each aimed at a different goal.Section Description Core Concepts Building blocks of the Permaweb that are good to know for development Guides Snack-sized guides about different tools for development References References to commonly needed code snippets Starter Kits Front-end Framework Starters to get you started building on the Permaweb in no time Quick Starts These are small guides to help developers from every experience level to ship code the the permaweb.Hello World (No Code) Hello World (CLI)

---

# 295. Vouch  Cooking with the Permaweb

Document Number: 295
Source: https://cookbook.arweave.net/concepts/vouch.html
Words: 179
Extraction Method: html

Vouch Overview Motivation Vouching provides a decentralized approach to Sybil resistance. A Sybil attack is when an attacker subverts the network by creating a large number of pseudonymous identities to gain a disproportionately large influence.Vouch Protocol Arweave introduced the concept of the ANS-109 Vouch (Assertion of Identity). It is a standard that uses a specific transaction format along with some tags to allows anyone on the permaweb to "vouch" for the identity and humanity of any Arweave address.Adding a standard such as the ANS-109 to the permaweb will help minimize Sybil attacks and bad actors, making it a safer experience for permaweb users.ANS-109 Transaction Format Tag Name Optional?Tag Value App-Name False Vouch Vouch-For False Arweave address that is being vouched for in this transaction App-Version True 0.1 Verification-Method True Method of verification of identity for the person. Example - Twitter / In-Person / Gmail / Facebook User-Identifier True An identifier for the user based on the Verification Method. Example - abhav@arweave.org Resources ANS-109 Docs open in new window Built with  ❤️  by the Arweave community. Learn more at  Arweave.org

---

# 296. Contributing Workflow  Cooking with the Permaweb

Document Number: 296
Source: https://cookbook.arweave.net/getting-started/contributing.html
Words: 315
Extraction Method: html

Contributing Workflow Anyone in the community is welcome to contribute to the Permaweb Cookbook, as community members we want a high quality reference guide of little snack bite sized nuggets of information. Below is a step by step workflow of how anyone can contribute to this project.What do you need to know?Git and Github - publishes content to github.com.Markdown - Markdown is a text based markup language that can be transformed into HTML Arweave and the Permaweb - Have some knowledge about the Permaweb that should be shared Steps to Contribute  Commiting work We are using conventional commits for this repository.General flow for making a contribution:Fork the repo on GitHub Clone the project to your own machine Commit changes to your own branch Push your work back up to your fork Submit a Pull request so that we can review your changes NOTE: Be sure to merge the latest from "upstream" before making a pull request!Style Here are some suggestions on tone and style from some contributors:TIP In writing them, I'm getting a feeling for the tone that's appropriate for each. CoreConcepts should be rather textbook like, neutral voice, objective. "This is how Arweave works" For Guides, I think it's ok to have a more personal voice. Refer to the reader as "you" and speak in the collaborative voice "next we'll take a look at..." This may just be personal preference, but in general I feel this tone much more supportive and accessible when following a longer form guide. Indeed, its the voice that most popular tutorials from other ecosystems are written in. For Resources, I think it shares the same voice as core concepts, with a preference for brevity.dmac TIP Conceptual and referencial data should have a more cold scientific tone and guides should be a supportive or even humorous tone. Longer form content needs to pull readers in without them zoning out.Arch_Druid CONTRIBUTING

---

# 297. Search Indexing Service  Cooking with the Permaweb

Document Number: 297
Source: https://cookbook.arweave.net/guides/querying-arweave/search-indexing-service.html
Words: 595
Extraction Method: html

Search Indexing Service tl;dr Backwards compatible syntax with Arweave GraphQL Faster response times for complex queries (ie multi-tag search) More query options  Goldsky 's free search service uses an optimized backend that allows for faster searches for complex queries across arweave blocks and transactions, and also introduces additional querying syntax for fuzzy and wildcard search use-cases.The Search GraphQL syntax is a superset of the Arweave GraphQL syntax. It's fully backwards compatible and will return the same results for the same queries, but has some additional modifiers that can be useful.Flexible tag filters Search for just a tag name or value Advanced tag filters Fuzzy search Wildcard search Filter for L1 transactions only Result set total counts For any custom needs or feature ideas, feel free to contact the Goldsky team through email or on discord!Search Gateway Endpoints Currently, the only service with this syntax is hosted Goldsky. If anybody is interested in hosting their own gateway with the same syntax, feel free to contact the Goldsky for help.Goldsky Search Service Features Flexible Tag Filters The Search Gateway Syntax is less strict, and allows for searching just for the Tag name or value Examples Search for transactions with the tag value 'cat' query just_values {
  transactions(
    first: 10,
    tags: [
      {
        values: ["cat"]
      }
    ]
  ) 
  {
    edges {
      node {
        id
        tags {
          name
          value
        }
      }
    }
  }
} Search for transactions that have an In-Response-To-ID query just_name {
  transactions(
    first: 10,
    tags: [
      {
        name: "In-Response-To-ID"
      }
    ]
  ) 
  {
    edges {
      node {
        id
        tags {
          name
          value
        }
      }
    }
  }
} Advanced tag filters The Search Gateway Syntax offers an additional parameter to the tag filter, match.Match value Description EXACT (default) exact matches only.WILDCARD Enables * to match any amount of characters, ie. text/* FUZZY_AND Fuzzy match containing all search terms FUZZY_OR Fuzzy match containing at least one search term Open up the playground and try some of the following queries!Searching all transactions with an image content type using a wildcard {
    transactions(        
      tags: [
        { name: "Content-Type", values: "image/*", match: WILDCARD}
      ]
      first: 10
    ) {
        edges {
            cursor
            node {
                id
              tags {
                name
                value
              }
              block { height }
              bundledIn {id}
            }
        }
    }
} Fuzzy search is very powerful, and can search for 'similar' text with many variations.Searching all transactions with 'cat' OR 'dog' (or CAT or doG or cAts or CAAts etcs). So the tag could contain at least of cat-like or dog-like term.{
    transactions(        
      tags: [
        { name: "Content-Type", values: ["cat", "dog"], match: "FUZZY_OR"}
      ]
      first: 10
    ) {
        edges {
            cursor
            node {
                id
              tags {
                name
                value
              }
              block { height }
              bundledIn {id}
            }
        }
    }
} Search for transactions that have cat-like AND dog-like tag values {
    transactions(        
      tags: [
        { name: "Content-Type", values: ["cat", "dog"], match: "FUZZY_AND"}
      ]
      first: 10
    ) {
        edges {
            cursor
            node {
                id
              tags {
                name
                value
              }
              block { height }
              bundledIn {id}
            }
        }
    }
} Exclude Bundled (L2) Transactions Simply set bundledIn: NULL query just_l1 {
  transactions(
    first: 10,
    bundledIn: null
  ) 
  {
    edges {
      node {
        id
        signature
        owner {
          address
        }
        block {
          height
        }
      }
    }
  }
} Getting total counts given a query If you'd like to understand how many transactions fit a certain set of filters, just use the count field. This will trigger an additional optimized count operation. This will likely double the time it would take to return the query, so use only when needed.query count_mirror {
  {
      transactions(tags:{values:["MirrorXYZ"]})
      {
        count
      }
  }
}

---

# 298. Cooking with the Permaweb  Cooking with the Permaweb

Document Number: 298
Source: https://cookbook.arweave.net/index.html
Words: 227
Extraction Method: html

Cooking with the Permaweb The Permaweb Cookbook is a developer resource that provides the essential concepts and references for buiding applications on the Permaweb. Each concept and reference will focus on specific aspects of the Permaweb development ecosystem while providing additional details and usage examples.Developers Welcome to the Arweave development community, where the past is forever etched in the blockchain and the future is full of endless possibilities. Let's build the decentralized web together!Read More Contributing The Cookbook is designed in a way that makes it easy for new Permaweb developers to contribute. Even if you don't know how to do something, contributing to the cookbook is a great way to learn!Check out all open issues here. Contribution guidelines here. if you find the cookbook is missing a concept, guide or reference, please add an issue.Read More How to Read the Cookbook The Permaweb Cookbook is split into different sections, each aimed at a different goal.Section Description Core Concepts Building blocks of the Permaweb that are good to know for development Guides Snack-sized guides about different tools for development References References to commonly needed code snippets Starter Kits Front-end Framework Starters to get you started building on the Permaweb in no time Quick Starts These are small guides to help developers from every experience level to ship code the the permaweb.Hello World (No Code) Hello World (CLI)

---

# 299. Hash Verification Strategy - ARIO Docs

Document Number: 299
Source: https://docs.ar.io/wayfinder/core/verification-strategies/hash-verification
Words: 97
Extraction Method: html

HashVerificationStrategy Overview The HashVerificationStrategy verifies data integrity by comparing SHA-256 hashes of fetched data against trusted gateway digest headers. This strategy provides fast, cryptographically secure verification for high-throughput applications.How It Works Fetch Data: Retrieve content from the selected gateway Request Digest: Get the digest from trusted gateways via HTTP headers using HEAD/GET requests from a trusted gatweay Compute Hash: Calculate the SHA-256 hash of the received data Compare: Verify that both hashes match exactly Result: Pass or fail based on hash comparison Basic Usage Related Signature Verification: Understand authenticity validation Data Root Verification: Explore maximum security verification

---

# 300. ao  Cookbook

Document Number: 300
Source: https://cookbook_ao.arweave.net/guides/aos/modules/ao.html
Words: 94
Extraction Method: html

Skip to content  ao Built-in global library for process communication and management. The ao object provides core functionality for sending messages, spawning processes, and logging.Core Functions ao.send(msg) Sends a message to another process. See the ao.send reference for more information.ao.spawn(module: string, spawn: table) Creates a new process from a module. See the ao.spawn reference for more information.ao.log(string|table) Logs messages or data that can be read by process callers.Environment The ao.env variable contains process initialization info like ID, owner, and tags.For the complete API reference including all properties and functions, see the ao reference documentation.

---

# 301. Building ao Processes - HyperBEAM - Documentation

Document Number: 301
Source: https://hyperbeam.arweave.net/build/building-on-ao.html
Words: 92
Extraction Method: html

Building on HyperBEAM with ao The guides for building applications on HyperBEAM and interacting with ao processes have been moved to the AO Processes Cookbook to provide a centralized resource for developers.Here are some helpful resources from the AO Processes Cookbook:Get Started with ao to learn the basics of ao and how to start building processes.Migration Guide for moving processes from legacynet and using new HyperBEAM features.Using aos with HyperBEAM for using the aos command-line tool with HyperBEAM.Using aoconnect with HyperBEAM for using the aoconnect library to interact with processes on HyperBEAM.

---

# 302. JSON  Cookbook

Document Number: 302
Source: https://cookbook_ao.arweave.net/guides/aos/modules/json.html
Words: 85
Extraction Method: html

Skip to content  JSON The JSON module allows you to encode and decode objects using JavaScript Object Notation.Example usage Module functions encode() This function returns a string representation of a Lua object in JSON.Parameters:val: {any} The object to format as JSON Returns: JSON string representation of the provided object Example decode() The function takes a JSON string and turns it into a Lua object.Parameters:val: {any} The JSON string to decode Returns: Lua object corresponding to the JSON string (throws an error for invalid JSON strings)

---

# 303. LLMs Documentation  Cookbook

Document Number: 303
Source: https://cookbook_ao.arweave.net/llms-explanation.html
Words: 84
Extraction Method: html

Skip to content  LLMs Documentation llms.txt:Structured overview of the ao ecosystem.Ideal for AI tools navigating documentation or answering general questions.Suited for agents with web search capabilities.llms-full.txt:Complete technical documentation.Designed for in-depth analysis, troubleshooting, or chatbot integration.Provides exhaustive details for complex queries.INFO The llms-full.txt file only contains content from references and release notes, as testing showed this focused approach performs better with current AI models.Permaweb LLMs.txt:The following is a tool that allows you to build your own LLMs.txt files based on docs from the permaweb ecosystem.

---

# 304. Eval  Cookbook

Document Number: 304
Source: https://cookbook_ao.arweave.net/concepts/eval.html
Words: 81
Extraction Method: html

Skip to content  Eval Each AO process includes an onboard Eval handler that evaluates any new code it receives. This handler determines the appropriate action for the code and verifies that the message originates from the process owner.The Eval handler can also be manually triggered to evaluate the Data field from an incoming message. When you use the .load function to load a file into a process, it relies on the Eval handler to evaluate the file’s content under the hood.

---

# 305. Creating and Deploying Manifests  Cooking with the Permaweb

Document Number: 305
Source: https://cookbook.arweave.net/guides/deploying-manifests/deployingManifests.html
Words: 76
Extraction Method: html

Creating and Deploying Manifests  This guide lays out how to create and deploy a path manifest manually.The path manifests core concepts page has more information on what manifests are, and why they might be useful for your project.If you follow this guide to deploy a path manifest, it will need to follow the manifest structure laid out in the core concepts open in new window page.Built with  ❤️  by the Arweave community. Learn more at  Arweave.org

---

# 306. Monitoring Cron  Cookbook

Document Number: 306
Source: https://cookbook_ao.arweave.net/guides/aoconnect/monitoring-cron.html
Words: 74
Extraction Method: html

Skip to content  Monitoring Cron When using cron messages, ao users need a way to start ingesting the messages, using this monitor method, ao users can initiate the subscription service for cron messages. Setting cron tags means that your process will start producing cron results in its outbox, but you need to monitor these results if you want messages from those results to be pushed through the network.You can stop monitoring by calling unmonitor

---

# 307. ARIO Docs

Document Number: 307
Source: https://docs.ar.io/wayfinder/core/gateway-providers
Words: 70
Extraction Method: html

Gateway Providers Overview Gateway providers are responsible for discovering and managing AR.IO gateways that Wayfinder can use to access Arweave data. They abstract the complexity of gateway discovery and provide a consistent interface for routing strategies to select optimal gateways.Provider Comparison Related NetworkGatewaysProvider: Network-based gateway discovery StaticGatewaysProvider: Static gateway configuration SimpleCacheGatewaysProvider: Caching wrapper for providers Routing Strategies: How gateways are selected for requests Wayfinder Configuration: Main wayfinder setup and usage

---

# 308. Cooking with the Permaweb

Document Number: 308
Source: https://cookbook.arweave.net/guides/deploying-manifests/ardrive.html
Words: 69
Extraction Method: html

ArDrive  You can create a manifest for a folder or group of folders with ardrive create-manifest using the ArDrive CLI open in new window.Further Reading: ArDrive CLI Docs open in new window  Alternatively, you can create a manifest using the ArDrive open in new window web app by selecting New → Create manifest while inside of a drive. Built with  ❤️  by the Arweave community. Learn more at  Arweave.org

---

# 309. ARIO Docs

Document Number: 309
Source: https://docs.ar.io/wayfinder/core/gateway-providers/static
Words: 66
Extraction Method: html

StaticGatewaysProvider Overview The StaticGatewaysProvider uses a predefined list of gateway URLs, making it ideal for development, testing, or when you need to use specific trusted gateways. It provides fast, predictable gateway discovery without network calls.Basic Usage Configuration Options StaticGatewaysProviderOptions Related Documentation Gateway Providers Overview: Compare all gateway providers NetworkGatewaysProvider: Dynamic network discovery SimpleCacheGatewaysProvider: Caching wrapper Wayfinder Configuration: Main wayfinder setup Routing Strategies: How gateways are selected

---

# 310. Cooking with the Permaweb

Document Number: 310
Source: https://cookbook.arweave.net/concepts/arfs/schema-diagrams.html
Words: 62
Extraction Method: html

Schema Diagrams The following diagrams show complete examples of Drive, Folder, and File entity Schemas.Public Drive  Public Drive Schema Private Drive  Private Drive Schema Arweave GQL Tag Byte Limit is restricted to 2048. There is no determined limit on Data JSON custom metadata, though more data results in a higher upload cost.Built with  ❤️  by the Arweave community. Learn more at  Arweave.org

---

# 311. Permaweb Cookbook - Core Concepts  Cooking with the Permaweb

Document Number: 311
Source: https://cookbook.arweave.net/concepts/index.html
Words: 61
Extraction Method: html

Core Concepts Foundations of Arweave and the Permaweb.Posting Transactions Metadata (Tags) Querying Fetching Data Transaction Types Bundles Path Manifests Wallets and Keys Permaweb Permaweb Applications Gateway Services Bundling Services Do you think a permaweb core concept is missing? Create a issue at Github open in new window or consider contributing Built with  ❤️  by the Arweave community. Learn more at  Arweave.org

---

# 312. Permaweb Cookbook - References  Cooking with the Permaweb

Document Number: 312
Source: https://cookbook.arweave.net/references/index.html
Words: 59
Extraction Method: html

References Quick reference materials and lookup guides for working with the permaweb.LLMs.txt - Standard format for providing LLM-readable documentation Glossary - Definitions of key permaweb and Arweave terms Do you think a permaweb reference is missing? Create an issue at Github open in new window or consider contributing Built with  ❤️  by the Arweave community. Learn more at  Arweave.org

---

# 313. Tutorials  Cookbook

Document Number: 313
Source: https://cookbook_ao.arweave.net/tutorials/index.html
Words: 52
Extraction Method: html

Skip to content  Tutorials Here, we've created a series of tutorials to help you get started with aos and build your first processes. These tutorials include interactive guides, code snippets, and examples to help you get comfortable with the aos environment.List of Tutorials Getting Started - An Interactive Guide Bots and Games

---

# 314. Cooking with the Permaweb

Document Number: 314
Source: https://cookbook.arweave.net/guides/deploying-manifests/arweave-app.html
Words: 51
Extraction Method: html

Arweave.app  Uploading a directory through Arweave.app open in new window will automatically create a manifest for all of the files in the directory. Alternatively, you can upload your own manifest file manually, add the following tag, and submit the transaction. Built with  ❤️  by the Arweave community. Learn more at  Arweave.org

---

# 315. Pretty  Cookbook

Document Number: 315
Source: https://cookbook_ao.arweave.net/guides/aos/modules/pretty.html
Words: 48
Extraction Method: html

Skip to content  Pretty This module allows printing formatted, human-friendly and readable syntax.Module functions tprint() Returns a formatted string of the structure of the provided table.Parameters:tbl: {table} The table to format indent: {number} Optional indentation of each level of the table Returns: Table structure formatted as a string

---

# 316. Installing ao connect  Cookbook

Document Number: 316
Source: https://cookbook_ao.arweave.net/guides/aoconnect/installing-connect.html
Words: 46
Extraction Method: html

Skip to content  Installing ao connect Prerequisites  In order to install ao connect into your app you must have NodeJS/NPM 18 or higher. Installing npm yarn  This module can now be used from NodeJS as well as a browser, it can be included as shown below.

---

# 317. aoconnect  Cookbook

Document Number: 317
Source: https://cookbook_ao.arweave.net/guides/aoconnect/aoconnect.html
Words: 44
Extraction Method: html

Skip to content  aoconnect ao connect is a Javascript/Typescript library to interact with the system from Node JS or the browser.Guides in this section provide snippets on how to utilize ao connect. All snippets are written in Javascript but should translate easily to Typescript.

---

# 318. Blueprints  Cookbook

Document Number: 318
Source: https://cookbook_ao.arweave.net/guides/aos/blueprints/index.html
Words: 38
Extraction Method: html

Skip to content  Blueprints Blueprints are predesigned templates that help you quickly build in ao. They are a great way to get started and can be customized to fit your needs.Available Blueprints Chatroom CRED Utils Staking Token Voting

---

# 319. LLMstxt  Cooking with the Permaweb

Document Number: 319
Source: https://cookbook.arweave.net/references/llms.html
Words: 36
Extraction Method: html

LLMs.txt The following is a tool open in new window that allows you to build your own LLMs.txt files based on docs from the permaweb ecosystem.Built with  ❤️  by the Arweave community. Learn more at  Arweave.org

---

# 320. Glossary  Cooking with the Permaweb

Document Number: 320
Source: https://cookbook.arweave.net/references/glossary.html
Words: 33
Extraction Method: html

Glossary The following is an embedded glossary tool for the permaweb ecosystem. It supports both light and dark mode, matching the documentation theme.Built with  ❤️  by the Arweave community. Learn more at  Arweave.org

---

# 321. Permaweb Cookbook - Legacy  Cooking with the Permaweb

Document Number: 321
Source: https://cookbook.arweave.net/legacy/index.html
Words: 23
Extraction Method: html

Legacy The following concepts and guides have been deprecated.Concepts Smartweave Guides Smartweave Built with  ❤️  by the Arweave community. Learn more at  Arweave.org

---

# 322. Permaweb Cookbook - Community  Cooking with the Permaweb

Document Number: 322
Source: https://cookbook.arweave.net/community/index.html
Words: 11
Extraction Method: html

Built with  ❤️  by the Arweave community. Learn more at  Arweave.org

---

# 323. Starter Kits  Cooking with the Permaweb

Document Number: 323
Source: https://cookbook.arweave.net/kits/index.html
Words: 11
Extraction Method: html

Built with  ❤️  by the Arweave community. Learn more at  Arweave.org

---

# 324. Routing Strategies - ARIO Docs

Document Number: 324
Source: https://docs.ar.io/wayfinder/core/routing-strategies
Words: 52
Extraction Method: html

Routing strategies determine how Wayfinder selects which AR.IO gateway to use for each request. Different strategies optimize for different goals like performance, reliability, or load distribution.Strategy Comparison Related FastestPingRoutingStrategy: Network-based gateway discovery PreferredWithFallbackRoutingStrategy: Static gateway configuration RoundRobinRoutingStrategy: Even distribution across gateways RandomRoutingStrategy: Randomized gateway selection StaticRoutingStrategy: Always use a single, fixed gateway
